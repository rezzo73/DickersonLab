{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script creates the SUBJECTSFILE and demographics.tsv which are used for the swarmplot. Also, it creates the .fsgd file, which determines which subjects should be included on the GLM as well as the symlinks which are set up in each pipelines respective analyses_nip directory. The recon_notes.csv in their respective pipelines determine who will be excluded from this list (including columns EXCLUDE=1.0 and STATUS_RECONEDIT=complete for flair and STATUS_INITIAL=complete for 3T).\n",
    "???For all cases, only subjects of whom we have their AMY status are included in the analysis.?? \n",
    "Total of 46 jobs are submitted with this script:\n",
    "Jobs: aparc for lh+rh (area, vol, thickness, thicknessstd) and aseg (vol, mean, std) for the 3T and FLAIR pipelines = total of 22 jobs\n",
    "Jobs: glm group steps for lh+rh (mris_preproc, mri_surf2surf, and glm_fit) for the 2 conditions (AMYneg vs. CN and AMYpos vs. CN) for both the 3T and FLAIR pipelines = total of 24 jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: make generalizable to any project; make the sleep time dynamic to the number of processing elements.\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import glob\n",
    "from os import system\n",
    "from paramiko import SSHClient\n",
    "import time\n",
    "\n",
    "project = 'leads' # specifiy project (this is make for leads)\n",
    "\n",
    "# make new stats file of the directory of subjects\n",
    "# make new demographic file by using the \n",
    "pipelines = ['RECON_FLAIR','RECON_3T']\n",
    "downloads_info = '/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv'\n",
    "subjectinfo = pd.read_csv(downloads_info)\n",
    "amyinfo = pd.read_csv('/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_PETINFO/LEADS_AMYELG.csv')\n",
    "fsaverage_path = '/autofs/cluster/freesurfer/centos7_x86_64/stable6/subjects/fsaverage/'\n",
    "scripts_path = '/autofs/cluster/animal/scan_data/leads/analyses_nip/scripts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your PASSWORD for launchpad access: \n",
      "········\n"
     ]
    }
   ],
   "source": [
    "def credentials(): # combined with find_dicom\n",
    "    import getpass\n",
    "    USER = getpass.getuser()\n",
    "    print('Please enter your PASSWORD for launchpad access: ')\n",
    "    PASS= getpass.getpass()\n",
    "    return USER, PASS\n",
    "\n",
    "[user,pw] = credentials()\n",
    "\n",
    "host=\"launchpad\"\n",
    "client=SSHClient()\n",
    "client.load_system_host_keys()\n",
    "client.connect(host,username=user,password=pw, look_for_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDS0070120_20190620\n",
      "LDS0110021_20181016\n",
      "LDS0110022_20181129\n",
      "LDS0110040_20190123\n",
      "LDS0110041_20190212\n",
      "LDS0110052_20190205\n",
      "LDS0110053_20190227\n",
      "LDS0110078_20190320\n",
      "LDS0110079_20190411\n",
      "LDS0220026_20181109\n",
      "LDS0220031_20181130\n",
      "LDS0220050_20190208\n",
      "LDS0220071_20190311\n",
      "LDS0220081_20190322\n",
      "LDS0220084_20190418\n",
      "LDS0220088_20190408\n",
      "LDS0220099_20190515\n",
      "LDS0360098_20190524\n",
      "LDS0370001_20180509\n",
      "LDS0370002_20180606\n",
      "LDS0370005_20180802\n",
      "LDS0370006_20180726\n",
      "LDS0370007_20180801\n",
      "LDS0370008_20180815\n",
      "LDS0370010_20180815\n",
      "LDS0370011_20180822\n",
      "LDS0370012_20180824\n",
      "LDS0370013_20180822\n",
      "LDS0370014_20180913\n",
      "LDS0370015_20181113\n",
      "LDS0370016_20180912\n",
      "LDS0370017_20181001\n",
      "LDS0370018_20181015\n",
      "LDS0370019_20181121\n",
      "LDS0370020_20181212\n",
      "LDS0370029_20181210\n",
      "LDS0370034_20190107\n",
      "LDS0370037_20181218\n",
      "LDS0370038_20181217\n",
      "LDS0370042_20190108\n",
      "LDS0370047_20190226\n",
      "LDS0370058_20190318\n",
      "LDS0370061_20190219\n",
      "LDS0370065_20190227\n",
      "LDS0370073_20190308\n",
      "LDS0370074_20190404\n",
      "LDS0370086_20190329\n",
      "LDS0370097_20190430\n",
      "LDS0370100_20190502\n",
      "LDS0370108_20190620\n",
      "LDS0370110_20190617\n",
      "LDS0670048_20190320\n",
      "LDS0670070_20190305\n",
      "LDS0670076_20190312\n",
      "LDS0670077_20190318\n",
      "LDS0670085_20190327\n",
      "LDS0670090_20190409\n",
      "LDS0670104_20190614\n",
      "LDS0670124_20190620\n",
      "LDS0730024_20181107\n",
      "LDS0730044_20190129\n",
      "LDS0730051_20190313\n",
      "LDS0730055_20190304\n",
      "LDS0730057_20190402\n",
      "LDS0730083_20190624\n",
      "LDS0730101_20190606\n",
      "LDS1770063_20190330\n",
      "LDS1770064_20190222\n",
      "LDS1770082_20190129\n",
      "LDS3600030_20181219\n",
      "LDS3600032_20190123\n",
      "LDS3600043_20190118\n",
      "LDS3600056_20190323\n",
      "LDS3600067_20190320\n",
      "LDS3600068_20190325\n",
      "LDS3600087_20190417\n",
      "LDS3600093_20190430\n",
      "LDS3600105_20190625\n",
      "LDS9410023_20181127\n",
      "LDS9410025_20181128\n",
      "LDS9410027_20181105\n",
      "LDS9410028_20181109\n",
      "LDS9410035_20181126\n",
      "LDS9410036_20181203\n",
      "LDS9410049_20190118\n",
      "LDS9410060_20190219\n",
      "LDS9410066_20190227\n",
      "LDS0070120_20190620\n",
      "LDS0110021_20181016\n",
      "LDS0110022_20181129\n",
      "LDS0110040_20190123\n",
      "LDS0110041_20190212\n",
      "LDS0110052_20190205\n",
      "LDS0110053_20190227\n",
      "LDS0110078_20190320\n",
      "LDS0110079_20190411\n",
      "LDS0220026_20181109\n",
      "LDS0220031_20181130\n",
      "LDS0220050_20190208\n",
      "LDS0220071_20190311\n",
      "LDS0220081_20190322\n",
      "LDS0220084_20190418\n",
      "LDS0220088_20190408\n",
      "LDS0220099_20190515\n",
      "LDS0220123_20190701\n",
      "linked /autofs/cluster/animal/scan_data/leads/recon_nip/RECON_3T/LDS0220123_20190701/FS6_02 /autofs/cluster/animal/scan_data/leads/analyses_nip/RECON_3T/LDS0220123_20190701\n",
      "LDS0360098_20190524\n",
      "LDS0370001_20180509\n",
      "LDS0370002_20180606\n",
      "LDS0370005_20180802\n",
      "LDS0370006_20180726\n",
      "LDS0370008_20180815\n",
      "LDS0370010_20180815\n",
      "LDS0370011_20180822\n",
      "LDS0370012_20180824\n",
      "LDS0370013_20180822\n",
      "LDS0370014_20180913\n",
      "LDS0370015_20181113\n",
      "LDS0370016_20180912\n",
      "LDS0370017_20181001\n",
      "LDS0370018_20181015\n",
      "LDS0370019_20181121\n",
      "LDS0370020_20181212\n",
      "LDS0370029_20181210\n",
      "LDS0370034_20190107\n",
      "LDS0370037_20181218\n",
      "LDS0370038_20181217\n",
      "LDS0370042_20190108\n",
      "LDS0370047_20190226\n",
      "LDS0370058_20190318\n",
      "LDS0370061_20190219\n",
      "LDS0370065_20190227\n",
      "LDS0370073_20190308\n",
      "LDS0370074_20190404\n",
      "LDS0370086_20190329\n",
      "LDS0370089_20190403\n",
      "LDS0370094_20190626\n",
      "linked /autofs/cluster/animal/scan_data/leads/recon_nip/RECON_3T/LDS0370094_20190626/FS6_03 /autofs/cluster/animal/scan_data/leads/analyses_nip/RECON_3T/LDS0370094_20190626\n",
      "LDS0370097_20190430\n",
      "LDS0370100_20190502\n",
      "LDS0370108_20190620\n",
      "LDS0370110_20190617\n",
      "LDS0370127_20190701\n",
      "linked /autofs/cluster/animal/scan_data/leads/recon_nip/RECON_3T/LDS0370127_20190701/FS6_02 /autofs/cluster/animal/scan_data/leads/analyses_nip/RECON_3T/LDS0370127_20190701\n",
      "LDS0370128_20190701\n",
      "linked /autofs/cluster/animal/scan_data/leads/recon_nip/RECON_3T/LDS0370128_20190701/FS6_02 /autofs/cluster/animal/scan_data/leads/analyses_nip/RECON_3T/LDS0370128_20190701\n",
      "LDS0670048_20190320\n",
      "LDS0670070_20190305\n",
      "LDS0670076_20190312\n",
      "LDS0670077_20190318\n",
      "LDS0670080_20190315\n",
      "LDS0670085_20190327\n",
      "LDS0670090_20190409\n",
      "LDS0670104_20190614\n",
      "LDS0670111_20190625\n",
      "linked /autofs/cluster/animal/scan_data/leads/recon_nip/RECON_3T/LDS0670111_20190625/FS6_02 /autofs/cluster/animal/scan_data/leads/analyses_nip/RECON_3T/LDS0670111_20190625\n",
      "LDS0670124_20190620\n",
      "LDS0730024_20181107\n",
      "LDS0730044_20190129\n",
      "LDS0730051_20190313\n",
      "LDS0730055_20190304\n",
      "LDS0730057_20190402\n",
      "LDS0730083_20190624\n",
      "LDS0730101_20190606\n",
      "LDS0730103_20190702\n",
      "linked /autofs/cluster/animal/scan_data/leads/recon_nip/RECON_3T/LDS0730103_20190702/FS6_02 /autofs/cluster/animal/scan_data/leads/analyses_nip/RECON_3T/LDS0730103_20190702\n",
      "LDS1770063_20190330\n",
      "LDS1770064_20190222\n",
      "LDS1770082_20190129\n",
      "LDS3600030_20181219\n",
      "LDS3600032_20190123\n",
      "LDS3600043_20190118\n",
      "LDS3600056_20190323\n",
      "LDS3600067_20190320\n",
      "LDS3600068_20190325\n",
      "LDS3600087_20190417\n",
      "LDS3600093_20190430\n",
      "LDS3600105_20190625\n",
      "LDS3600114_20190702\n",
      "LDS9410023_20181127\n",
      "LDS9410025_20181128\n",
      "LDS9410027_20181105\n",
      "LDS9410028_20181109\n",
      "LDS9410035_20181126\n",
      "LDS9410036_20181203\n",
      "LDS9410049_20190118\n",
      "LDS9410060_20190219\n",
      "LDS9410066_20190227\n"
     ]
    }
   ],
   "source": [
    "# loop for each pipeline (start with FLAIR)\n",
    "# list all folder in directory that begin with LDS\n",
    "group_array = [[],[]]\n",
    "amy_array = [[],[]]\n",
    "sub_array = []\n",
    "\n",
    "\n",
    "for idx, pip in enumerate(pipelines): \n",
    "    if pip == 'RECON_FLAIR':\n",
    "        recon = 'edit'\n",
    "    elif pip == \"RECON_3T\":\n",
    "        recon = 'FS'\n",
    "    analysis_path = '/autofs/cluster/animal/scan_data/leads/analyses_nip/'+pip+'/'\n",
    "    recon_path = '/autofs/cluster/animal/scan_data/leads/recon_nip/'+pip+'/'\n",
    "    recon_notes = pd.read_csv(recon_path+'/recon_notes.csv')\n",
    "    os.chdir(analysis_path) \n",
    "    # look for all subjects that are determined for inclusion and are complete in processing (keeping those who are flagged fow now)\n",
    "    good_subjects = recon_notes[(recon_notes['EXCLUDE']== 0.0) & (recon_notes['STATUS_FINAL']== 'complete')]\n",
    "    sub_array.append(good_subjects['LEADS_ID'].tolist())\n",
    "        \n",
    "    # now make a list of new subject IDs with there corresponding group and write to text file\n",
    "    for sub in sub_array[idx]:\n",
    "        print(sub)\n",
    "        folders = []\n",
    "\n",
    "        # if fsaverage is not in folder (make symlink)\n",
    "        if not os.path.islink(analysis_path+'fsaverage'):\n",
    "            os.symlink(fsaverage_path, analysis_path+'fsaverage')\n",
    "\n",
    "        # copy over symlink if not there (only if INCLUDE and COMPLETE -- still could be left out of final analyses\n",
    "        # folder if no AMY info)\n",
    "        # check if symlink exists in analysis folder, \n",
    "        if os.path.exists(analysis_path+sub):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                folders = [x for x in os.listdir(recon_path+sub) if x.startswith(recon)] \n",
    "                editreconpath = recon_path+sub+'/'+folders[0]\n",
    "                os.symlink(editreconpath, analysis_path+sub)\n",
    "                print('linked '+editreconpath, analysis_path+sub)\n",
    "            except(IndexError):\n",
    "                print(sub+\" has no edited recon yet.\")\n",
    "        \n",
    "        #group_array[idx].append(subjectinfo[subjectinfo['Subject']==sub.split('_')[0]].Group.values[0])\n",
    "        if subjectinfo[subjectinfo['Subject']==sub.split('_')[0]].Group.values[0] == 'CN':\n",
    "            group_array[idx].append(subjectinfo[subjectinfo['Subject']==sub.split('_')[0]].Group.values[0])\n",
    "            #print(sub+' is in group '+subjectinfo[subjectinfo['Subject']==sub.split('_')[0]].Group.values[0])\n",
    "        elif subjectinfo[subjectinfo['Subject']==sub.split('_')[0]].Group.values[0] == 'PT':\n",
    "            try:\n",
    "                group_array[idx].append(str(amyinfo[amyinfo['subject_code']==sub.split('_')[0].lower()].outcome.values[0]))\n",
    "                #print(sub+' is in group '+subjectinfo[subjectinfo['Subject']==sub.split('_')[0]].Group.values[0]+' and has AMY '+str(amyinfo[amyinfo['subject_code']==sub.split('_')[0].lower()].outcome.values[0]))\n",
    "            except(IndexError):\n",
    "                #print(sub+' is in group '+subjectinfo[subjectinfo['Subject']==sub.split('_')[0]].Group.values[0]+' and has AMY '+str(amyinfo[amyinfo['subject_code']==sub.split('_')[0].lower()].outcome.values[0]))\n",
    "                group_array[idx].append(\"\") # change this to PT if want without AMY status\n",
    "        else:\n",
    "            #print(sub+ ' has no group data.')\n",
    "            group_array[idx].append(\"\")\n",
    "\n",
    "    # further refine group (replace all PT with AMY+ or AMY-)\n",
    "    group_status = [w.replace('0', 'N').replace('1','P').replace('CN','C') for w in group_array[idx]] ## I THINK PROBLEM STARTS HERE\n",
    "\n",
    "    \n",
    "    # put into a dataframe\n",
    "    zippedList =  list(zip(sub_array[idx], group_status)) # group_stats is redefined per iteration\n",
    "    demographics_df = pd.DataFrame(zippedList, columns = ['Subject' , 'Group']) #, index=['a', 'b', 'c']) \n",
    "    demographics_df.set_index('Subject', inplace=True)\n",
    "    \n",
    "    # now create new dataframe eliminating instances of PT (ones that do not have AMY status)\n",
    "    cleaned_demographics = demographics_df[demographics_df['Group']!='']\n",
    "    #demographics_df = demographics_df[demographics_df.Group != '']\n",
    "    \n",
    "    # now save this to the appropriate analysis pipeline\n",
    "    cleaned_demographics.to_csv(analysis_path+'/demographics_generated', sep='\\t') #, index=False\n",
    "    \n",
    "    \n",
    "    # NOTE:  where did Jess get those other subjects from?\n",
    "    # ignore for now\n",
    "    \n",
    "    # create a SUBJECTSFILE WITH THIS LIST and save to respective analysis folder\n",
    "    # (this will determine what stats will be generated)\n",
    "        \n",
    "    with open(analysis_path+'SUBJECTSFILE_automated', 'w') as f: # does not exclude subjects with no AMY status\n",
    "        for item in cleaned_demographics.index.values.tolist():\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "    # now also make fsgd file for GLM for each pipeline (compare anything other than C to C)\n",
    "    group_list = list(cleaned_demographics.Group.unique())\n",
    "    group_list.remove('C')\n",
    "    \n",
    "    for inst in group_list:\n",
    "        # make new data frame with only that inst and C, then add a column Input\n",
    "        new_compare = cleaned_demographics.loc[cleaned_demographics['Group'].isin(['C',inst])]\n",
    "        sublist = new_compare.index.values.tolist()\n",
    "        sublist.sort()\n",
    "        grouplist = list(new_compare['Group'])\n",
    "        indexlist = ['Input'] * len(grouplist)\n",
    "\n",
    "        with open(analysis_path+'/AMY'+inst+'vsCN.fsgd', 'w') as f:\n",
    "            writer = csv.writer(f, delimiter='\\t')\n",
    "            f.write('GroupDescriptorFile 1\\n# One Factor/Two Levels (no covariates)\\nTitle LEADS\\nClass C plus green\\nClass %s circle blue\\nVariables\\n' %inst) #%inst\n",
    "            writer.writerows(zip(indexlist,sublist,grouplist))\n",
    "        os.system(\"cat AMYPvsCN.fsgd | sed 's/\\r/\\n/g' > new.AMYPvsCN.fsgd\") \n",
    "        sh_list = list(cleaned_demographics.index.values)    \n",
    "        \n",
    "#########################################################################################\n",
    "    #now generate stats files as csvs and glm output (make sure metric and hemisphere is in file)\n",
    "    statstr = '(cd %s; setenv m %s ; setenv p %s ; ./run_generatestats.sh)' % (scripts_path, analysis_path, project)\n",
    "    stdin, stdout, stderr = client.exec_command(statstr)\n",
    "    time.sleep(30) \n",
    "    glmstr = '(cd %s; setenv m %s ; setenv p %s ; ./run_groupglm.sh)' % (scripts_path, analysis_path, project)\n",
    "    stdin, stdout, stderr = client.exec_command(glmstr)\n",
    "    time.sleep(30) \n",
    "    #err = \"stderr: \", stderr.readlines()\n",
    "    #out = \"pwd: \", stdout.readlines()\n",
    "        \n",
    "    # Now open each of the stats files and put a control in the first row for every file (how to know when job is done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
