{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIREMENTS FOR THIS PIPELINE\n",
    "#  the recon output will be named e.g. \"unedit.FS6_02\". After manually editing\n",
    "#  please rename this recon folder to \"edit.FS6_02\". Then the next time \n",
    "#  pipeline is run, it will re-submit the recon for this folder and create symlink.\n",
    "\n",
    "#  the following notes need to updated periodically so this info can be piped:\n",
    "#  files: LEADS_#####_date.csv (csv download file), Mayo_ADRIL_MRI_Quality_date.csv\n",
    "\n",
    "        #details:\n",
    "        #MOST IMPORTANT: once a new download has been initiated, load the download CSV file into\n",
    "        #/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/\n",
    "        #if it is a new image collection on loni, it will create a separate CSV file, \n",
    "        #otherwise will replace old one with concatenated data of that image collection.\n",
    "        \n",
    "        #must also download Mayo_ADRIL_MRI_Quality_date.csv and save to:\n",
    "        #/autofs/cluster/animal/scan_data/leads/spreadsheets/MRIQUALITY\n",
    "        #this file will replace the old one (concatenates all data on loni about QC).\n",
    "\n",
    "#  if any files need to be re-run / re-processed, delete the files in the \n",
    "#  folder. This pipeline does not overwrite anything.\n",
    "\n",
    "# TO DO\n",
    "#  send a recon job- just do for ones that do not have output?\n",
    "#  not sure why but import_loni_notes is creates a new column in the dataframe scannotes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import io, os, sys, types # needed\n",
    "import glob # needed\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode # needed\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "import nipype.interfaces.io as nio\n",
    "import nipype.pipeline.engine as pe\n",
    "from nipype.interfaces.freesurfer import MRIConvert\n",
    "from nipype.interfaces.freesurfer import ReconAll\n",
    "from nipype import config\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import pathlib\n",
    "import pydicom\n",
    "from pydicom.tag import Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and update spreadsheets\n",
    "\n",
    "# these do not concatenate, must do this\n",
    "downloadsdir = '/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/'\n",
    "\n",
    "# vertically concatenate all csvs\n",
    "downloadlist = glob.glob(downloadsdir+'*.csv')\n",
    "downloadlist.remove('/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv')\n",
    "\n",
    "# vertically concatenate all csvs\n",
    "combined_csv = pd.concat( [ pd.read_csv(f) for f in downloadlist ] , sort=False)\n",
    "\n",
    "# drop all non-MPRAGES, sort dataframe by subject column, drop all duplicates\n",
    "combined_csv = combined_csv[combined_csv.Description == 'Accelerated Sagittal MPRAGE']\n",
    "combined_csv = combined_csv.sort_values(by=['Downloaded'])\n",
    "combined_csv = combined_csv.drop_duplicates(['Image Data ID'], keep='last')\n",
    "\n",
    "# save combined download file\n",
    "combined_csv.to_csv(\"/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv\", index=False,)\n",
    "\n",
    "# these download already concatenating all sessions ; use latest\n",
    "qualitydir = '/autofs/cluster/animal/scan_data/leads/spreadsheets/MRIQUALITY/'\n",
    "\n",
    "list_of_qualityfiles = glob.glob(qualitydir+'*.csv')\n",
    "MRIQUALITY = max(list_of_qualityfiles, key=os.path.getctime)\n",
    "\n",
    "# make master list that pushes to scannotes or visa versa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(subject):\n",
    "    subsessions = glob.glob(dicomdir+subject+'/Accelerated_Sagittal_MPRAGE/*/*/')\n",
    "    repeat_tag = '-'\n",
    "    for num in range(len(subsessions)):\n",
    "        # look to see if more than one session on the same date\n",
    "        # then look to see if more than one date (or both)\n",
    "        parentfolder = subsessions[num].split('/')[8]\n",
    "        filename = os.listdir(subsessions[num])[0] # dicom name to extract date\n",
    "        #extract date from dicom:\n",
    "        ds = pydicom.read_file(subsessions[num]+'/'+filename)\n",
    "        date = str(ds[0x08, 0x22].value)\n",
    "        #date = re.search('raw_'+r'+[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]', filename).group()[4:]\n",
    "        if num == len(subsessions)-1:\n",
    "            namedate = dicomdir+subject+'_'+date\n",
    "            os.rename(dicomdir+subject, namedate)\n",
    "        else:\n",
    "            namedateplus = dicomdir+subject+'_'+date+repeat_tag+'/Accelerated_Sagittal_MPRAGE/'+folder1\n",
    "            pathlib.Path(namedateplus).mkdir(parents=True, exist_ok=True)\n",
    "            shutil.move(subsessions[num], namedateplus)\n",
    "            if not os.listdir(dicomdir+subject+'/Accelerated_Sagittal_MPRAGE/'+parentfolder):\n",
    "                os.rmdir(dicomdir+subject+'/Accelerated_Sagittal_MPRAGE/'+parentfolder)\n",
    "            repeat_tag = repeat_tag+'-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify variables\n",
    "leadsdir = '/cluster/animal/scan_data/leads/'\n",
    "os.chdir(leadsdir)\n",
    "dicomdir = \"/cluster/animal/scan_data/leads/LEADS/\"\n",
    "unpacklog = \"/autofs/cluster/animal/scan_data/leads/recon/unpack.log\"\n",
    "recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'\n",
    "folders = [x for x in os.listdir(dicomdir) if not x.startswith(\".\")]\n",
    "subjlist = [f for f in os.listdir(dicomdir) if ((\"_\") not in f) and (not f.startswith('.') and (\"duplicate\" not in f))]\n",
    "\n",
    "# wipe clean batch.recon.list\n",
    "open(recondir+'batch.recon.list', 'w').close()\n",
    "\n",
    "# unwravel all subjects with multiple sessions and rename to include date\n",
    "for sub in subjlist:\n",
    "    scan(sub)\n",
    "\n",
    "# now just make a list of subjects by ID (this will be the input to the nodes)\n",
    "# will define dicom path within node\n",
    "sh_dicomlist = [f for f in os.listdir(dicomdir) if ((\"_\" in f) and (\"REPEAT_RUNS\" not in f))]\n",
    "\n",
    "# define workflow\n",
    "leads_workflow = Workflow(name='leads_workflow') #, base_dir = '/autofs/cluster/animal/scan_data/leads/recon_nip/') # add  base_dir='/shared_fs/nipype_scratch'\n",
    "\n",
    "# configure to stop on first crash\n",
    "cfg = dict(execution={'stop_on_first_crash': True})\n",
    "config.update_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging\n",
    "sh_dicomlist = ['LDS3600056_20190323', 'LDS3600068_20190325']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST NODE : PASSSWORDS\n",
    "\n",
    "def credentials(): # combined with find_dicom\n",
    "    import getpass\n",
    "    USER = getpass.getuser()\n",
    "    print('Please enter your PASSWORD for launchpad access: ')\n",
    "    PASS= getpass.getpass()\n",
    "    return USER, PASS\n",
    "\n",
    "PASSWORDS = pe.Node(Function(input_names=[\"user\", \"pw\"],\n",
    "                         output_names=[\"USER\",\"PASS\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=credentials),\n",
    "                        name='PASSWORDS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : CREATEDIR\n",
    "def createdir(val, USER, PASS):\n",
    "    import os\n",
    "    import re\n",
    "    import glob\n",
    "    import pydicom\n",
    "    from pydicom.tag import Tag\n",
    "    val = val.split('/')[-1]\n",
    "    dicomdir = \"/autofs/cluster/animal/scan_data/leads/LEADS/\"\n",
    "    recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/' \n",
    "    reconpath = recondir+val+'/'\n",
    "    imgpath = reconpath+'/mri/orig/'\n",
    "    dumplocation = imgpath+'001.mgz'\n",
    "    subject = val.split('_')[0]\n",
    "    #date = val.split('_')[1].replace(\"-\",\"\")  # take out any '-'\n",
    "    MPRAGE_path = glob.glob(dicomdir+val+'/Accelerated_Sagittal_MPRAGE/*/*')[0]\n",
    "    pickdicom = glob.glob(dicomdir+val+'/Accelerated_Sagittal_MPRAGE/*/*/*')[0]\n",
    "    ds = pydicom.read_file(pickdicom) #(MPRAGE_path+'/'+pickdicom)\n",
    "    date = str(ds[0x08, 0x22].value)\n",
    "    sessionid = MPRAGE_path.split(\"/\")[-1]\n",
    "    try:\n",
    "        os.makedirs(imgpath)\n",
    "    except(FileExistsError):\n",
    "        pass\n",
    "    return reconpath, MPRAGE_path, pickdicom, dumplocation, recondir, USER, PASS, imgpath, date\n",
    "        \n",
    "CREATEDIR = pe.Node(Function(input_names=[\"val\", \"USER\", \"PASS\"],\n",
    "                         output_names=[\"createdir_out1\",\"createdir_out2\", \"createdir_out3\", \"createdir_out4\", \"createdir_out5\", \"USER\", \"PASS\", \"createdir_out6\", \"date\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=createdir),\n",
    "                        name='CREATEDIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretend_df = pd.read_csv('/autofs/cluster/animal/scan_data/leads/recon_nip/LDS9410060_20190220/scannotes.csv')\n",
    "# #pretend_df.loc[pretend_df.index[0], 'scan_notes'] = \"some value\"\n",
    "# qc_pass = ''\n",
    "# concat_comments = 'test'\n",
    "# download_date = '44/44/44'\n",
    "# pretend_df.loc[pretend_df.index[0], 'loni_overallpass'] = qc_pass\n",
    "# pretend_df.loc[pretend_df.index[0], 'scan_notes'] = concat_comments\n",
    "# pretend_df.loc[pretend_df.index[0], 'download_date'] = download_date\n",
    "\n",
    "# pretend_df\n",
    "\n",
    "# # Unnamed: 0 ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : IMPORT_LONI_INFO\n",
    "\n",
    "def import_loni_notes(dicomname, date):\n",
    "    import pandas as pd\n",
    "    import glob\n",
    "    import os\n",
    "    import re\n",
    "    # download info\n",
    "    download_df = pd.read_csv('/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv')\n",
    "    recon_dir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'\n",
    "    dicom = dicomname.split(\"/\")[-1]\n",
    "    subid = dicom.split(\"_\")[1]\n",
    "    #date = re.search('raw_'+r'+[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]', dicomname).group()[4:]\n",
    "    imageid = dicom.split(\"_\")[12][1:-4]\n",
    "    scannotes_df = pd.read_csv(recon_dir+subid+'_'+date+'/scannotes.csv')\n",
    "    try:\n",
    "        download_date = download_df.loc[download_df['Image Data ID'] == float(imageid), 'Downloaded'].values[0]\n",
    "    except(IndexError):\n",
    "        pass\n",
    "    # loni notes\n",
    "    qualitydir = '/autofs/cluster/animal/scan_data/leads/spreadsheets/MRIQUALITY/'\n",
    "    list_qc_files = glob.glob(qualitydir+'*.csv')\n",
    "    MRIQUALITY = max(list_qc_files, key=os.path.getctime)\n",
    "    # these download already concatenating all sessions ; use latest\n",
    "    quality_df = pd.read_csv(MRIQUALITY)\n",
    "    try:\n",
    "        loni_overallpass = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_overallpass'].values[0]\n",
    "        if loni_overallpass == 1:\n",
    "            qc_pass = '1'\n",
    "        elif loni_overallpass == 4:\n",
    "            qc_pass = '0'\n",
    "        else:\n",
    "            qc_pass = ''\n",
    "        study_comments = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_comments'].values[0]\n",
    "        study_protocol_comment = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_protocol_comment'].values[0]\n",
    "        protocol_comments = quality_df.loc[quality_df['loni_image'] == float(imageid), 'protocol_comments'].values[0]\n",
    "        series_comments = quality_df.loc[quality_df['loni_image'] == float(imageid), 'series_comments'].values[0]\n",
    "        series_quality = quality_df.loc[quality_df['loni_image'] == float(imageid), 'series_quality'].values[0]# if 3, needs review; if 2 it is ok\n",
    "        if series_quality == 2:\n",
    "            s_quality = 'Scan quality is acceptable according to acquisition team. '\n",
    "        elif series_quality == 3:\n",
    "            s_quality = 'Scan quality is questionable according to acquisition team and needs review. '\n",
    "        elif series_quality == 4:\n",
    "            s_quality = 'Scan quality is poor according to acquisition team and needs review. '\n",
    "        else:\n",
    "            s_quality = 'No scan quality data recorded from acquisition team. '\n",
    "        study_rescan_requested = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_rescan_requested'].values[0]\n",
    "        if study_rescan_requested == 'TRUE':\n",
    "            rescan_requested = ' Study rescan has been requested. '\n",
    "        else:\n",
    "            rescan_requested = '. No study rescans have been requested. '\n",
    "\n",
    "        # delete duplicates within list, delete nans\n",
    "        comments_list = [s_quality,str(study_comments),str(study_protocol_comment),str(series_comments),str(protocol_comments),rescan_requested]\n",
    "        cleanedList = [x for x in comments_list if (x != 'nan')]\n",
    "        concat_comments = ''.join(cleanedList)\n",
    "    except(IndexError):\n",
    "        concat_comments = 'No comments from acquisition team. '\n",
    "        qc_pass = ''\n",
    "    \n",
    "    scannotes_df.loc[scannotes_df.index[0], 'loni_overallpass'] = qc_pass\n",
    "    scannotes_df.loc[scannotes_df.index[0], 'scan_notes'] = concat_comments\n",
    "    scannotes_df.loc[scannotes_df.index[0], 'download_date'] = download_date\n",
    "    scannotes_df.to_csv(recon_dir+subid+'_'+date+'/scannotes.csv')\n",
    "    \n",
    "IMPORT_LONI_INFO = pe.Node(Function(input_names=[\"dicomname\", \"date\"],\n",
    "                         function=import_loni_notes),\n",
    "                        name='IMPORT_LONI_INFO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : UNPACK\n",
    "\n",
    "def unpack(subjectdir, MPRAGE_path):\n",
    "    from os import system\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    import os.path\n",
    "    if not os.path.isfile(subjectdir+'scan.info'): \n",
    "        cmdstring = 'unpacksdcmdir -src %s -targ %s -scanonly %s/scan.info' % (MPRAGE_path, subjectdir, subjectdir)\n",
    "        system(cmdstring)\n",
    "    if not os.path.isfile(subjectdir+'scaninfo.csv'):\n",
    "        with open(subjectdir+'/scan.info', 'r') as in_file:\n",
    "            for line in in_file:\n",
    "                editline = line.split()\n",
    "                with open(subjectdir+'/scaninfo.csv', 'w') as result:\n",
    "                    wr = csv.writer(result, dialect='excel')\n",
    "                    wr.writerow(editline)\n",
    "                result.close()\n",
    "            in_file.close()\n",
    "    scan_info = pd.read_csv(subjectdir+'/scaninfo.csv', header=None)\n",
    "    subname = subjectdir.split('/')[-2]\n",
    "    return subname, subjectdir, scan_info\n",
    "\n",
    "UNPACK = pe.Node(Function(input_names=[\"subjectdir\",\"MPRAGE_path\"],\n",
    "                         output_names=[\"unpack_out1\",\"unpack_out2\", \"unpack_out3\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=unpack),\n",
    "                        name='UNPACK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : CONVERT2MGZ\n",
    "# I think I can set subject as variable to pass on here and others send elsewhere\n",
    "\n",
    "CONVERT2MGZ = pe.Node(MRIConvert(out_type='mgz'), \n",
    "                        name='CONVERT2MGZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NODE : CONVERT2MGZ (only runs if .mgz is not available)\n",
    "\n",
    "# def convert_dicom(in_file, out_file, imgpath):\n",
    "#     import os\n",
    "#     if os.path.isdir(imgpath):\n",
    "#         if not os.path.exists(in_file):\n",
    "#             cmdstring = 'mri_convert %s %s' % (in_file, out_file)\n",
    "#             system(cmdstring)\n",
    "\n",
    "# CONVERT2MGZ = pe.Node(Function(input_names=[\"in_file\", \"out_file\", \"imgpath\"],\n",
    "#                          output_names=[\"out_file\"],\n",
    "#                          function=convert_dicom),\n",
    "#                         name='CONVERT2MGZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE SCAN_AND_LOG\n",
    "# note: decided to add this afterward precaution to increase efficiency because there are few errors\n",
    "# and want to run the unpack and convert2mgz in parallel)\n",
    "\n",
    "def scan_and_log(subjectdir, scan_info, mgz, reconfolder, subname):\n",
    "    import re\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    # load in the scaninfo file\n",
    "    dicomdir = \"/cluster/animal/scan_data/leads/LEADS/\"\n",
    "    check = scan_info.iloc[0,2] # first row (only row); second col (validity)\n",
    "    if check != 'ok':\n",
    "        with open(reconfolder+'/scanerrors', \"a\") as efile:\n",
    "            efile.write(scan_info.iloc[0,7]) # log for errors in dicoms (or any ommitted scans)\n",
    "    else:\n",
    "        with open(reconfolder+'/batch.recon.list', \"a\") as bfile:\n",
    "            bfile.write(subname)\n",
    "        with open(reconfolder+'/unpack.log', \"a\") as ufile:\n",
    "            ufile.write(subname)\n",
    "        # should I makea scannotes? (will add info after recon)\n",
    "        Elements = {'scan_notes': [''],'loni_overallpass': [''], 'download_date':[''], 'xnat_upload':[''], 'recon_path':[''],'recon_notes':[''], 'dickerson_overallpass':['']}\n",
    "        df = pd.DataFrame(Elements, columns= ['scan_notes', 'loni_overallpass', 'download_date','xnat_upload','recon_path','recon_notes','dickerson_overallpass'])\n",
    "        df.to_csv(subjectdir+'/scannotes.csv')\n",
    "    return subjectdir, subname\n",
    "\n",
    "SCAN_AND_LOG = pe.Node(Function(input_names=[\"subjectdir\",\"scan_info\",'mgz', 'reconfolder', 'subname'],\n",
    "                         output_names=[\"subjectdir\", \"subname\"],\n",
    "                         function=scan_and_log),\n",
    "                        name='SCAN_AND_LOG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE RECON_JOB\n",
    "\n",
    "def recon_job(subjectname, USER, PASS): # add in username, pass, and subjectname\n",
    "    # add condition :: run this only is FS_XX, or scripts does not exist!!\n",
    "    import os\n",
    "    import glob\n",
    "    from paramiko import SSHClient\n",
    "    reconpath = '/autofs/cluster/animal/scan_data/leads/recon_nip/'\n",
    "    if ((glob.glob('/autofs/cluster/animal/scan_data/leads/recon_nip/'+subjectname+'/FS*') == []) or (glob.glob('/autofs/cluster/animal/scan_data/leads/recon_nip/'+subjectname+'/script*/') == [])):\n",
    "        host=\"launchpad\"\n",
    "        user=USER\n",
    "        pw=PASS\n",
    "        client=SSHClient()\n",
    "        client.load_system_host_keys()\n",
    "        client.connect(host,username=user,password=pw, look_for_keys=False)\n",
    "        tmpstr = '(cd /autofs/cluster/animal/scan_data/leads/analyses_nip; setenv p %s ; ./batch.recon.sh)' % subjectname\n",
    "        stdin, stdout, stderr = client.exec_command(tmpstr)\n",
    "        #stin = print(\"stdin: \", stdin.readlines())\n",
    "        err = \"stderr: \", stderr.readlines()\n",
    "        out = \"pwd: \", stdout.readlines()\n",
    "        if len(err) < 1:\n",
    "            warning = '0'\n",
    "        else:\n",
    "            warning = '1'\n",
    "        return err, out, warning, subjectname\n",
    "\n",
    "RECON_JOB = pe.Node(Function(input_names=[\"subjectname\",\"USER\", \"PASS\"], \n",
    "                        output_names=[ 'err', 'out', 'warning','subjectname'],\n",
    "                         function=recon_job),\n",
    "                        name='RECON_JOB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE: GATHER_FS_DETAILS (this part only after recon is done)\n",
    "\n",
    "def gather_FS_details(subjectname): # add in username, pass, and subjectname\n",
    "    import csv\n",
    "    import os\n",
    "    recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'\n",
    "    # if you can access the status (complete) and \n",
    "    if os.path.isfile(recondir+subjectname+'/scripts/recon-all.done'): # replaces from old: recon-all.log\n",
    "        recon_pending = 0\n",
    "    #try:\n",
    "        # obtain FS version\n",
    "        versionfile = open(recondir+subjectname+'/scripts/build-stamp.txt', 'r')\n",
    "        versionstring = versionfile.read()\n",
    "        version = versionstring.split('-')\n",
    "        result = [i for i in version if i.startswith('v')][0]\n",
    "        long = result[1:]\n",
    "        \n",
    "        #obtain short verison of long\n",
    "        size = len(long)\n",
    "        x = 0\n",
    "        while x ==0:\n",
    "            if (long[-1] == '0') or (long[-1] == '.'): # shave off any . or 0s from the end of version number.\n",
    "                long = long[:-1]\n",
    "            else:\n",
    "                x =1\n",
    "        vlabel = 'FS'+long\n",
    "\n",
    "        # obtain run number\n",
    "        with open(recondir+subjectname+'/scaninfo.csv','r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            scan_list = list(reader)\n",
    "            runstring = scan_list[0][0] # run\n",
    "            if len(runstring) == 1:\n",
    "                runstring = '0'+runstring\n",
    "        recon_name = vlabel+'_'+runstring\n",
    "    else:\n",
    "    #except(FileNotFoundError):\n",
    "        recon_pending = 1\n",
    "        print(subjectname+\" Recon either already organized or not ready yet.\")\n",
    "        recon_name = ''\n",
    "    return subjectname, recon_name, recon_pending\n",
    "        \n",
    "FS_DETAILS = pe.Node(Function(input_names=[\"subjectname\"], \n",
    "                        output_names=[ 'subjectname', 'recon_name', 'recon_pending'],\n",
    "                         function=gather_FS_details),\n",
    "                        name='FS_DETAILS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : MAKE_ORIG_FOLDER\n",
    "\n",
    "def create_orig_folder(subjectname, recon_name, recon_pending):\n",
    "    import os\n",
    "    import shutil\n",
    "    recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'\n",
    "    freesurfer_dirs = ['mri', 'stats', 'tmp', 'trash', 'touch', 'label', 'surf', 'scripts']\n",
    "    if recon_pending == 0:\n",
    "        # move all subfolders into this recon_name folder\n",
    "        for fsdir in freesurfer_dirs:\n",
    "            if os.path.isdir(recondir+subjectname+'/'+fsdir):\n",
    "                shutil.move(recondir+subjectname+'/'+fsdir, recondir+subjectname+'/'+recon_name+'/'+fsdir)\n",
    "    else:\n",
    "        print(subjectname+\" files moves already, or not prepared.\")\n",
    "    return subjectname, recon_name, recondir, recon_pending\n",
    "\n",
    "MAKE_ORIGINAL_DIR = pe.Node(Function(input_names=[\"subjectname\", \"recon_name\", \"recon_pending\"], \n",
    "                        output_names=['subjectname', 'recon_name', 'recondir', 'recon_pending'],\n",
    "                         function=create_orig_folder),\n",
    "                        name='MAKE_ORIGINAL_DIR')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : PREPARE_MANEDITS\n",
    "\n",
    "def preparing_manedits(subjectname, recon_name, recondir, recon_pending):\n",
    "    import shutil\n",
    "    import pathlib\n",
    "    import os\n",
    "    analysesdir = '/autofs/cluster/animal/scan_data/leads/analyses_nip/'\n",
    "    if recon_pending == 0: # otherwise dir already created or not ready)\n",
    "        recon_name2 = 'unedit.'+recon_name\n",
    "        #FS2 = pathlib.Path(recondir+subjectname+'/'+recon_name2).mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copytree(recondir+subjectname+'/'+recon_name, recondir+subjectname+'/'+recon_name2)\n",
    "        # if edit.recon folder does not already exist, add to log\n",
    "        with open(recondir+'to_edit.list\\n', \"a\") as myfile:\n",
    "            myfile.write(subjectname)\n",
    "        \n",
    "    # this will symlink before manual editing but ok since renaming unedit >> edit is required\n",
    "    #Now create symlink to edit. in analysis folder!!\n",
    "#     try:\n",
    "#         os.symlink(recondir+subjectname+'/'+recon_name2, analysesdir+sub)\n",
    "#     except(FileExistsError):\n",
    "#         print(subjectname+\" in analyses is already linked to unedit.recon.\")\n",
    "    return subjectname\n",
    "        \n",
    "PREPARE_MANEDITS = pe.Node(Function(input_names=['subjectname', 'recon_name', 'recondir', 'recon_pending'], \n",
    "                        output_names=['subjectname'],\n",
    "                         function=preparing_manedits),\n",
    "                        name='PREPARE_MANEDITS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : obtain post-edit notes , FS version, push notes to xnat and update xnat_upload field\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # NODE : INFOSOURCE\n",
    "INFOSOURCE = Node(IdentityInterface(fields=['subject_name'], mandatory_inputs=False),\n",
    "                  name=\"INFOSOURCE\")\n",
    "\n",
    "INFOSOURCE.iterables = ('subject_name', sh_dicomlist)\n",
    "\n",
    "# NODE : SELECTFILES\n",
    "#templates = dict(dicom=sh_dicomlist[0])    ## THIS WORKED!\n",
    "templates = {\n",
    "    \"dicom\": \"{subject_name}\" \n",
    "    }\n",
    "SELECTFILES = Node(nio.SelectFiles(templates, base_directory=dicomdir),\n",
    "                   name=\"SELECTFILES\")\n",
    "\n",
    "# NODE : DATASINK\n",
    "DATASINK = Node(nio.DataSink(base_directory=leadsdir,\n",
    "                container='recon_nip'),\n",
    "                name=\"DATASINK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect all nodes (including INFOSOURCE, SELECTFILES, and DATASINK) to workflow\n",
    "\n",
    "leads_workflow.connect([(INFOSOURCE, SELECTFILES, [('subject_name', 'subject_name')]),\n",
    "                (SELECTFILES, CREATEDIR, [('dicom', 'val')]),\n",
    "                (PASSWORDS, CREATEDIR, [('USER', 'USER')]),\n",
    "                (PASSWORDS, CREATEDIR, [('PASS', 'PASS')]), \n",
    "                (CREATEDIR, IMPORT_LONI_INFO, [('date', 'date')]),\n",
    "                (CREATEDIR, IMPORT_LONI_INFO, [('createdir_out3', 'dicomname')]),\n",
    "                (CREATEDIR, UNPACK, [('createdir_out1', 'subjectdir')]),\n",
    "                 (CREATEDIR, UNPACK, [('createdir_out2', 'MPRAGE_path')]),\n",
    "                 (CREATEDIR, CONVERT2MGZ, [('createdir_out3', 'in_file')]),\n",
    "                 (CREATEDIR, CONVERT2MGZ, [('createdir_out4', 'out_file')]),\n",
    "                #(CREATEDIR, CONVERT2MGZ, [('createdir_out6', 'imgpath')]),\n",
    "                (CONVERT2MGZ, SCAN_AND_LOG, [('out_file', 'mgz')]),\n",
    "                (CREATEDIR, SCAN_AND_LOG, [('createdir_out5', 'reconfolder')]),\n",
    "                (UNPACK, SCAN_AND_LOG, [('unpack_out1', 'subname')]),\n",
    "                (UNPACK, SCAN_AND_LOG, [('unpack_out2', 'subjectdir')]),\n",
    "                (UNPACK, SCAN_AND_LOG, [('unpack_out3', 'scan_info')]),\n",
    "                (CREATEDIR, RECON_JOB, [('USER','USER')]),\n",
    "                (CREATEDIR, RECON_JOB, [('PASS','PASS')]),\n",
    "                (SCAN_AND_LOG, RECON_JOB, [('subname','subjectname')]), \n",
    "                (RECON_JOB, FS_DETAILS, [('subjectname','subjectname')]),\n",
    "                (FS_DETAILS, MAKE_ORIGINAL_DIR, [('subjectname','subjectname')]), \n",
    "                (FS_DETAILS, MAKE_ORIGINAL_DIR, [('recon_pending','recon_pending')]), \n",
    "                (FS_DETAILS, MAKE_ORIGINAL_DIR, [('recon_name','recon_name')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('subjectname','subjectname')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('recon_name','recon_name')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('recondir','recondir')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('recon_pending','recon_pending')]), \n",
    "                (PREPARE_MANEDITS, DATASINK, [('subjectname','backup')])  # backup folder?\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190327-09:55:49,29 nipype.workflow INFO:\n",
      "\t Workflow leads_workflow settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "190327-09:55:49,94 nipype.workflow INFO:\n",
      "\t Running serially.\n",
      "190327-09:55:49,96 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.SELECTFILES\" in \"/tmp/tmpi6p2oo2s/leads_workflow/_subject_name_LDS3600068_20190325/SELECTFILES\".\n",
      "190327-09:55:49,103 nipype.workflow INFO:\n",
      "\t [Node] Running \"SELECTFILES\" (\"nipype.interfaces.io.SelectFiles\")\n",
      "190327-09:55:49,112 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.SELECTFILES\".\n",
      "190327-09:55:49,114 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.SELECTFILES\" in \"/tmp/tmp1_spplpb/leads_workflow/_subject_name_LDS3600056_20190323/SELECTFILES\".\n",
      "190327-09:55:49,120 nipype.workflow INFO:\n",
      "\t [Node] Running \"SELECTFILES\" (\"nipype.interfaces.io.SelectFiles\")\n",
      "190327-09:55:49,126 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.SELECTFILES\".\n",
      "190327-09:55:49,127 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.PASSWORDS\" in \"/tmp/tmplj7roe17/leads_workflow/PASSWORDS\".\n",
      "190327-09:55:49,131 nipype.workflow INFO:\n",
      "\t [Node] Running \"PASSWORDS\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "Please enter your PASSWORD for launchpad access: \n",
      "········\n",
      "190327-09:55:54,195 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.PASSWORDS\".\n",
      "190327-09:55:54,197 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CREATEDIR\" in \"/tmp/tmpycac5v4u/leads_workflow/_subject_name_LDS3600068_20190325/CREATEDIR\".\n",
      "190327-09:55:54,206 nipype.workflow INFO:\n",
      "\t [Node] Running \"CREATEDIR\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:55:54,234 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CREATEDIR\".\n",
      "190327-09:55:54,235 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CONVERT2MGZ\" in \"/tmp/tmpbci8y00j/leads_workflow/_subject_name_LDS3600068_20190325/CONVERT2MGZ\".\n",
      "190327-09:55:54,261 nipype.workflow INFO:\n",
      "\t [Node] Running \"CONVERT2MGZ\" (\"nipype.interfaces.freesurfer.preprocess.MRIConvert\"), a CommandLine Interface with command:\n",
      "mri_convert --out_type mgz --input_volume /autofs/cluster/animal/scan_data/leads/LEADS/LDS3600068_20190325/Accelerated_Sagittal_MPRAGE/2019-03-25_09_42_12.0/S809792/LEADS_LDS3600068_MR_Accelerated_Sagittal_MPRAGE__br_raw_20190325145425343_187_S809792_I1146946.dcm --output_volume /autofs/cluster/animal/scan_data/leads/recon_nip/LDS3600068_20190325//mri/orig/001.mgz\n",
      "190327-09:55:54,362 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:55:54.362512:mri_convert.bin --out_type mgz --input_volume /autofs/cluster/animal/scan_data/leads/LEADS/LDS3600068_20190325/Accelerated_Sagittal_MPRAGE/2019-03-25_09_42_12.0/S809792/LEADS_LDS3600068_MR_Accelerated_Sagittal_MPRAGE__br_raw_20190325145425343_187_S809792_I1146946.dcm --output_volume /autofs/cluster/animal/scan_data/leads/recon_nip/LDS3600068_20190325//mri/orig/001.mgz \n",
      "190327-09:55:54,368 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:55:54.368168:$Id: mri_convert.c,v 1.226 2016/02/26 16:15:24 mreuter Exp $\n",
      "190327-09:55:54,369 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:55:54.368168:reading from /autofs/cluster/animal/scan_data/leads/LEADS/LDS3600068_20190325/Accelerated_Sagittal_MPRAGE/2019-03-25_09_42_12.0/S809792/LEADS_LDS3600068_MR_Accelerated_Sagittal_MPRAGE__br_raw_20190325145425343_187_S809792_I1146946.dcm...\n",
      "190327-09:55:54,370 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:55:54.370333:Getting Series No \n",
      "190327-09:55:54,371 nipype.interface INFO:\n",
      "\t stderr 2019-03-27T09:55:54.371726:INFO: Found 210 files in /autofs/cluster/animal/scan_data/leads/LEADS/LDS3600068_20190325/Accelerated_Sagittal_MPRAGE/2019-03-25_09_42_12.0/S809792\n",
      "190327-09:55:54,372 nipype.interface INFO:\n",
      "\t stderr 2019-03-27T09:55:54.371726:INFO: Scanning for Series Number 2\n",
      "190327-09:55:54,373 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:55:54.373701:Scanning Directory \n",
      "190327-09:55:55,500 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:55:55.500686:INFO: loading series header info.\n",
      "190327-09:55:55,502 nipype.interface INFO:\n",
      "\t stderr 2019-03-27T09:55:55.502727:INFO: found 208 files in series\n",
      "190327-09:56:16,89 nipype.interface INFO:\n",
      "\t stderr 2019-03-27T09:56:16.089468:\n",
      "190327-09:56:16,90 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.090681:INFO: sorting.\n",
      "190327-09:56:16,91 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.090681:sdfiSameSlicePos() eps = 0.000001\n",
      "190327-09:56:16,92 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.090681:INFO: (240 256 208), nframes = 1, ismosaic=0\n",
      "190327-09:56:16,93 nipype.interface INFO:\n",
      "\t stderr 2019-03-27T09:56:16.093854:RunNo = 1\n",
      "190327-09:56:16,97 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.097577:sdfi->UseSliceScaleFactor 0\n",
      "190327-09:56:16,98 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.097577:datatype = 4, short=4, float=3\n",
      "190327-09:56:16,100 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.097577:PE Dir ROW ROW\n",
      "190327-09:56:16,107 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.107780:AutoAlign matrix detected \n",
      "190327-09:56:16,117 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.117021:AutoAlign Matrix --------------------- \n",
      "190327-09:56:16,118 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.117021: 1.00000   0.00000   0.00000   0.00000;\n",
      "190327-09:56:16,119 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.117021: 0.00000   1.00000   0.00000   0.00000;\n",
      "190327-09:56:16,121 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.117021: 0.00000   0.00000   1.00000   0.00000;\n",
      "190327-09:56:16,123 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.117021: 0.00000   0.00000   0.00000   1.00000;\n",
      "190327-09:56:16,150 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\n",
      "190327-09:56:16,152 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:FileName \t\t/autofs/cluster/animal/scan_data/leads/LEADS/LDS3600068_20190325/Accelerated_Sagittal_MPRAGE/2019-03-25_09_42_12.0/S809792/LEADS_LDS3600068_MR_Accelerated_Sagittal_MPRAGE__br_raw_20190325145517731_1_S809792_I1146946.dcm\n",
      "190327-09:56:16,153 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:Identification\n",
      "190327-09:56:16,154 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tNumarisVer        syngo MR E11\n",
      "190327-09:56:16,156 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tScannerModel      Prisma_fit\n",
      "190327-09:56:16,157 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tPatientName       DE-IDENTIFIED \n",
      "190327-09:56:16,158 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:Date and time\n",
      "190327-09:56:16,159 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tStudyDate         20190325\n",
      "190327-09:56:16,160 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tStudyTime         094212.338000 \n",
      "190327-09:56:16,161 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tSeriesTime        100103.501000 \n",
      "190327-09:56:16,162 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tAcqTime           095534.127500 \n",
      "190327-09:56:16,164 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:Acquisition parameters\n",
      "190327-09:56:16,165 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tPulseSeq          *tfl3d1_16ns\n",
      "190327-09:56:16,166 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tProtocol          Accelerated Sagittal MPRAGE \n",
      "190327-09:56:16,167 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tPhEncDir          ROW\n",
      "190327-09:56:16,168 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tEchoNo            1\n",
      "190327-09:56:16,169 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tFlipAngle         9\n",
      "190327-09:56:16,170 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tEchoTime          2.98\n",
      "190327-09:56:16,172 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tInversionTime     900\n",
      "190327-09:56:16,173 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tRepetitionTime    2300\n",
      "190327-09:56:16,174 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tPhEncFOV          240\n",
      "190327-09:56:16,175 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tReadoutFOV        256\n",
      "190327-09:56:16,176 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:Image information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190327-09:56:16,177 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tRunNo             1\n",
      "190327-09:56:16,179 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tSeriesNo          2\n",
      "190327-09:56:16,180 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tImageNo           1\n",
      "190327-09:56:16,181 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tNImageRows        256\n",
      "190327-09:56:16,182 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tNImageCols        240\n",
      "190327-09:56:16,183 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tNFrames           1\n",
      "190327-09:56:16,184 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tSliceArraylSize   1\n",
      "190327-09:56:16,185 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tIsMosaic          0\n",
      "190327-09:56:16,186 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tImgPos            123.8390 139.6610 141.1186 \n",
      "190327-09:56:16,187 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tVolRes              1.0000   1.0000   1.0000 \n",
      "190327-09:56:16,189 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tVolDim            240      256      208 \n",
      "190327-09:56:16,190 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tVc                 -0.0000  -1.0000   0.0000 \n",
      "190327-09:56:16,191 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tVr                 -0.0000  -0.0000  -1.0000 \n",
      "190327-09:56:16,192 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tVs                 -1.0000  -0.0000   0.0000 \n",
      "190327-09:56:16,193 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tVolCenter          19.8390  19.6610  13.1186 \n",
      "190327-09:56:16,194 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:\tTransferSyntaxUID 1.2.840.10008.1.2.1\n",
      "190327-09:56:16,196 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:UseSliceScaleFactor 0 (slice 0: 1)\n",
      "190327-09:56:16,197 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:16.150673:IsDWI = 0\n",
      "190327-09:56:20,168 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:20.168063:INFO: no Siemens slice order reversal detected (good!). \n",
      "190327-09:56:20,169 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:20.168063:TR=2300.00, TE=2.98, TI=900.00, flip angle=9.00\n",
      "190327-09:56:20,171 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:20.168063:i_ras = (-0, -1, 0)\n",
      "190327-09:56:20,173 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:20.168063:j_ras = (-0, -0, -1)\n",
      "190327-09:56:20,174 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:20.168063:k_ras = (-1, -0, 0)\n",
      "190327-09:56:20,176 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:20.168063:writing to /autofs/cluster/animal/scan_data/leads/recon_nip/LDS3600068_20190325//mri/orig/001.mgz...\n",
      "190327-09:56:20,269 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CONVERT2MGZ\".\n",
      "190327-09:56:20,270 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.UNPACK\" in \"/tmp/tmpuhp0t3c6/leads_workflow/_subject_name_LDS3600068_20190325/UNPACK\".\n",
      "190327-09:56:20,277 nipype.workflow INFO:\n",
      "\t [Node] Running \"UNPACK\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:56:42,142 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.UNPACK\".\n",
      "190327-09:56:42,145 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.SCAN_AND_LOG\" in \"/tmp/tmp_geg_nxg/leads_workflow/_subject_name_LDS3600068_20190325/SCAN_AND_LOG\".\n",
      "190327-09:56:42,175 nipype.workflow INFO:\n",
      "\t [Node] Running \"SCAN_AND_LOG\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:56:42,198 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.SCAN_AND_LOG\".\n",
      "190327-09:56:42,199 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.RECON_JOB\" in \"/tmp/tmp7b73cl_b/leads_workflow/_subject_name_LDS3600068_20190325/RECON_JOB\".\n",
      "190327-09:56:42,206 nipype.workflow INFO:\n",
      "\t [Node] Running \"RECON_JOB\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:56:54,599 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.RECON_JOB\".\n",
      "190327-09:56:54,604 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.FS_DETAILS\" in \"/tmp/tmpidtfafbd/leads_workflow/_subject_name_LDS3600068_20190325/FS_DETAILS\".\n",
      "190327-09:56:54,613 nipype.workflow INFO:\n",
      "\t [Node] Running \"FS_DETAILS\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "LDS3600068_20190325 Recon either already organized or not ready yet.\n",
      "190327-09:56:54,628 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.FS_DETAILS\".\n",
      "190327-09:56:54,630 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.MAKE_ORIGINAL_DIR\" in \"/tmp/tmpuy237i_g/leads_workflow/_subject_name_LDS3600068_20190325/MAKE_ORIGINAL_DIR\".\n",
      "190327-09:56:54,639 nipype.workflow INFO:\n",
      "\t [Node] Running \"MAKE_ORIGINAL_DIR\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:56:54,653 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.MAKE_ORIGINAL_DIR\".\n",
      "190327-09:56:54,654 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.PREPARE_MANEDITS\" in \"/tmp/tmpwbvslv0c/leads_workflow/_subject_name_LDS3600068_20190325/PREPARE_MANEDITS\".\n",
      "190327-09:56:54,661 nipype.workflow INFO:\n",
      "\t [Node] Running \"PREPARE_MANEDITS\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:56:54,893 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.PREPARE_MANEDITS\".\n",
      "190327-09:56:54,895 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.DATASINK\" in \"/tmp/tmpdt_olzv7/leads_workflow/_subject_name_LDS3600068_20190325/DATASINK\".\n",
      "190327-09:56:54,905 nipype.workflow INFO:\n",
      "\t [Node] Running \"DATASINK\" (\"nipype.interfaces.io.DataSink\")\n",
      "190327-09:56:54,913 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.DATASINK\".\n",
      "190327-09:56:54,915 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.IMPORT_LONI_INFO\" in \"/tmp/tmpa4gbggsw/leads_workflow/_subject_name_LDS3600068_20190325/IMPORT_LONI_INFO\".\n",
      "190327-09:56:54,923 nipype.workflow INFO:\n",
      "\t [Node] Running \"IMPORT_LONI_INFO\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:56:54,972 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.IMPORT_LONI_INFO\".\n",
      "190327-09:56:54,974 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CREATEDIR\" in \"/tmp/tmpetxyy3_q/leads_workflow/_subject_name_LDS3600056_20190323/CREATEDIR\".\n",
      "190327-09:56:54,981 nipype.workflow INFO:\n",
      "\t [Node] Running \"CREATEDIR\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:56:55,4 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CREATEDIR\".\n",
      "190327-09:56:55,5 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CONVERT2MGZ\" in \"/tmp/tmpl22u80my/leads_workflow/_subject_name_LDS3600056_20190323/CONVERT2MGZ\".\n",
      "190327-09:56:55,17 nipype.workflow INFO:\n",
      "\t [Node] Running \"CONVERT2MGZ\" (\"nipype.interfaces.freesurfer.preprocess.MRIConvert\"), a CommandLine Interface with command:\n",
      "mri_convert --out_type mgz --input_volume /autofs/cluster/animal/scan_data/leads/LEADS/LDS3600056_20190323/Accelerated_Sagittal_MPRAGE/2019-03-23_11_02_04.0/S810079/LEADS_LDS3600056_MR_Accelerated_Sagittal_MPRAGE__br_raw_20190326132346157_111_S810079_I1147398.dcm --output_volume /autofs/cluster/animal/scan_data/leads/recon_nip/LDS3600056_20190323//mri/orig/001.mgz\n",
      "190327-09:56:55,70 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:55.070833:mri_convert.bin --out_type mgz --input_volume /autofs/cluster/animal/scan_data/leads/LEADS/LDS3600056_20190323/Accelerated_Sagittal_MPRAGE/2019-03-23_11_02_04.0/S810079/LEADS_LDS3600056_MR_Accelerated_Sagittal_MPRAGE__br_raw_20190326132346157_111_S810079_I1147398.dcm --output_volume /autofs/cluster/animal/scan_data/leads/recon_nip/LDS3600056_20190323//mri/orig/001.mgz \n",
      "190327-09:56:55,76 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:55.076279:$Id: mri_convert.c,v 1.226 2016/02/26 16:15:24 mreuter Exp $\n",
      "190327-09:56:55,77 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:55.076279:reading from /autofs/cluster/animal/scan_data/leads/LEADS/LDS3600056_20190323/Accelerated_Sagittal_MPRAGE/2019-03-23_11_02_04.0/S810079/LEADS_LDS3600056_MR_Accelerated_Sagittal_MPRAGE__br_raw_20190326132346157_111_S810079_I1147398.dcm...\n",
      "190327-09:56:55,78 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:55.078725:Getting Series No \n",
      "190327-09:56:55,80 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:55.080891:Scanning Directory \n",
      "190327-09:56:55,81 nipype.interface INFO:\n",
      "\t stderr 2019-03-27T09:56:55.081845:INFO: Found 210 files in /autofs/cluster/animal/scan_data/leads/LEADS/LDS3600056_20190323/Accelerated_Sagittal_MPRAGE/2019-03-23_11_02_04.0/S810079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190327-09:56:55,83 nipype.interface INFO:\n",
      "\t stderr 2019-03-27T09:56:55.081845:INFO: Scanning for Series Number 12\n",
      "190327-09:56:56,101 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:56:56.101430:INFO: loading series header info.\n",
      "190327-09:56:56,103 nipype.interface INFO:\n",
      "\t stderr 2019-03-27T09:56:56.103057:INFO: found 208 files in series\n",
      "190327-09:57:16,683 nipype.interface INFO:\n",
      "\t stderr 2019-03-27T09:57:16.683443:\n",
      "190327-09:57:16,684 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.684695:INFO: sorting.\n",
      "190327-09:57:16,685 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.684695:sdfiSameSlicePos() eps = 0.000001\n",
      "190327-09:57:16,686 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.684695:INFO: (240 256 208), nframes = 1, ismosaic=0\n",
      "190327-09:57:16,687 nipype.interface INFO:\n",
      "\t stderr 2019-03-27T09:57:16.687477:RunNo = 11\n",
      "190327-09:57:16,691 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.691168:sdfi->UseSliceScaleFactor 0\n",
      "190327-09:57:16,692 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.691168:datatype = 4, short=4, float=3\n",
      "190327-09:57:16,694 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.691168:PE Dir ROW ROW\n",
      "190327-09:57:16,702 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.702452:AutoAlign matrix detected \n",
      "190327-09:57:16,711 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.711685:AutoAlign Matrix --------------------- \n",
      "190327-09:57:16,712 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.711685: 1.00000   0.00000   0.00000   0.00000;\n",
      "190327-09:57:16,713 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.711685: 0.00000   1.00000   0.00000   0.00000;\n",
      "190327-09:57:16,714 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.711685: 0.00000   0.00000   1.00000   0.00000;\n",
      "190327-09:57:16,715 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.711685: 0.00000   0.00000   0.00000   1.00000;\n",
      "190327-09:57:16,743 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\n",
      "190327-09:57:16,745 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:FileName \t\t/autofs/cluster/animal/scan_data/leads/LEADS/LDS3600056_20190323/Accelerated_Sagittal_MPRAGE/2019-03-23_11_02_04.0/S810079/LEADS_LDS3600056_MR_Accelerated_Sagittal_MPRAGE__br_raw_20190326132425689_1_S810079_I1147398.dcm\n",
      "190327-09:57:16,745 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:Identification\n",
      "190327-09:57:16,746 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tNumarisVer        syngo MR E11\n",
      "190327-09:57:16,747 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tScannerModel      Prisma_fit\n",
      "190327-09:57:16,748 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tPatientName       DE-IDENTIFIED \n",
      "190327-09:57:16,749 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:Date and time\n",
      "190327-09:57:16,749 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tStudyDate         20190323\n",
      "190327-09:57:16,750 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tStudyTime         110204.098000 \n",
      "190327-09:57:16,751 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tSeriesTime        121709.435000 \n",
      "190327-09:57:16,752 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tAcqTime           121135.287500 \n",
      "190327-09:57:16,753 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:Acquisition parameters\n",
      "190327-09:57:16,754 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tPulseSeq          *tfl3d1_16ns\n",
      "190327-09:57:16,755 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tProtocol          Accelerated Sagittal MPRAGE \n",
      "190327-09:57:16,755 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tPhEncDir          ROW\n",
      "190327-09:57:16,756 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tEchoNo            1\n",
      "190327-09:57:16,757 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tFlipAngle         9\n",
      "190327-09:57:16,758 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tEchoTime          2.98\n",
      "190327-09:57:16,759 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tInversionTime     900\n",
      "190327-09:57:16,760 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tRepetitionTime    2300\n",
      "190327-09:57:16,760 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tPhEncFOV          240\n",
      "190327-09:57:16,761 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tReadoutFOV        256\n",
      "190327-09:57:16,762 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:Image information\n",
      "190327-09:57:16,763 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tRunNo             11\n",
      "190327-09:57:16,763 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tSeriesNo          12\n",
      "190327-09:57:16,764 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tImageNo           1\n",
      "190327-09:57:16,765 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tNImageRows        256\n",
      "190327-09:57:16,766 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tNImageCols        240\n",
      "190327-09:57:16,766 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tNFrames           1\n",
      "190327-09:57:16,767 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tSliceArraylSize   1\n",
      "190327-09:57:16,768 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tIsMosaic          0\n",
      "190327-09:57:16,769 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tImgPos            108.9237 133.5593 151.9661 \n",
      "190327-09:57:16,770 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tVolRes              1.0000   1.0000   1.0000 \n",
      "190327-09:57:16,771 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tVolDim            240      256      208 \n",
      "190327-09:57:16,772 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tVc                 -0.0000  -1.0000   0.0000 \n",
      "190327-09:57:16,774 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tVr                 -0.0000  -0.0000  -1.0000 \n",
      "190327-09:57:16,774 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tVs                 -1.0000  -0.0000   0.0000 \n",
      "190327-09:57:16,775 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tVolCenter           4.9237  13.5593  23.9661 \n",
      "190327-09:57:16,776 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:\tTransferSyntaxUID 1.2.840.10008.1.2.1\n",
      "190327-09:57:16,777 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:UseSliceScaleFactor 0 (slice 0: 1)\n",
      "190327-09:57:16,777 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:16.743803:IsDWI = 0\n",
      "190327-09:57:20,675 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:20.675897:INFO: no Siemens slice order reversal detected (good!). \n",
      "190327-09:57:20,677 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:20.675897:TR=2300.00, TE=2.98, TI=900.00, flip angle=9.00\n",
      "190327-09:57:20,678 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:20.675897:i_ras = (-0, -1, 0)\n",
      "190327-09:57:20,679 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:20.675897:j_ras = (-0, -0, -1)\n",
      "190327-09:57:20,679 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:20.675897:k_ras = (-1, -0, 0)\n",
      "190327-09:57:20,680 nipype.interface INFO:\n",
      "\t stdout 2019-03-27T09:57:20.675897:writing to /autofs/cluster/animal/scan_data/leads/recon_nip/LDS3600056_20190323//mri/orig/001.mgz...\n",
      "190327-09:57:20,760 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CONVERT2MGZ\".\n",
      "190327-09:57:20,762 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.UNPACK\" in \"/tmp/tmpulom05mj/leads_workflow/_subject_name_LDS3600056_20190323/UNPACK\".\n",
      "190327-09:57:20,767 nipype.workflow INFO:\n",
      "\t [Node] Running \"UNPACK\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:57:20,785 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.UNPACK\".\n",
      "190327-09:57:20,786 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.SCAN_AND_LOG\" in \"/tmp/tmpkhx3sba0/leads_workflow/_subject_name_LDS3600056_20190323/SCAN_AND_LOG\".\n",
      "190327-09:57:20,811 nipype.workflow INFO:\n",
      "\t [Node] Running \"SCAN_AND_LOG\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:57:20,832 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.SCAN_AND_LOG\".\n",
      "190327-09:57:20,833 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.RECON_JOB\" in \"/tmp/tmp83589bjd/leads_workflow/_subject_name_LDS3600056_20190323/RECON_JOB\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190327-09:57:20,839 nipype.workflow INFO:\n",
      "\t [Node] Running \"RECON_JOB\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:57:32,451 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.RECON_JOB\".\n",
      "190327-09:57:32,454 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.FS_DETAILS\" in \"/tmp/tmp4kflo1n6/leads_workflow/_subject_name_LDS3600056_20190323/FS_DETAILS\".\n",
      "190327-09:57:32,463 nipype.workflow INFO:\n",
      "\t [Node] Running \"FS_DETAILS\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "LDS3600056_20190323 Recon either already organized or not ready yet.\n",
      "190327-09:57:32,476 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.FS_DETAILS\".\n",
      "190327-09:57:32,477 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.MAKE_ORIGINAL_DIR\" in \"/tmp/tmppwyoaq9k/leads_workflow/_subject_name_LDS3600056_20190323/MAKE_ORIGINAL_DIR\".\n",
      "190327-09:57:32,486 nipype.workflow INFO:\n",
      "\t [Node] Running \"MAKE_ORIGINAL_DIR\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:57:32,497 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.MAKE_ORIGINAL_DIR\".\n",
      "190327-09:57:32,499 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.PREPARE_MANEDITS\" in \"/tmp/tmpsduc31l2/leads_workflow/_subject_name_LDS3600056_20190323/PREPARE_MANEDITS\".\n",
      "190327-09:57:32,507 nipype.workflow INFO:\n",
      "\t [Node] Running \"PREPARE_MANEDITS\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:57:32,669 nipype.workflow WARNING:\n",
      "\t [Node] Error on \"leads_workflow.PREPARE_MANEDITS\" (/tmp/tmpsduc31l2/leads_workflow/_subject_name_LDS3600056_20190323/PREPARE_MANEDITS)\n",
      "190327-09:57:32,675 nipype.workflow ERROR:\n",
      "\t Node PREPARE_MANEDITS.a0 failed to run on host nimbus.nmr.mgh.harvard.edu.\n",
      "190327-09:57:32,684 nipype.workflow ERROR:\n",
      "\t Saving crash info to /autofs/homes/002/rje11/crash-20190327-095732-rje11-PREPARE_MANEDITS.a0-2b579375-9915-499b-a778-e2bda188173c.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\", line 44, in run\n",
      "    node.run(updatehash=updatehash)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 473, in run\n",
      "    result = self._run_interface(execute=True)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 557, in _run_interface\n",
      "    return self._run_command(execute)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 637, in _run_command\n",
      "    result = self._interface.run(cwd=outdir)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/interfaces/base/core.py\", line 511, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/interfaces/utility/wrappers.py\", line 144, in _run_interface\n",
      "    out = function_handle(**args)\n",
      "  File \"<string>\", line 9, in preparing_manedits\n",
      "  File \"/usr/pubsw/packages/python/anaconda3-5.2.0/lib/python3.6/shutil.py\", line 315, in copytree\n",
      "    os.makedirs(dst)\n",
      "  File \"/usr/pubsw/packages/python/anaconda3-5.2.0/lib/python3.6/os.py\", line 220, in makedirs\n",
      "    mkdir(name, mode)\n",
      "FileExistsError: [Errno 17] File exists: '/autofs/cluster/animal/scan_data/leads/recon_nip/LDS3600056_20190323/unedit.'\n",
      "\n",
      "190327-09:57:32,690 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.IMPORT_LONI_INFO\" in \"/tmp/tmpqvgyqywx/leads_workflow/_subject_name_LDS3600056_20190323/IMPORT_LONI_INFO\".\n",
      "190327-09:57:32,699 nipype.workflow INFO:\n",
      "\t [Node] Running \"IMPORT_LONI_INFO\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190327-09:57:32,746 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.IMPORT_LONI_INFO\".\n",
      "190327-09:57:32,747 nipype.workflow INFO:\n",
      "\t ***********************************\n",
      "190327-09:57:32,749 nipype.workflow ERROR:\n",
      "\t could not run node: leads_workflow.PREPARE_MANEDITS.a0\n",
      "190327-09:57:32,750 nipype.workflow INFO:\n",
      "\t crashfile: /autofs/homes/002/rje11/crash-20190327-095732-rje11-PREPARE_MANEDITS.a0-2b579375-9915-499b-a778-e2bda188173c.pklz\n",
      "190327-09:57:32,751 nipype.workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Workflow did not execute cleanly. Check log for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c00658ff177a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Execute your workflow in sequential way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# leads_workflow.run(run(plugin='MultiProc', plugin_args={'n_procs' : 2})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mleads_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mleads_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph2use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'flat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%dT%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_provenance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_callback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mreport_nodes_not_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nipype/pipeline/plugins/tools.py\u001b[0m in \u001b[0;36mreport_nodes_not_run\u001b[0;34m(notrun)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         raise RuntimeError(('Workflow did not execute cleanly. '\n\u001b[0m\u001b[1;32m     83\u001b[0m                             'Check log for details'))\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Workflow did not execute cleanly. Check log for details"
     ]
    }
   ],
   "source": [
    "# Execute your workflow in sequential way\n",
    "# leads_workflow.run(run(plugin='MultiProc', plugin_args={'n_procs' : 2})\n",
    "leads_workflow.run()\n",
    "\n",
    "leads_workflow.write_graph(graph2use='flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_workflow.write_graph(graph2use='flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectname = 'LDS3600068_20190325'\n",
    "glob.glob('/autofs/cluster/animal/scan_data/leads/recon_nip/'+subjectname+'/script*/') == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
