{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(NOTE: all of the following locations are referenced from leads home directory: /autofs/cluster/animal/scan_data/leads). This script first unfoils the dicom folder to identify any repeat MPRAGEs in ./LEADS. It will unpack the dicoms into an mgz that will go into the ./recon_nip/RECON_3T/ and ./recon_nip/RECON_FLAIR/ respective folders and start the initial recons for the two pipelines. The scaninfo is unpacked from the dicoms and put into ./recon_nip/RECON_NOTES folder and scan notes downloaded from LONI and put into ./spreadsheets/ which will also be put into the database in recon_nip/RECON_NOTES/? As you update and QC the recons (and rerun them as you edit -- don't forget to include the FLAIR flag! Otherwise it will not apply. But you can always add the flair later as well as it is part of autorecon3 (postediting stage).\n",
    "\n",
    "REQUIREMENTS FOR THIS PIPELINE\n",
    "the recon output will be named e.g. \"unedit.FS6_02\". After manually editing please rename this recon folder to \"edit.FS6_02\" and start another recon manually. Then the next time this script is run, it will record the appropriate information into the databases.\n",
    "the following notes need to be updated upon each batch download so this info can be piped:\n",
    "\n",
    "        details:\n",
    "        MOST IMPORTANT: once a new download has been initiated, load the CSV download file into\n",
    "        /autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/ (REQUIRED!!!!)\n",
    "        if it is a new image collection on loni, it will create a separate CSV file, \n",
    "        otherwise will replace old one with concatenated data of that image collection. So archive\n",
    "        any csvs that are old versions of downloaded collections. But the code also does this automatically.\n",
    "        \n",
    "        Also, occasionally download Mayo_ADRIL_MRI_Quality_date.csv and AMYELG and xxxx into ./spreadsheets to\n",
    "        update database before analyses -- (e.g. we need the AMY info for this).\n",
    "        \n",
    "        Download and save to the following locations:\n",
    "        Mayo_ADRIL_MRI_Quality_date.csv to /autofs/cluster/animal/scan_data/leads/spreadsheets/MRIQUALITY\n",
    "        this file will replace the old one (concatenates all data on loni about QC).\n",
    "\n",
    "If any files need to be re-run / re-processed, delete the files in the folder. This pipeline does not overwrite anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# send a recon job- just do for ones that do not have output?\n",
    "# decide if I want to have the option to re-run edits manually of implement here?\n",
    "# way to automate csv download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import io, os, sys, types # needed\n",
    "import glob # needed\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode # needed\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "import nipype.interfaces.io as nio\n",
    "import nipype.pipeline.engine as pe\n",
    "from nipype.interfaces.freesurfer import MRIConvert\n",
    "from nipype.interfaces.freesurfer import ReconAll\n",
    "from nipype import config\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import pathlib\n",
    "import pydicom\n",
    "from pydicom.tag import Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and update spreadsheets\n",
    "\n",
    "# these do not concatenate, must do this\n",
    "downloadsdir = '/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/'\n",
    "\n",
    "# vertically concatenate all csvs\n",
    "downloadlist = glob.glob(downloadsdir+'*.csv')\n",
    "downloadlist.remove('/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv')\n",
    "\n",
    "# vertically concatenate all csvs\n",
    "combined_csv = pd.concat( [ pd.read_csv(f) for f in downloadlist ] , sort=False)\n",
    "\n",
    "# drop all non-MPRAGES, sort dataframe by subject column, drop all duplicates\n",
    "combined_csv_index = combined_csv.Description.str.contains('Accelerated Sagittal MPRAGE|Sagittal 3D Accelerated MPRAGE', regex=True)\n",
    "combined_csv['keep'] = combined_csv_index\n",
    "combined_csv = combined_csv[combined_csv.keep == True]\n",
    "\n",
    "combined_csv = combined_csv.sort_values(by=['Downloaded'])\n",
    "combined_csv = combined_csv.drop_duplicates(['Image Data ID'], keep='last')\n",
    "\n",
    "del combined_csv['keep']\n",
    "\n",
    "# # save combined download file\n",
    "combined_csv.to_csv(\"/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv\", index=False,)\n",
    "\n",
    "# # these download already concatenating all sessions ; use latest\n",
    "qualitydir = '/autofs/cluster/animal/scan_data/leads/spreadsheets/MRIQUALITY/'\n",
    "\n",
    "list_of_qualityfiles = glob.glob(qualitydir+'*.csv')\n",
    "MRIQUALITY = max(list_of_qualityfiles, key=os.path.getctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(subject):\n",
    "    subsessions = glob.glob(dicomdir+subject+'/Accelerated_Sagittal_MPRAGE/*/*/')\n",
    "    mprage_name = 'Accelerated_Sagittal_MPRAGE'\n",
    "    if subsessions == []:\n",
    "        subsessions = glob.glob(dicomdir+subject+'/Sagittal_3D_Accelerated_MPRAGE/*/*/')\n",
    "        mprage_name = 'Sagittal_3D_Accelerated_MPRAGE'\n",
    "    repeat_tag = '-'\n",
    "    print(mprage_name)\n",
    "    print(subsessions)\n",
    "    for num in range(len(subsessions)):\n",
    "        # look to see if more than one session on the same date\n",
    "        # then look to see if more than one date (or both)\n",
    "        parentfolder = subsessions[num].split('/')[8]\n",
    "        filename = os.listdir(subsessions[num])[0] # dicom name to extract date\n",
    "        #extract date from dicom:\n",
    "        ds = pydicom.read_file(subsessions[num]+'/'+filename)\n",
    "        #print(ds)\n",
    "        try:\n",
    "            date = str(ds[0x08, 0x22].value)\n",
    "        except(KeyError):\n",
    "            date = str(ds[0x08, 0x21].value)\n",
    "        print(date)\n",
    "        if num == len(subsessions)-1:\n",
    "            namedate = dicomdir+subject+'_'+date\n",
    "            os.rename(dicomdir+subject, namedate)\n",
    "        else:\n",
    "            namedateplus = dicomdir+subject+'_'+date+repeat_tag+'/'+mprage_name+'/'+parentfolder\n",
    "            pathlib.Path(namedateplus).mkdir(parents=True, exist_ok=True)\n",
    "            shutil.move(subsessions[num], namedateplus)\n",
    "            if not os.listdir(dicomdir+subject+'/'+mprage_name+'/'+parentfolder):\n",
    "                os.rmdir(dicomdir+subject+'/'+mprage_name+'/'+parentfolder)\n",
    "            repeat_tag = repeat_tag+'-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sagittal_3D_Accelerated_MPRAGE\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# specify variables\n",
    "leadsdir = '/cluster/animal/scan_data/leads/'\n",
    "os.chdir(leadsdir)\n",
    "dicomdir = \"/cluster/animal/scan_data/leads/LEADS/\"\n",
    "unpacklog = \"/autofs/cluster/animal/scan_data/leads/recon_nip/SCAN_NOTES/unpack.log\"\n",
    "recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_FLAIR/'\n",
    "recondir_3t = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_3T/'\n",
    "recondir_edit = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_FLAIR_EDITED/'\n",
    "folders = [x for x in os.listdir(dicomdir) if not x.startswith(\".\")]\n",
    "subjlist = [f for f in os.listdir(dicomdir) if ((\"_\") not in f) and (not f.startswith('.') and (\"duplicate\" not in f))]\n",
    "\n",
    "# wipe clean batch.recon.list\n",
    "open('/autofs/cluster/animal/scan_data/leads/recon_nip/batch.recon.list', 'w').close()\n",
    "\n",
    "# unwravel all subjects with multiple sessions and rename to include date\n",
    "for sub in subjlist:\n",
    "    scan(sub)\n",
    "\n",
    "# now just make a list of subjects by ID (this will be the input to the nodes)\n",
    "# will define dicom path within node\n",
    "sh_dicomlist = [f for f in os.listdir(dicomdir) if ((\"_\" in f) and (\"REPEAT_RUNS\" not in f))]\n",
    "\n",
    "# define workflow\n",
    "leads_workflow = Workflow(name='leads_workflow') #, base_dir = '/autofs/cluster/animal/scan_data/leads/recon_nip/') # add  base_dir='/shared_fs/nipype_scratch'\n",
    "\n",
    "# configure to stop on first crash\n",
    "cfg = dict(execution={'stop_on_first_crash': True})\n",
    "config.update_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sh_dicomlist = [x for x in os.listdir('/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_FLAIR') if x.startswith(\"LDS\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging\n",
    "sh_dicomlist = ['LDS0180106_20190621']\n",
    "#['LDS0220154_20190812','LDS3600140_20190813'] #,'\n",
    "\n",
    "#['LDS0730144_20190801','LDS0730150_20190731','LDS3600119_20190807','LDS0180153_20190806']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST NODE : PASSSWORDS\n",
    "\n",
    "def credentials(): # combined with find_dicom\n",
    "    import getpass\n",
    "    USER = getpass.getuser()\n",
    "    print('Please enter your PASSWORD for launchpad access: ')\n",
    "    PASS= getpass.getpass()\n",
    "    return USER, PASS\n",
    "\n",
    "PASSWORDS = pe.Node(Function(input_names=[\"user\", \"pw\"],\n",
    "                         output_names=[\"USER\",\"PASS\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=credentials),\n",
    "                        name='PASSWORDS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : CREATEDIR\n",
    "def createdir(val, USER, PASS):\n",
    "    import os\n",
    "    import re\n",
    "    import glob\n",
    "    import pydicom\n",
    "    from pydicom.tag import Tag\n",
    "    val = val.split('/')[-1]\n",
    "    dicomdir = \"/autofs/cluster/animal/scan_data/leads/LEADS/\"\n",
    "    pipelines = ['RECON_3T', 'RECON_FLAIR']\n",
    "    for pipe in pipelines:\n",
    "        recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'+pipe+'/' \n",
    "        reconpath = recondir+val+'/'\n",
    "        imgpath = reconpath+'mri/orig/'\n",
    "        if not glob.glob(reconpath + '/**/*mri', recursive=True): # changed recondir to reconpath\n",
    "            os.makedirs(imgpath) # edited this part!\n",
    "    dumplocation = imgpath+'001.mgz'\n",
    "    flairdumplocation = imgpath+'FLAIR.mgz'\n",
    "    subject = val.split('_')[0]\n",
    "    mprage_names = ['Accelerated_Sagittal_MPRAGE','Sagittal_3D_Accelerated_MPRAGE'] # nomenclature differs\n",
    "    for name in mprage_names:\n",
    "        try:\n",
    "            MPRAGE_path = glob.glob(dicomdir+val+'/'+name+'/*/*')[0]\n",
    "            pickdicom = glob.glob(dicomdir+val+'/'+name+'/*/*/*')[0]\n",
    "        except(IndexError):\n",
    "            pass\n",
    "    pickflair = glob.glob(dicomdir+val+'/Sagittal_3D_FLAIR/*/*/*')[0]\n",
    "    ds = pydicom.read_file(pickdicom)\n",
    "    try:\n",
    "        date = str(ds[0x08, 0x22].value)\n",
    "    except(KeyError):\n",
    "        date = str(ds[0x08, 0x21].value)\n",
    "    sessionid = MPRAGE_path.split(\"/\")[-1]\n",
    "    return reconpath, MPRAGE_path, pickdicom, dumplocation, recondir, USER, PASS, imgpath, date, flairdumplocation, pickflair\n",
    "        \n",
    "CREATEDIR = pe.Node(Function(input_names=[\"val\", \"USER\", \"PASS\"],\n",
    "                         output_names=[\"createdir_out1\",\"createdir_out2\", \"createdir_out3\", \"createdir_out4\", \"createdir_out5\", \"USER\", \"PASS\", \"createdir_out6\", \"date\", \"flairdumplocation\",\"pickflair\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=createdir),\n",
    "                        name='CREATEDIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : IMPORT_LONI_INFO\n",
    "\n",
    "def import_loni_notes(dicomname, date, subjectdir):\n",
    "    import pandas as pd\n",
    "    import glob\n",
    "    import os\n",
    "    import re\n",
    "    # download info\n",
    "    download_df = pd.read_csv('/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv')\n",
    "    recon_dir = '/autofs/cluster/animal/scan_data/leads/recon_nip/SCAN_NOTES/'\n",
    "    mgh_subs = '/autofs/cluster/animal/scan_data/leads/spreadsheets/IDENTIFICATION/MGH_SUBJECTS.tsv'\n",
    "    notes_dir = recon_dir+subjectdir.split('/')[-2]\n",
    "    dicom = dicomname.split(\"/\")[-1]\n",
    "    subid = dicom.split(\"_\")[1]\n",
    "    imageid = dicom.split(\"_\")[12][1:-4]\n",
    "    scannotes_df = pd.read_csv(notes_dir+'/scannotes.csv')\n",
    "    try:\n",
    "        download_date = download_df.loc[download_df['Image Data ID'] == float(imageid), 'Downloaded'].values[0]\n",
    "    except(IndexError):\n",
    "        pass\n",
    "    except(UnboundLocalError):\n",
    "        raise Exception('LONI download information for one or more included subjects cannot be located. Please update spreadsheet in LONI_DOWNLOADS. For more information about this, read comments in beginning of this script.')\n",
    "    # loni notes\n",
    "    qualitydir = '/autofs/cluster/animal/scan_data/leads/spreadsheets/MRIQUALITY/'\n",
    "    list_qc_files = glob.glob(qualitydir+'*.csv')\n",
    "    MRIQUALITY = max(list_qc_files, key=os.path.getctime)\n",
    "    # these download already concatenating all sessions ; use latest\n",
    "    quality_df = pd.read_csv(MRIQUALITY)\n",
    "    try:\n",
    "        loni_overallpass = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_overallpass'].values[0]\n",
    "        if loni_overallpass == 1:\n",
    "            qc_pass = '1'\n",
    "        elif loni_overallpass == 4:\n",
    "            qc_pass = '0'\n",
    "        else:\n",
    "            qc_pass = ''\n",
    "        study_comments = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_comments'].values[0]\n",
    "        study_protocol_comment = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_protocol_comment'].values[0]\n",
    "        protocol_comments = quality_df.loc[quality_df['loni_image'] == float(imageid), 'protocol_comments'].values[0]\n",
    "        series_comments = quality_df.loc[quality_df['loni_image'] == float(imageid), 'series_comments'].values[0]\n",
    "        series_quality = quality_df.loc[quality_df['loni_image'] == float(imageid), 'series_quality'].values[0]# if 3, needs review; if 2 it is ok\n",
    "        if series_quality == 2:\n",
    "            s_quality = 'Scan quality is acceptable according to MAYO. '\n",
    "        elif series_quality == 3:\n",
    "            s_quality = 'Scan quality is questionable according to MAYO and needs review. '\n",
    "        elif series_quality == 4:\n",
    "            s_quality = 'Scan quality is poor according to MAYO and needs review. '\n",
    "        else:\n",
    "            s_quality = 'No scan quality data recorded from MAYO. '\n",
    "        study_rescan_requested = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_rescan_requested'].values[0]\n",
    "        if study_rescan_requested == 'TRUE':\n",
    "            rescan_requested = ' Study rescan has been requested. '\n",
    "        else:\n",
    "            rescan_requested = '. No study rescans have been requested. '\n",
    "\n",
    "        # delete duplicates within list, delete nans\n",
    "        comments_list = [s_quality,str(study_comments),str(study_protocol_comment),str(series_comments),str(protocol_comments),rescan_requested]\n",
    "        cleanedList = [x for x in comments_list if (x != 'nan')]\n",
    "        concat_comments = ''.join(cleanedList)+\" QC_pass from original site is \"+qc_pass+\" .\"\n",
    "        xnat_upload = '0'\n",
    "    except(IndexError):\n",
    "        if subid[0:6] == 'LDS360':   # if its MGH data # line 57\n",
    "            mgh_df = pd.read_csv(mgh_subs,index_col=False, sep='\\t')\n",
    "            concat_comments = mgh_df.loc[mgh_df['leads_sessionid'] == subid+'_'+date, 'notes'].values[0]\n",
    "            xnat_upload = mgh_df.loc[mgh_df['leads_sessionid'] == subid+'_'+date, 'XNAT_upload'].values[0]\n",
    "            if str(mgh_df.loc[mgh_df['leads_sessionid'] == subid+'_'+date, 'notes'].values[0]) == 'nan':\n",
    "                concat_comments = 'No comments from MAYO. '\n",
    "            qc_pass = \"No data.\"\n",
    "        else:\n",
    "            concat_comments = 'No comments from MAYO. '\n",
    "            qc_pass = 'No data.'\n",
    "            xnat_upload = '0'\n",
    "            \n",
    "    # add recon path by taking newest edit folder (if exists)\n",
    "    try:\n",
    "        edit_dir = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_FLAIR_EDITED/'+subjectdir.split('/')[-2]\n",
    "        edit_folders = [f for f in os.listdir(edit_dir) if f.startswith(\"edit.\")]\n",
    "        ext_folders = [edit_dir + s for s in edit_folders]\n",
    "        try:\n",
    "            recon_folder = max(ext_folders, key=os.path.getctime)\n",
    "        except(ValueError):\n",
    "            recon_folder = ''\n",
    "    except(FileNotFoundError):\n",
    "        recon_folder = ''\n",
    "        \n",
    "    scannotes_df.loc[scannotes_df.index[0], 'leads_id'] = subid+'_'+date\n",
    "    scannotes_df.loc[scannotes_df.index[0], 'mayo_overallpass'] = qc_pass\n",
    "    scannotes_df.loc[scannotes_df.index[0], 'mayo_notes'] = concat_comments\n",
    "    scannotes_df.loc[scannotes_df.index[0], 'download_date'] = download_date\n",
    "    scannotes_df.to_csv(notes_dir+'/scannotes.csv', index=False)\n",
    "    return dicomname, subjectdir, imageid\n",
    "    \n",
    "IMPORT_LONI_INFO = pe.Node(Function(input_names=[\"dicomname\", \"date\", \"subjectdir\"],\n",
    "                        output_names = [\"dicomname\", \"subjectdir\", \"imageid\"], \n",
    "                        function=import_loni_notes),\n",
    "                        name='IMPORT_LONI_INFO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : PREPARE_4_REDCAP\n",
    "# note that QC notes and status are manually recorded in scannotes\n",
    "\n",
    "def prepare_redcap(dicomname, subjectdir, imageid, pickdicom, fsversion):\n",
    "    import pandas as pd\n",
    "    import pydicom\n",
    "    from pydicom.tag import Tag\n",
    "    import csv\n",
    "    subject = subjectdir.split(\"/\")[-1]\n",
    "    convert_sex = '/autofs/cluster/animal/scan_data/leads/spreadsheets/IDENTIFICATION/DEMOGRAPHIC_IDS.csv'\n",
    "    demo_form = '/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv'\n",
    "    site_conversion = '/autofs/cluster/animal/scan_data/leads/spreadsheets/IDENTIFICATION/SITE_IDS.csv'\n",
    "    notes_dir = subjectdir.replace('RECON_FLAIR','SCAN_NOTES')\n",
    "    scannotes = pd.read_csv(notes_dir+'scannotes.csv')\n",
    "    download_df = pd.read_csv(demo_form)\n",
    "    reader = csv.reader(open(convert_sex))\n",
    "    ds = pydicom.read_file(pickdicom)\n",
    "    d={}\n",
    "    for row in reader:\n",
    "        d[row[0]]=row[1:][0]\n",
    "    sex = str(ds[0x10,0x40].value)\n",
    "    SEX = d.get(sex)\n",
    "    try:\n",
    "        AQ_DATE = ds[0x08,0x22].value\n",
    "    except(KeyError):\n",
    "        AQ_DATE = ds[0x08,0x21].value\n",
    "    age = ds[0x00101010].value\n",
    "    try:\n",
    "        AGE = str(int(age[:-1]))\n",
    "    except(ValueError):\n",
    "        AGE = ''\n",
    "    \n",
    "    try:\n",
    "        SITE = str(ds[0x08, 0x80].value) #Institution Name\n",
    "    except(KeyError):\n",
    "        d={}\n",
    "        reader = csv.reader(open(site_conversion))\n",
    "        for row in reader:\n",
    "            d[row[0]]=row[1:][0]\n",
    "        t = subject[3:6]\n",
    "        SITE = d.get(t)    \n",
    "\n",
    "    dicom_path = pickdicom.strip(pickdicom.split('/')[-1]) # or use scaninfo ?\n",
    "    GROUP = download_df.loc[download_df['Image Data ID'] == float(imageid), 'Group'].values[0]\n",
    "    GEN_NOTES = ''\n",
    "    RECON_PATH = '' #decide which recon to use\n",
    "    FS_VERSION = fsversion\n",
    "    \n",
    "    #Can be found in scannotes:\n",
    "    DN_DATE = scannotes.loc[scannotes.index[0], 'download_date']\n",
    "    acq_notes = scannotes.loc[scannotes.index[0], 'mayo_notes']\n",
    "\n",
    "\n",
    "PREPARE_4_REDCAP = pe.Node(Function(input_names=[\"dicomname\",\"subjectdir\", \"imageid\", \"pickdicom\", \"fsversion\"],\n",
    "                        function=prepare_redcap),\n",
    "                          name='PREPARE_4_REDCAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : UNPACK\n",
    "\n",
    "def unpack(subjectdir, MPRAGE_path, pickdicom):\n",
    "    from os import system\n",
    "    from os import path\n",
    "    from os import makedirs\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    import os.path\n",
    "    import pydicom\n",
    "    notesdir = subjectdir.replace('RECON_FLAIR',\"SCAN_NOTES\")\n",
    "    ## add header and save (if 8 columns, it's old type ; if 9 its new :: new can unpack GE and Philips and Siemens)\n",
    "    #old_unpack_sys_header = ['run/series number','protocol','error status','number of columns','number of rows','number of slices','number of frames','DICOM file']\n",
    "    new_unpack_sys_header = ['run/series number','protocol','echo time','repetition time', 'flip angle', 'unknown', 'In-Plane-Phase-Encoding Dir','pixel bandwidth','DICOM file', 'manufacturer']\n",
    "    if not path.exists(notesdir):\n",
    "        makedirs(notesdir)\n",
    "    if not os.path.isfile(notesdir+'scan.info'): \n",
    "        #cmdstring = 'unpacksdcmdir -src %s -targ %s -scanonly %s/scan.info' % (MPRAGE_path, notesdir, notesdir)\n",
    "        cmdstring = 'dcmunpack -src %s -scanonly %s/scan.info' % (MPRAGE_path,notesdir)\n",
    "        system(cmdstring)\n",
    "    if not os.path.isfile(notesdir+'scaninfo.csv'):\n",
    "        ds = pydicom.read_file(pickdicom)\n",
    "        MANU = str(ds[0x08, 0x70].value) # look at dicom manufacturer field\n",
    "        with open(notesdir+'/scan.info', 'r') as in_file:\n",
    "            for line in in_file:\n",
    "                editline = line.split()\n",
    "                editline.append(MANU)\n",
    "                with open(notesdir+'/scaninfo.csv', 'w') as result:\n",
    "                    wr = csv.writer(result, dialect='excel')\n",
    "                    wr.writerow(editline)\n",
    "                result.close()\n",
    "            in_file.close()\n",
    "        scaninfo = pd.read_csv(notesdir+'/scaninfo.csv', names=new_unpack_sys_header)\n",
    "        scaninfo.to_csv(notesdir+'/scaninfo.csv', index=False)\n",
    "    scan_info = notesdir+'/scaninfo.csv'\n",
    "    \n",
    "    subname = notesdir.split('/')[-2]\n",
    "    return subname, subjectdir, scan_info, MPRAGE_path, pickdicom\n",
    "\n",
    "UNPACK = pe.Node(Function(input_names=[\"subjectdir\",\"MPRAGE_path\", \"pickdicom\"],\n",
    "                         output_names=[\"unpack_out1\",\"unpack_out2\", \"unpack_out3\", \"unpack_out4\",\"unpack_out5\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=unpack),\n",
    "                        name='UNPACK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE CHECK DICOM INFO\n",
    "\n",
    "def check_info(subname, subjectdir, scan_info, MPRAGE_path, pickdicom):\n",
    "    import pandas as pd\n",
    "    import pydicom\n",
    "    notesdir = subjectdir.replace('RECON_FLAIR',\"SCAN_NOTES\")\n",
    "    scaninfo = pd.read_csv(notesdir+'scaninfo.csv')\n",
    "    # check to make sure TR is filled out (for some scans MGH function dcmunpack does not work)\n",
    "    if scaninfo.loc[0, \"echo time\"] == \"unknown\":\n",
    "        ds = pydicom.read_file(pickdicom)\n",
    "        series = ds[0x52009229].value\n",
    "        tmp = str(series[0]).split(\"\\n\")\n",
    "        for el in tmp:\n",
    "            if \"(0018, 0080)\" in el: # repetition time\n",
    "                TR = el.split(\"\\\"\")[1]\n",
    "            elif \"(0018, 1312)\" in el: # phase encoding dir\n",
    "                encoding_dir = el.split(\"\\'\")[1]\n",
    "            elif \"(0018, 1314)\" in el: #flip angle\n",
    "                flip_angle = el.split(\"\\\"\")[1]\n",
    "            elif \"(0018, 0095)\" in el:\n",
    "                pixel_band = el.split(\"\\\"\")[1]\n",
    "        image = ds[0x52009230].value\n",
    "        tmp2 = str(image[0]).split(\"\\n\")\n",
    "        for el in tmp2:\n",
    "            if \"(0018, 9082)\" in el:  # echo time\n",
    "                echo_time = str(el).split(\"FD: \")[1]\n",
    "        scaninfo['echo time'] = echo_time\n",
    "        scaninfo['repetition time'] = TR\n",
    "        scaninfo['flip angle'] = flip_angle\n",
    "        scaninfo['In-Plane-Phase-Encoding Dir'] = encoding_dir\n",
    "        scaninfo['pixel bandwidth'] = pixel_band\n",
    "        scaninfo.to_csv(notesdir+'/scaninfo.csv', index=False)\n",
    "    else:\n",
    "        pass\n",
    "    return subname, subjectdir, scan_info\n",
    "\n",
    "CHECKINFO = pe.Node(Function(input_names=[\"subname\", \"subjectdir\", \"scan_info\", \"MPRAGE_path\", \"pickdicom\"],\n",
    "                         output_names=[\"check_out1\",\"check_out2\", \"check_out3\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=check_info),\n",
    "                        name='CHECKINFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (first option) # # NODE : CONVERT2MGZ (only runs if .mgz is not available)\n",
    "\n",
    "def convert_dicom(in_file, out_file, reconpath):\n",
    "    import os\n",
    "    import glob\n",
    "    from os import system\n",
    "    #import time # just see if this works if waits\n",
    "    # check for a file called 001.mgz\n",
    "    if not glob.glob(reconpath + '/**/*001.mgz', recursive=True):\n",
    "        cmdstring = 'mri_convert %s %s' % (in_file, out_file)\n",
    "        system(cmdstring)\n",
    "        complete = 1\n",
    "    else:\n",
    "        complete = 1\n",
    "\n",
    "    return complete\n",
    "\n",
    "CONVERT2MGZ = pe.Node(Function(input_names=[\"in_file\", \"out_file\", \"reconpath\"],\n",
    "                         output_names=[\"out_file\"],\n",
    "                         function=convert_dicom),\n",
    "                        name='CONVERT2MGZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_flair(pickflair, flairdumplocation, reconpath, out_file):\n",
    "    import os\n",
    "    import glob\n",
    "    from os import system\n",
    "    import shutil\n",
    "    reconpath_3t = reconpath.replace('RECON_FLAIR','RECON_3T')\n",
    "\n",
    "    # put back after copying all flairs ad hoc\n",
    "    if not glob.glob(reconpath + '/**/*FLAIR.mgz', recursive=True):\n",
    "        cmdstring = 'mri_convert %s %s' % (pickflair, flairdumplocation)\n",
    "        system(cmdstring)\n",
    "        complete = 1\n",
    "    else:\n",
    "        complete = 1  \n",
    "    if not glob.glob(reconpath_3t + '/**/*001.mgz', recursive=True):\n",
    "        for root, dirs, files in os.walk(reconpath): \n",
    "            for file in files:  \n",
    "                if file == '001.mgz': \n",
    "                    shutil.copyfile(root+'/'+str(file), reconpath_3t+'/mri/orig/001.mgz')\n",
    "                    complete = 1\n",
    "    else:\n",
    "        complete = 1\n",
    "    return complete\n",
    "\n",
    "CONVERTFLAIR = pe.Node(Function(input_names=[\"pickflair\", \"flairdumplocation\", \"reconpath\", \"out_file\"],\n",
    "                         output_names=[\"out_file\"],\n",
    "                         function=convert_flair),\n",
    "                        name='CONVERTFLAIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE SCAN_AND_LOG\n",
    "# note: decided to add this afterward precaution to increase efficiency because there are few errors\n",
    "# and want to run the unpack and convert2mgz in parallel)\n",
    "\n",
    "def scan_and_log(subjectdir, scan_info, mgz, reconfolder, subname):\n",
    "    import re\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    notesdir = subjectdir.replace('RECON_FLAIR',\"SCAN_NOTES\")\n",
    "    # load in the scaninfo file\n",
    "    dicomdir = \"/cluster/animal/scan_data/leads/LEADS/\"\n",
    "    scaninfo = pd.read_csv(scan_info)\n",
    "    with open('/autofs/cluster/animal/scan_data/leads/recon_nip/batch.recon.list', \"a\") as bfile:\n",
    "        bfile.write(subname)\n",
    "    with open('/autofs/cluster/animal/scan_data/leads/recon_nip/SCAN_NOTES/unpack.log', \"a\") as ufile:\n",
    "        ufile.write(subname)\n",
    "    # should I makea scannotes? (will add info after recon)\n",
    "    Elements = {'leads_id': [''],'mayo_notes': [''],'mayo_overallpass': [''], 'download_date':['']}\n",
    "    df = pd.DataFrame(Elements, columns= ['leads_id','mayo_notes', 'mayo_overallpass', 'download_date'])\n",
    "    df.to_csv(notesdir+'/scannotes.csv',index=False)\n",
    "    return subjectdir, subname\n",
    "\n",
    "SCAN_AND_LOG = pe.Node(Function(input_names=[\"subjectdir\",\"scan_info\",'mgz', 'reconfolder', 'subname'],\n",
    "                         output_names=[\"subjectdir\", \"subname\"],\n",
    "                         function=scan_and_log),\n",
    "                        name='SCAN_AND_LOG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE RECON_JOB\n",
    "\n",
    "def recon_job(subjectname, USER, PASS): # add in username, pass, and subjectname\n",
    "    # add condition :: run this only is FS_XX, or scripts does not exist!!\n",
    "    import os\n",
    "    import glob\n",
    "    from paramiko import SSHClient\n",
    "    analyses_pipes = ['RECON_FLAIR','RECON_3T']\n",
    "    for pipeline in analyses_pipes:\n",
    "        reconpath = '/autofs/cluster/animal/scan_data/leads/recon_nip/'+pipeline\n",
    "        if not glob.glob('/autofs/cluster/animal/scan_data/leads/recon_nip/'+pipeline+'/'+subjectname + '/**/*scripts', recursive=True):\n",
    "            host=\"launchpad\"\n",
    "            user=USER\n",
    "            pw=PASS\n",
    "            client=SSHClient()\n",
    "            client.load_system_host_keys()\n",
    "            client.connect(host,username=user,password=pw, look_for_keys=False)\n",
    "            tmpstr = '(cd /autofs/cluster/animal/scan_data/leads/analyses_nip/%s; setenv p %s ; ./batch.recon.sh)' % (pipeline, subjectname)\n",
    "            stdin, stdout, stderr = client.exec_command(tmpstr)\n",
    "            err = \"stderr: \", stderr.readlines()\n",
    "            out = \"pwd: \", stdout.readlines()\n",
    "            if len(err) < 1:\n",
    "                warning = '0'\n",
    "            else:\n",
    "                warning = '1'\n",
    "            with open(reconpath+'/log_nip.txt','a') as outf:\n",
    "                outf.write(tmpstr+'\\n')\n",
    "        else:\n",
    "            err = \"\"\n",
    "            out = \"\"\n",
    "            warning = \"na\"\n",
    "            \n",
    "    return err, out, warning, subjectname\n",
    "\n",
    "RECON_JOB = pe.Node(Function(input_names=[\"subjectname\",\"USER\", \"PASS\"], \n",
    "                        output_names=[ 'err', 'out', 'warning','subjectname'],\n",
    "                         function=recon_job),\n",
    "                        name='RECON_JOB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE: GATHER_FS_DETAILS (this part only after recon is done)\n",
    "\n",
    "def gather_FS_details(subjectname): # add in username, pass, and subjectname\n",
    "    import csv\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    pipelines = ['RECON_FLAIR','RECON_3T']\n",
    "    recon_pending = []\n",
    "    recon_name = []\n",
    "    for pip in pipelines:\n",
    "        recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'+pip+'/'\n",
    "        # if you can access the status (complete) and in directory\n",
    "        try: \n",
    "            with open(recondir+subjectname+'/scripts/recon-all.done') as f:\n",
    "                first_line = f.readline()\n",
    "            if first_line == '1\\n':\n",
    "                recon_pending.append(1) # catch recons that are done processing but with errors\n",
    "            else:\n",
    "                recon_pending.append(0)\n",
    "                \n",
    "            # obtain FS version\n",
    "            versionfile = open(recondir+subjectname+'/scripts/build-stamp.txt', 'r')\n",
    "            versionstring = versionfile.read()\n",
    "            version = versionstring.split('-')\n",
    "            result = [i for i in version if i.startswith('v')][0]\n",
    "            long = result[1:]\n",
    "\n",
    "            #obtain short verison of long\n",
    "            size = len(long)\n",
    "            x = 0\n",
    "            while x ==0:\n",
    "                if (long[-1] == '0') or (long[-1] == '.'): # shave off any . or 0s from the end of version number.\n",
    "                    long = long[:-1]\n",
    "                else:\n",
    "                    x =1\n",
    "            vlabel = 'FS'+long\n",
    "\n",
    "            # obtain run number\n",
    "            notesdir = recondir.replace(pip,\"SCAN_NOTES\")\n",
    "#             with open(notesdir+subjectname+'/scaninfo.csv','r') as f:\n",
    "#                 reader = csv.reader(f)\n",
    "#                 scan_list = list(reader)\n",
    "#                 runstring = scan_list[0][0] # run\n",
    "            scaninfo = pd.read_csv(notesdir+subjectname+'/scaninfo.csv')    # newly added !!\n",
    "            runstring = str(scaninfo.loc[0,'run/series number'])          # newly added !!\n",
    "            if len(runstring) == 1:\n",
    "                runstring = '0'+runstring\n",
    "            recon_name.append(vlabel+'_'+runstring)\n",
    "        \n",
    "        #else: # otherwise incomplete, not run yet, or already moved (could have errors though)\n",
    "        except(FileNotFoundError):\n",
    "            recon_pending.append(1)\n",
    "            recon_name.append('')\n",
    "            long = '' \n",
    "    return subjectname, recon_name, recon_pending, long, pipelines \n",
    "\n",
    "FS_DETAILS = pe.Node(Function(input_names=[\"subjectname\"], \n",
    "                        output_names=[ 'subjectname', 'recon_name', 'recon_pending', 'long', 'pipelines'],\n",
    "                         function=gather_FS_details),\n",
    "                        name='FS_DETAILS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : MAKE_ORIG_FOLDER\n",
    "\n",
    "def create_orig_folder(subjectname, recon_name, recon_pending, pipelines):\n",
    "    import os\n",
    "    import shutil\n",
    "    freesurfer_dirs = ['mri', 'stats', 'tmp', 'trash', 'touch', 'label', 'surf', 'scripts']\n",
    "    for idx, pip in enumerate(pipelines):\n",
    "        recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'+pip+'/'\n",
    "        if recon_pending[idx] == 0:\n",
    "            # move all subfolders into this recon_name folder\n",
    "            for fsdir in freesurfer_dirs:\n",
    "                if os.path.isdir(recondir+subjectname+'/'+fsdir):\n",
    "                    shutil.move(recondir+subjectname+'/'+fsdir, recondir+subjectname+'/'+recon_name[idx]+'/'+fsdir) # does this create FS6_02?\n",
    "        else:\n",
    "            print(subjectname+\" files moves already, or not yet prepared.\")\n",
    "    return subjectname, recon_name, recondir, recon_pending, pipelines\n",
    "\n",
    "MAKE_ORIGINAL_DIR = pe.Node(Function(input_names=[\"subjectname\", \"recon_name\", \"recon_pending\", \"pipelines\"], \n",
    "                        output_names=['subjectname', 'recon_name', 'recondir', 'recon_pending', 'pipelines'],\n",
    "                         function=create_orig_folder),\n",
    "                        name='MAKE_ORIGINAL_DIR')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : PREPARE_MANEDITS\n",
    "\n",
    "def preparing_manedits(subjectname, recon_name, recon_pending, pipelines):\n",
    "    import shutil\n",
    "    import pathlib\n",
    "    import os\n",
    "    for idx, pip in enumerate([pipelines[0]]): # just doing edits for FLAIR pipeline\n",
    "        recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'+pip+'/'\n",
    "        analysesdir = '/autofs/cluster/animal/scan_data/leads/analyses_nip/'+pip+'/'\n",
    "        if recon_pending[idx] == 0: # otherwise dir already created or not ready)\n",
    "            recon_name2 = 'unedit.'+recon_name[idx] # overwrite recon_name2 through iteration\n",
    "            shutil.copytree(recondir+subjectname+'/'+recon_name[idx], recondir+subjectname+'/'+recon_name2)\n",
    "            shutil.copyfile(recondir+subjectname+'/'+recon_name2+'/mri/brain.finalsurfs.mgz', recondir+subjectname+'/'+recon_name2+'/mri/brain.finalsurfs.manedit.mgz')\n",
    "    return subjectname, recon_name, recon_pending, pipelines\n",
    "        \n",
    "PREPARE_MANEDITS = pe.Node(Function(input_names=['subjectname', 'recon_name', 'recon_pending', 'pipelines'], \n",
    "                        output_names=['subjectname', 'recon_name', 'recon_pending', 'pipelines'],\n",
    "                         function=preparing_manedits),\n",
    "                        name='PREPARE_MANEDITS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : EXTRACT_RECON_DETAILS AND PUT INTO RECON_NOTES.CSV\n",
    "\n",
    "def make_pandas(subjectname, recon_name, recon_pending, pipelines, long):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    status, pbsjob, command, subdir, recon_path = ([[\"\",\"\"],[\"\",\"\"]] for i in range(5))\n",
    "    final = ['FS', 'edit']\n",
    "    \n",
    "    for idx, pip in enumerate(pipelines):\n",
    "        subjectdir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'+pip+'/'+subjectname+'/'\n",
    "        notes = pd.read_csv('/autofs/cluster/animal/scan_data/leads/recon_nip/'+pip+'/recon_notes.csv',index_col= 'LEADS_ID').replace(np.nan, '', regex=True)\n",
    "        current_subjects = notes.index.values\n",
    "        if subjectname not in current_subjects: #subject is not in notes:\n",
    "            notes_cols = ['LEADS_ID'] + list(notes)\n",
    "            initialize_fields = [[''] * len(notes_cols)]\n",
    "            new_df = pd.DataFrame(columns=notes_cols, data=initialize_fields)\n",
    "            new_df.at[[0],'LEADS_ID'] = subjectname \n",
    "            # set other values too\n",
    "            new_df = new_df.set_index(['LEADS_ID'])\n",
    "            notes = pd.concat([new_df, notes])\n",
    "            notes = notes.sort_values('LEADS_ID', axis=0, ascending=True)\n",
    "            notes.to_csv('/autofs/cluster/animal/scan_data/leads/recon_nip/'+pip+'/recon_notes.csv') #,index_col= 'LEADS_ID')\n",
    "        # update the relevant fields (for new and old subjects\n",
    "        \n",
    "        #for init in final:\n",
    "        for versidx, init in enumerate(final):\n",
    "            folders = [subjectdir+x for x in os.listdir(subjectdir) if x.startswith(init)] # this means recon is running or done\n",
    "\n",
    "            try: \n",
    "                statusfile = max(folders, key=os.path.getmtime)+'/scripts/recon-all.done'\n",
    "                with open(statusfile, 'r') as fh:\n",
    "                    num_lines = sum(1 for line in open(statusfile))\n",
    "                    if num_lines < 2:\n",
    "                        qcstatus = 'error' # check for recon errors\n",
    "                        recon_path[idx][versidx] = ''\n",
    "                    else:\n",
    "                        for line in fh:\n",
    "                            if line.startswith(\"CMDARGS\"):\n",
    "                                com = line.replace(\"CMDARGS\",\"recon-all\") # initial recon command\n",
    "                                command[idx][versidx] = com[:-1]\n",
    "                        qcstatus = 'complete'\n",
    "                        recon_path[idx][versidx] = max(folders, key=os.path.getmtime)\n",
    "            except(ValueError): # the folder does not exist (need especially for edit.)\n",
    "                qcstatus = '' # subject is still running\n",
    "                command[idx][versidx] = ''\n",
    "                recon_path[idx][versidx] = ''\n",
    "            except(FileNotFoundError): # the file does not exist but the folder does\n",
    "                qcstatus = 'inprogress' # subject is still running\n",
    "                command[idx][versidx] = ''\n",
    "                recon_path[idx][versidx] = ''\n",
    "\n",
    "            try:\n",
    "                envfile = max(folders, key=os.path.getmtime)+'/scripts/recon-all.env'\n",
    "                with open(envfile, 'r') as fw:\n",
    "                    for line in fw:\n",
    "                        if line.startswith(\"PBS_JOBNAME\"):\n",
    "                            pbsjob[idx][versidx] = line.replace(\"PBS_JOBNAME=\",\"\").rstrip()\n",
    "                        elif line.startswith('SUBJECTS_DIR'):\n",
    "                            subdir[idx][versidx] = line.replace(\"SUBJECTS_DIR=\",\"\").rstrip()\n",
    "            except(ValueError): # status (still running or has not begun running)\n",
    "                qcstatus = '' # overwrite recon did not start\n",
    "                pbsjob[idx][versidx] = ''\n",
    "                subdir[idx][versidx] = ''\n",
    "            except(FileNotFoundError): # the file does not exist\n",
    "                qcstatus = '' # overwrite variable recon did not start\n",
    "                pbsjob[idx][versidx] = ''\n",
    "                subdir[idx][versidx] = ''            \n",
    "            status[idx][versidx] = qcstatus\n",
    "\n",
    "        # for pip set variables\n",
    "        notes = pd.read_csv('/autofs/cluster/animal/scan_data/leads/recon_nip/'+pip+'/recon_notes.csv').replace(np.nan, '', regex=True)\n",
    "        notes.loc[notes['LEADS_ID'] == subjectname, 'FS_INITIAL_SUBDIR'] = subdir[idx][0]\n",
    "        notes.loc[notes['LEADS_ID'] == subjectname, 'FS_POSTEDIT_SUBDIR'] = subdir[idx][1]\n",
    "        notes.loc[notes['LEADS_ID'] == subjectname, 'FS_INITIAL_COMMAND'] = command[idx][0]\n",
    "        notes.loc[notes['LEADS_ID'] == subjectname, 'FS_POSTEDIT_COMMAND'] = command[idx][1]\n",
    "        notes.loc[notes['LEADS_ID'] == subjectname, 'PBSJOB_INITIAL'] = pbsjob[idx][0]\n",
    "        notes.loc[notes['LEADS_ID'] == subjectname, 'PBSJOB_EDIT'] = pbsjob[idx][1]\n",
    "        notes.loc[notes['LEADS_ID'] == subjectname, 'FS_VERSION'] = long # this does not work unless before folder is copied?\n",
    "        notes.loc[notes['LEADS_ID'] == subjectname, 'RECON_PATH'] = ''\n",
    "        notes.loc[notes['LEADS_ID'] == subjectname, 'EDITOR'] = 'RJE'    \n",
    "        if pip == 'RECON_FLAIR':\n",
    "            notes.loc[notes['LEADS_ID'] == subjectname, 'RECON_PATH'] = recon_path[idx][1] # recon edit.FS..\n",
    "            notes.loc[notes['LEADS_ID'] == subjectname, 'STATUS_FINAL'] = status[idx][1]\n",
    "        elif pip == 'RECON_3T':\n",
    "            notes.loc[notes['LEADS_ID'] == subjectname, 'RECON_PATH'] = recon_path[idx][0] # recon FS..\n",
    "            notes.loc[notes['LEADS_ID'] == subjectname, 'STATUS_FINAL'] = status[idx][0]\n",
    "        notes.to_csv('/autofs/cluster/animal/scan_data/leads/recon_nip/'+pip+'/recon_notes.csv', index=False)\n",
    "    return subjectname\n",
    "            \n",
    "INITIALIZE_SUBJECT = pe.Node(Function(input_names=['subjectname', 'recon_name', 'recon_pending', 'pipelines', 'long'], \n",
    "                        output_names=['subjectname'],\n",
    "                         function=make_pandas),\n",
    "                        name='INITIALIZE_SUBJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # NODE : INFOSOURCE\n",
    "INFOSOURCE = Node(IdentityInterface(fields=['subject_name'], mandatory_inputs=False),\n",
    "                  name=\"INFOSOURCE\")\n",
    "\n",
    "INFOSOURCE.iterables = ('subject_name', sh_dicomlist)\n",
    "\n",
    "# NODE : SELECTFILES\n",
    "#templates = dict(dicom=sh_dicomlist[0])    ## THIS WORKED!\n",
    "templates = {\n",
    "    \"dicom\": \"{subject_name}\" \n",
    "    }\n",
    "SELECTFILES = Node(nio.SelectFiles(templates, base_directory=dicomdir),\n",
    "                   name=\"SELECTFILES\")\n",
    "\n",
    "# NODE : DATASINK\n",
    "DATASINK = Node(nio.DataSink(base_directory=leadsdir,\n",
    "                container='recon_nip'),\n",
    "                name=\"DATASINK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect all nodes (including INFOSOURCE, SELECTFILES, and DATASINK) to workflow\n",
    "\n",
    "leads_workflow.connect([(INFOSOURCE, SELECTFILES, [('subject_name', 'subject_name')]),\n",
    "                (SELECTFILES, CREATEDIR, [('dicom', 'val')]),\n",
    "                (PASSWORDS, CREATEDIR, [('USER', 'USER')]),\n",
    "                (PASSWORDS, CREATEDIR, [('PASS', 'PASS')]), \n",
    "                (CREATEDIR, IMPORT_LONI_INFO, [('date', 'date')]),\n",
    "                (CREATEDIR, IMPORT_LONI_INFO, [('createdir_out3', 'dicomname')]),\n",
    "                (CREATEDIR, IMPORT_LONI_INFO, [('createdir_out1', 'subjectdir')]),     # need actual subjectdir name (in case of repeats)\n",
    "                (IMPORT_LONI_INFO, PREPARE_4_REDCAP, [('dicomname', 'dicomname')]),\n",
    "                (IMPORT_LONI_INFO, PREPARE_4_REDCAP, [('subjectdir', 'subjectdir')]),\n",
    "                (IMPORT_LONI_INFO, PREPARE_4_REDCAP, [('imageid', 'imageid')]),\n",
    "                (CREATEDIR, PREPARE_4_REDCAP, [('createdir_out3', 'pickdicom')]),\n",
    "                (CREATEDIR, UNPACK, [('createdir_out1', 'subjectdir')]),\n",
    "                 (CREATEDIR, UNPACK, [('createdir_out2', 'MPRAGE_path')]),\n",
    "                (CREATEDIR, UNPACK, [('createdir_out3', 'pickdicom')]),\n",
    "                 (CREATEDIR, CONVERT2MGZ, [('createdir_out3', 'in_file')]),\n",
    "                 (CREATEDIR, CONVERT2MGZ, [('createdir_out4', 'out_file')]),\n",
    "                (CREATEDIR, CONVERT2MGZ, [('createdir_out1', 'reconpath')]),\n",
    "                (CONVERT2MGZ, CONVERTFLAIR, [('out_file', 'out_file')]), # added \n",
    "                (CREATEDIR, CONVERTFLAIR, [('flairdumplocation', 'flairdumplocation')]),\n",
    "                 (CREATEDIR, CONVERTFLAIR, [('pickflair', 'pickflair')]),\n",
    "                (CREATEDIR, CONVERTFLAIR, [('createdir_out1', 'reconpath')]),\n",
    "                (CONVERTFLAIR, SCAN_AND_LOG, [('out_file', 'mgz')]), #changed\n",
    "                (CREATEDIR, SCAN_AND_LOG, [('createdir_out5', 'reconfolder')]),\n",
    "                (UNPACK, CHECKINFO, [('unpack_out1', 'subname')]),\n",
    "                (UNPACK, CHECKINFO, [('unpack_out2', 'subjectdir')]),\n",
    "                (UNPACK, CHECKINFO, [('unpack_out3', 'scan_info')]),\n",
    "                (UNPACK, CHECKINFO, [('unpack_out4', 'MPRAGE_path')]),\n",
    "                (UNPACK, CHECKINFO, [('unpack_out5', 'pickdicom')]),      \n",
    "                (CHECKINFO, SCAN_AND_LOG, [('check_out1', 'subname')]),\n",
    "                (CHECKINFO, SCAN_AND_LOG, [('check_out2', 'subjectdir')]),\n",
    "                (CHECKINFO, SCAN_AND_LOG, [('check_out3', 'scan_info')]),\n",
    "                (CREATEDIR, RECON_JOB, [('USER','USER')]),\n",
    "                (CREATEDIR, RECON_JOB, [('PASS','PASS')]),\n",
    "                (SCAN_AND_LOG, RECON_JOB, [('subname','subjectname')]), \n",
    "                (RECON_JOB, FS_DETAILS, [('subjectname','subjectname')]),\n",
    "                (FS_DETAILS, MAKE_ORIGINAL_DIR, [('subjectname','subjectname')]), \n",
    "                (FS_DETAILS, MAKE_ORIGINAL_DIR, [('recon_pending','recon_pending')]), \n",
    "                (FS_DETAILS, MAKE_ORIGINAL_DIR, [('recon_name','recon_name')]), \n",
    "                (FS_DETAILS, MAKE_ORIGINAL_DIR, [('pipelines','pipelines')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('subjectname','subjectname')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('recon_name','recon_name')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('recon_pending','recon_pending')]),\n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('pipelines','pipelines')]),\n",
    "                (FS_DETAILS, INITIALIZE_SUBJECT, [('long','long')]), \n",
    "                (PREPARE_MANEDITS, INITIALIZE_SUBJECT, [('subjectname','subjectname')]), \n",
    "                (PREPARE_MANEDITS, INITIALIZE_SUBJECT, [('recon_name','recon_name')]), \n",
    "                (PREPARE_MANEDITS, INITIALIZE_SUBJECT, [('recon_pending','recon_pending')]),\n",
    "                (PREPARE_MANEDITS, INITIALIZE_SUBJECT, [('pipelines','pipelines')]),                           \n",
    "                (FS_DETAILS, PREPARE_4_REDCAP, [('long','fsversion')]), \n",
    "                (PREPARE_MANEDITS, DATASINK, [('subjectname','backup')])  # backup folder?\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190830-10:39:10,4 nipype.workflow INFO:\n",
      "\t Workflow leads_workflow settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "190830-10:39:10,51 nipype.workflow INFO:\n",
      "\t Running serially.\n",
      "190830-10:39:10,53 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.SELECTFILES\" in \"/tmp/tmpl3di3tqp/leads_workflow/_subject_name_LDS0180106_20190621/SELECTFILES\".\n",
      "190830-10:39:10,57 nipype.workflow INFO:\n",
      "\t [Node] Running \"SELECTFILES\" (\"nipype.interfaces.io.SelectFiles\")\n",
      "190830-10:39:10,66 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.SELECTFILES\".\n",
      "190830-10:39:10,68 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.PASSWORDS\" in \"/tmp/tmp_sp7qcfl/leads_workflow/PASSWORDS\".\n",
      "190830-10:39:10,72 nipype.workflow INFO:\n",
      "\t [Node] Running \"PASSWORDS\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "Please enter your PASSWORD for launchpad access: \n",
      "········\n",
      "190830-10:39:13,936 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.PASSWORDS\".\n",
      "190830-10:39:13,938 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CREATEDIR\" in \"/tmp/tmpuqysks0d/leads_workflow/_subject_name_LDS0180106_20190621/CREATEDIR\".\n",
      "190830-10:39:13,947 nipype.workflow INFO:\n",
      "\t [Node] Running \"CREATEDIR\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:39:14,46 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CREATEDIR\".\n",
      "190830-10:39:14,47 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CONVERT2MGZ\" in \"/tmp/tmp7_ofon7t/leads_workflow/_subject_name_LDS0180106_20190621/CONVERT2MGZ\".\n",
      "190830-10:39:14,54 nipype.workflow INFO:\n",
      "\t [Node] Running \"CONVERT2MGZ\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:39:23,333 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CONVERT2MGZ\".\n",
      "190830-10:39:23,335 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CONVERTFLAIR\" in \"/tmp/tmp108ww3ru/leads_workflow/_subject_name_LDS0180106_20190621/CONVERTFLAIR\".\n",
      "190830-10:39:23,344 nipype.workflow INFO:\n",
      "\t [Node] Running \"CONVERTFLAIR\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:39:31,626 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CONVERTFLAIR\".\n",
      "190830-10:39:31,628 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.UNPACK\" in \"/tmp/tmpjw6tu7lc/leads_workflow/_subject_name_LDS0180106_20190621/UNPACK\".\n",
      "190830-10:39:31,633 nipype.workflow INFO:\n",
      "\t [Node] Running \"UNPACK\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:39:53,802 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.UNPACK\".\n",
      "190830-10:39:53,803 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CHECKINFO\" in \"/tmp/tmpl50_xgbc/leads_workflow/_subject_name_LDS0180106_20190621/CHECKINFO\".\n",
      "190830-10:39:53,811 nipype.workflow INFO:\n",
      "\t [Node] Running \"CHECKINFO\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:39:53,826 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CHECKINFO\".\n",
      "190830-10:39:53,828 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.SCAN_AND_LOG\" in \"/tmp/tmp7ycctwj_/leads_workflow/_subject_name_LDS0180106_20190621/SCAN_AND_LOG\".\n",
      "190830-10:39:53,835 nipype.workflow INFO:\n",
      "\t [Node] Running \"SCAN_AND_LOG\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:39:53,855 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.SCAN_AND_LOG\".\n",
      "190830-10:39:53,856 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.RECON_JOB\" in \"/tmp/tmpnpqb43nh/leads_workflow/_subject_name_LDS0180106_20190621/RECON_JOB\".\n",
      "190830-10:39:53,863 nipype.workflow INFO:\n",
      "\t [Node] Running \"RECON_JOB\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:40:25,201 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.RECON_JOB\".\n",
      "190830-10:40:25,205 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.FS_DETAILS\" in \"/tmp/tmpz9urqb1t/leads_workflow/_subject_name_LDS0180106_20190621/FS_DETAILS\".\n",
      "190830-10:40:25,210 nipype.workflow INFO:\n",
      "\t [Node] Running \"FS_DETAILS\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:40:25,223 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.FS_DETAILS\".\n",
      "190830-10:40:25,224 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.MAKE_ORIGINAL_DIR\" in \"/tmp/tmppw_hfcib/leads_workflow/_subject_name_LDS0180106_20190621/MAKE_ORIGINAL_DIR\".\n",
      "190830-10:40:25,231 nipype.workflow INFO:\n",
      "\t [Node] Running \"MAKE_ORIGINAL_DIR\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "LDS0180106_20190621 files moves already, or not yet prepared.\n",
      "LDS0180106_20190621 files moves already, or not yet prepared.\n",
      "190830-10:40:25,240 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.MAKE_ORIGINAL_DIR\".\n",
      "190830-10:40:25,241 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.PREPARE_MANEDITS\" in \"/tmp/tmpqdnkdytw/leads_workflow/_subject_name_LDS0180106_20190621/PREPARE_MANEDITS\".\n",
      "190830-10:40:25,249 nipype.workflow INFO:\n",
      "\t [Node] Running \"PREPARE_MANEDITS\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:40:25,257 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.PREPARE_MANEDITS\".\n",
      "190830-10:40:25,258 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.DATASINK\" in \"/tmp/tmpw4uh1be9/leads_workflow/_subject_name_LDS0180106_20190621/DATASINK\".\n",
      "190830-10:40:25,266 nipype.workflow INFO:\n",
      "\t [Node] Running \"DATASINK\" (\"nipype.interfaces.io.DataSink\")\n",
      "190830-10:40:25,275 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.DATASINK\".\n",
      "190830-10:40:25,276 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.INITIALIZE_SUBJECT\" in \"/tmp/tmpc9f7bv9x/leads_workflow/_subject_name_LDS0180106_20190621/INITIALIZE_SUBJECT\".\n",
      "190830-10:40:25,284 nipype.workflow INFO:\n",
      "\t [Node] Running \"INITIALIZE_SUBJECT\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:40:25,504 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.INITIALIZE_SUBJECT\".\n",
      "190830-10:40:25,505 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.IMPORT_LONI_INFO\" in \"/tmp/tmpxz55rx_w/leads_workflow/_subject_name_LDS0180106_20190621/IMPORT_LONI_INFO\".\n",
      "190830-10:40:25,514 nipype.workflow INFO:\n",
      "\t [Node] Running \"IMPORT_LONI_INFO\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:40:25,574 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.IMPORT_LONI_INFO\".\n",
      "190830-10:40:25,576 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.PREPARE_4_REDCAP\" in \"/tmp/tmp_63m_865/leads_workflow/_subject_name_LDS0180106_20190621/PREPARE_4_REDCAP\".\n",
      "190830-10:40:25,585 nipype.workflow INFO:\n",
      "\t [Node] Running \"PREPARE_4_REDCAP\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190830-10:40:25,607 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.PREPARE_4_REDCAP\".\n",
      "190830-10:40:26,692 nipype.workflow INFO:\n",
      "\t Generated workflow graph: /autofs/cluster/animal/scan_data/leads/leads_workflow/workflow_graph.png (graph2use=flat, simple_form=True).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/autofs/cluster/animal/scan_data/leads/leads_workflow/workflow_graph.png'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute workflow in sequential way\n",
    "# leads_workflow.run(run(plugin='MultiProc', plugin_args={'n_procs' : 2})\n",
    "leads_workflow.run()\n",
    "\n",
    "leads_workflow.write_graph(dotfilename='/autofs/cluster/animal/scan_data/leads/leads_workflow/workflow_graph.dot',graph2use='flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leads_workflow.write_graph(graph2use='flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dickerson_database\n",
    "\n",
    "# # # specific upload mr function\n",
    "# # dickerson_database.dropbox.upload_mr.upload_mr(subject_id, date, t1_path, dry=True)\n",
    "\n",
    "# # # general dropbox interface\n",
    "# # dickerson_dropbox = dickerson_database.dropbox.DickersonLab()\n",
    "# print(dickerson_dropbox.exists('/Dickerson lab/0_Subject Data'))\n",
    "\n",
    "# print(dickerson_dropbox.list('/Dickerson lab'))\n",
    "# #   ['0_Subject Data',\n",
    "# #  'Abstracts',\n",
    "# #  'Administrative',\n",
    "# #  'Autopsy Reports',\n",
    "# #  ...]\n",
    "# # dickerson_dropbox.upload('/tmp/foo.nii.gz', '/Dickerson lab/foo.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
