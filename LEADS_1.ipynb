{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: i just added extra nodes that will only run if recon runs in the same go. What happens to subjects \n",
    "# that have been run through recon but not the rest of the script? Or what happens after recon? Should I have a status check\n",
    "# and connect them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import io, os, sys, types # needed\n",
    "import glob # needed\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode # needed\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "import nipype.interfaces.io as nio\n",
    "import nipype.pipeline.engine as pe\n",
    "from nipype.interfaces.freesurfer import MRIConvert\n",
    "from nipype.interfaces.freesurfer import ReconAll\n",
    "from nipype import config\n",
    "import re\n",
    "import shutil\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(subject):\n",
    "    subsessions = glob.glob(dicomdir+subject+'/Accelerated_Sagittal_MPRAGE/*/*/')\n",
    "    repeat_tag = '-'\n",
    "    for num in range(len(subsessions)):\n",
    "        # look to see if more than one session on the same date\n",
    "        # then look to see if more than one date (or both)\n",
    "        parentfolder = subsessions[num].split('/')[8]\n",
    "        filename = os.listdir(subsessions[num])[0] # dicom name to extract date\n",
    "        date = re.search('raw_'+r'+[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]', filename).group()[4:]\n",
    "        if num == len(subsessions)-1:\n",
    "            namedate = dicomdir+subject+'_'+date\n",
    "            os.rename(dicomdir+subject, namedate)\n",
    "        else:\n",
    "            namedateplus = dicomdir+subject+'_'+date+repeat_tag+'/Accelerated_Sagittal_MPRAGE/'+folder1\n",
    "            pathlib.Path(namedateplus).mkdir(parents=True, exist_ok=True)\n",
    "            shutil.move(subsessions[num], namedateplus)\n",
    "            if not os.listdir(dicomdir+subject+'/Accelerated_Sagittal_MPRAGE/'+parentfolder):\n",
    "                os.rmdir(dicomdir+subject+'/Accelerated_Sagittal_MPRAGE/'+parentfolder)\n",
    "            repeat_tag = repeat_tag+'-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify variables\n",
    "leadsdir = '/cluster/animal/scan_data/leads/'\n",
    "os.chdir(leadsdir)\n",
    "dicomdir = \"/cluster/animal/scan_data/leads/LEADS/\"\n",
    "unpacklog = \"/autofs/cluster/animal/scan_data/leads/recon/unpack.log\"\n",
    "recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'\n",
    "folders = [x for x in os.listdir(dicomdir) if not x.startswith(\".\")]\n",
    "subjlist = [f for f in os.listdir(dicomdir) if ((\"_\") not in f) and (not f.startswith('.') and (\"duplicate\" not in f))]\n",
    "\n",
    "# wipe clean batch.recon.list\n",
    "open(recondir+'batch.recon.list', 'w').close()\n",
    "\n",
    "# unwravel all subjects with multiple sessions and rename to include date\n",
    "for sub in subjlist:\n",
    "    scan(sub)\n",
    "\n",
    "# now just make a list of subjects by ID (this will be the input to the nodes)\n",
    "# will define dicom path within node\n",
    "sh_dicomlist = [f for f in os.listdir(dicomdir) if ((\"_\" in f) and (\"REPEAT_RUNS\" not in f))]\n",
    "\n",
    "# define workflow\n",
    "leads_workflow = Workflow(name='leads_workflow') #, base_dir = '/autofs/cluster/animal/scan_data/leads/recon_nip/') # add  base_dir='/shared_fs/nipype_scratch'\n",
    "\n",
    "# configure to stop on first crash\n",
    "cfg = dict(execution={'stop_on_first_crash': True})\n",
    "config.update_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging\n",
    "sh_dicomlist = ['LDS0370007_20180801']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST NODE : PASSSWORDS\n",
    "\n",
    "def credentials(): # combined with find_dicom\n",
    "    import getpass\n",
    "    USER = getpass.getuser()\n",
    "    print('Please enter your PASSWORD for launchpad access: ')\n",
    "    PASS= getpass.getpass()\n",
    "    return USER, PASS\n",
    "\n",
    "PASSWORDS = pe.Node(Function(input_names=[\"user\", \"pw\"],\n",
    "                         output_names=[\"USER\",\"PASS\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=credentials),\n",
    "                        name='PASSWORDS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : CREATEDIR\n",
    "def createdir(val, USER, PASS):\n",
    "    import os\n",
    "    import re\n",
    "    import glob\n",
    "    val = val.split('/')[-1]\n",
    "    dicomdir = \"/autofs/cluster/animal/scan_data/leads/LEADS/\"\n",
    "    recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/' \n",
    "    reconpath = recondir+val+'/'\n",
    "    imgpath = reconpath+'/mri/orig/'\n",
    "    dumplocation = imgpath+'001.mgz'\n",
    "    subject = val.split('_')[0]\n",
    "    date = val.split('_')[1].replace(\"-\",\"\")  # take out any '-'\n",
    "    MPRAGE_path = glob.glob(dicomdir+val+'/Accelerated_Sagittal_MPRAGE/*/*')[0]\n",
    "    pickdicom = glob.glob(dicomdir+val+'/Accelerated_Sagittal_MPRAGE/*/*/*')[0]\n",
    "    sessionid = MPRAGE_path.split(\"/\")[-1]\n",
    "    try:\n",
    "        os.makedirs(imgpath)\n",
    "    except(FileExistsError):\n",
    "        pass\n",
    "    return reconpath, MPRAGE_path, pickdicom, dumplocation, recondir, USER, PASS\n",
    "        \n",
    "CREATEDIR = pe.Node(Function(input_names=[\"val\", \"USER\", \"PASS\"],\n",
    "                         output_names=[\"createdir_out1\",\"createdir_out2\", \"createdir_out3\", \"createdir_out4\", \"createdir_out5\", \"USER\", \"PASS\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=createdir),\n",
    "                        name='CREATEDIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "di = '/autofs/cluster/animal/scan_data/leads/recon_nip/LDS0370007_20180801/'\n",
    "subname = di.split('/')[-1]\n",
    "print(subname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : UNPACK\n",
    "\n",
    "def unpack(subjectdir, MPRAGE_path):\n",
    "    from os import system\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    import os.path\n",
    "    if not os.path.isfile(subjectdir+'scan.info'): \n",
    "        cmdstring = 'unpacksdcmdir -src %s -targ %s -scanonly %s/scan.info' % (MPRAGE_path, subjectdir, subjectdir)\n",
    "        system(cmdstring)\n",
    "    if not os.path.isfile(subjectdir+'scaninfo.csv'):\n",
    "        with open(subjectdir+'/scan.info', 'r') as in_file:\n",
    "            for line in in_file:\n",
    "                editline = line.split()\n",
    "                with open(subjectdir+'/scaninfo.csv', 'w') as result:\n",
    "                    wr = csv.writer(result, dialect='excel')\n",
    "                    wr.writerow(editline)\n",
    "                result.close()\n",
    "            in_file.close()\n",
    "    scan_info = pd.read_csv(subjectdir+'/scaninfo.csv', header=None)\n",
    "    subname = subjectdir.split('/')[-2]\n",
    "    #try creating a txt file with subname\n",
    "    with open('/autofs/cluster/animal/scan_data/leads/recon_nip/SUBNAME_'+subname, 'w') as f:\n",
    "        f.write('subjectdir variable is '+subjectdir)\n",
    "        f.write('subname variable is '+subname)\n",
    "    return subname, subjectdir, scan_info\n",
    "\n",
    "UNPACK = pe.Node(Function(input_names=[\"subjectdir\",\"MPRAGE_path\"],\n",
    "                         output_names=[\"unpack_out1\",\"unpack_out2\", \"unpack_out3\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=unpack),\n",
    "                        name='UNPACK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : CONVERT2MGZ\n",
    "# I think I can set subject as variable to pass on here and others send elsewhere\n",
    "\n",
    "CONVERT2MGZ = pe.Node(MRIConvert(out_type='mgz'), \n",
    "                        name='CONVERT2MGZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE SCAN_AND_LOG\n",
    "# note: decided to add this afterward precaution to increase efficiency because there are few errors\n",
    "# and want to run the unpack and convert2mgz in parallel)\n",
    "\n",
    "def scan_and_log(subjectdir, scan_info, mgz, reconfolder, subname):\n",
    "    import re\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    # load in the scaninfo file\n",
    "    dicomdir = \"/cluster/animal/scan_data/leads/LEADS/\"\n",
    "    check = scan_info.iloc[0,2] # first row (only row); second col (validity)\n",
    "    if check != 'ok':\n",
    "        with open(reconfolder+'/scanerrors', \"a\") as efile:\n",
    "            efile.write(scan_info.iloc[0,7]) # log for errors in dicoms (or any ommitted scans)\n",
    "            # delete entire recon folder\n",
    "    else:\n",
    "        with open(reconfolder+'/batch.recon.list', \"a\") as bfile:\n",
    "            bfile.write(subname)\n",
    "        with open(reconfolder+'/unpack.log', \"a\") as ufile:\n",
    "            ufile.write(subname)\n",
    "        # should I makea scannotes? (will add info after recon)\n",
    "        Elements = {'scan_notes': [''],'loni_overallpass': [''], 'download_date':[''], 'xnat_upload':[''], 'recon_path':[''],'recon_notes':[''], 'dickerson_overallpass':['']}\n",
    "        df = pd.DataFrame(Elements, columns= ['scan_notes', 'loni_overallpass', 'download_date','xnat_upload','recon_path','recon_notes','dickerson_overallpass'])\n",
    "        df.to_csv(subjectdir+'/scannotes.csv')\n",
    "    return subjectdir, subname\n",
    "\n",
    "SCAN_AND_LOG = pe.Node(Function(input_names=[\"subjectdir\",\"scan_info\",'mgz', 'reconfolder', 'subname'],\n",
    "                         output_names=[\"subjectdir\", \"subname\"],\n",
    "                         function=scan_and_log),\n",
    "                        name='SCAN_AND_LOG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE RECON_JOB\n",
    "\n",
    "def recon_job(subjectname, USER, PASS): # add in username, pass, and subjectname\n",
    "    from paramiko import SSHClient\n",
    "    host=\"launchpad\"\n",
    "    user=USER\n",
    "    pw=PASS\n",
    "    client=SSHClient()\n",
    "    client.load_system_host_keys()\n",
    "    client.connect(host,username=user,password=pw, look_for_keys=False)\n",
    "    tmpstr = '(cd /autofs/cluster/animal/scan_data/leads/analyses_nip; setenv p %s ; ./batch.recon.sh)' % subjectname\n",
    "    stdin, stdout, stderr = client.exec_command(tmpstr)\n",
    "    #stin = print(\"stdin: \", stdin.readlines())\n",
    "    err = \"stderr: \", stderr.readlines()\n",
    "    out = \"pwd: \", stdout.readlines()\n",
    "    if len(err) < 1:\n",
    "        warning = '0'\n",
    "    else:\n",
    "        warning = '1'\n",
    "    return err, out, warning, subjectname\n",
    "\n",
    "RECON_JOB = pe.Node(Function(input_names=[\"subjectname\",\"USER\", \"PASS\"], \n",
    "                        output_names=[ 'err', 'out', 'warning','subjectname'],\n",
    "                         function=recon_job),\n",
    "                        name='RECON_JOB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE: GATHER_FS_DETAILS (this part only after recon is done)\n",
    "\n",
    "def gather_FS_details(subjectname): # add in username, pass, and subjectname\n",
    "    recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'\n",
    "    try:\n",
    "        # obtain FS version\n",
    "        versionfile = open(recondir+subjectname+'/scripts/build-stamp.txt', 'r')\n",
    "        versionstring = versionfile.read()\n",
    "        version = versionstring.split('-')\n",
    "        result = [i for i in version if i.startswith('v')][0]\n",
    "        long = result[1:]\n",
    "        \n",
    "        #obtain short verison of long\n",
    "        size = len(long)\n",
    "        x = 0\n",
    "        while x ==0:\n",
    "            if (long[-1] == '0') or (long[-1] == '.'): # shave off any . or 0s from the end of version number.\n",
    "                long = long[:-1]\n",
    "            else:\n",
    "                x =1\n",
    "        vlabel = 'FS'+long\n",
    "\n",
    "        # obtain run number\n",
    "        with open(recondir+subjectname+'/scaninfo.csv','r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            scan_list = list(reader)\n",
    "            runstring = scan_list[0][0] # run\n",
    "            if len(runstring) == 1:\n",
    "                runstring = '0'+runstring\n",
    "        recon_name = vlabel+'_'+runstring\n",
    "\n",
    "    except(FileNotFoundError):\n",
    "        print(subjectname+\" freesurfer recon not complete.\")\n",
    "        recon_name = 'FS--'\n",
    "    return subjectname, recon_name\n",
    "        \n",
    "FS_DETAILS = pe.Node(Function(input_names=[\"subjectname\"], \n",
    "                        output_names=[ 'subject', 'recon_name'],\n",
    "                         function=gather_FS_details),\n",
    "                        name='FS_DETAILS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : MAKE_ORIG_FOLDER\n",
    "\n",
    "def create_orig_folder(subjectname, recon_name):\n",
    "    recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'\n",
    "    freesurfer_dirs = ['mri', 'stats', 'tmp', 'trash', 'touch', 'label', 'surf', 'scripts']\n",
    "    # now go into each subject folder and create a folder with recon_name\n",
    "    try:\n",
    "        os.mkdir(recondir+subjectname+'/'+recon_name)\n",
    "    except(FileExistsError):\n",
    "        print(subjectname+\" original recon folder already created.\")\n",
    "    \n",
    "    # move all subfolders into this recon_name folder\n",
    "    for fsdir in freesurfer_dirs:\n",
    "        FS = Path(recondir+subjectname+'/'+fsdir)\n",
    "        if FS.exists():\n",
    "            shutil.move(recondir+subjectname+'/'+fsdir, recondir+subjectname+'/'+recon_name+'/'+fsdir)\n",
    "    return subjectname, recon_name, recondir\n",
    "\n",
    "MAKE_ORIGINAL_DIR = pe.Node(Function(input_names=[\"subjectname\", \"recon_name\"], \n",
    "                        output_names=['subjectname', 'recon_name', 'recondir'],\n",
    "                         function=create_orig_folder),\n",
    "                        name='MAKE_ORIGINAL_DIR')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : PREPARE_MANEDITS\n",
    "\n",
    "def preparing_manedits(subjectname, recon_name, recondir):\n",
    "    analysesdir = '/autofs/cluster/animal/scan_data/leads/analyses_nip/'\n",
    "    try:\n",
    "        recon_name2 = 'unedit.'+recon_name\n",
    "        FS2 = Path(recondir+subjectname+'/'+recon_name2)\n",
    "        shutil.copytree(recondir+subjectname+'/'+recon_name, FS2) # if edit.recon folder does not already exist, add to log\n",
    "        with open(recondir+'to_edit.list\\n', \"a\") as myfile:\n",
    "            myfile.write(sub)\n",
    "    except(FileExistsError):\n",
    "        print(subjectname+\" unedit.recon folder already created.\")\n",
    "        \n",
    "    #Now create symlink to edit. in analysis folder!!\n",
    "    try:\n",
    "        os.symlink(recondir+subjectname+'/'+recon_name2, analysesdir+sub)\n",
    "    except(FileExistsError):\n",
    "        print(subjectname+\" in analyses is already linked to unedit.recon.\")\n",
    "    return subjectname, recon_name, recondir\n",
    "        \n",
    "PREPARE_MANEDITS = pe.Node(Function(input_names=['subjectname', 'recon_name', 'recondir'], \n",
    "                        output_names=['subjectname', 'recon_name', 'recondir'],\n",
    "                         function=preparing_manedits),\n",
    "                        name='PREPARE_MANEDITS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # NODE : INFOSOURCE\n",
    "INFOSOURCE = Node(IdentityInterface(fields=['subject_name'], mandatory_inputs=False),\n",
    "                  name=\"INFOSOURCE\")\n",
    "\n",
    "INFOSOURCE.iterables = ('subject_name', sh_dicomlist)\n",
    "\n",
    "# NODE : SELECTFILES\n",
    "#templates = dict(dicom=sh_dicomlist[0])    ## THIS WORKED!\n",
    "templates = {\n",
    "    \"dicom\": \"{subject_name}\" \n",
    "    }\n",
    "SELECTFILES = Node(nio.SelectFiles(templates, base_directory=dicomdir),\n",
    "                   name=\"SELECTFILES\")\n",
    "\n",
    "# NODE : DATASINK\n",
    "DATASINK = Node(nio.DataSink(base_directory=leadsdir,\n",
    "                container='recon_nip'),\n",
    "                name=\"DATASINK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect all nodes (including INFOSOURCE, SELECTFILES, and DATASINK) to workflow\n",
    "\n",
    "leads_workflow.connect([(INFOSOURCE, SELECTFILES, [('subject_name', 'subject_name')]),\n",
    "                (SELECTFILES, CREATEDIR, [('dicom', 'val')]),\n",
    "                (PASSWORDS, CREATEDIR, [('USER', 'USER')]),\n",
    "                (PASSWORDS, CREATEDIR, [('PASS', 'PASS')]), \n",
    "                (CREATEDIR, UNPACK, [('createdir_out1', 'subjectdir')]),\n",
    "                 (CREATEDIR, UNPACK, [('createdir_out2', 'MPRAGE_path')]),\n",
    "                 (CREATEDIR, CONVERT2MGZ, [('createdir_out3', 'in_file')]),\n",
    "                 (CREATEDIR, CONVERT2MGZ, [('createdir_out4', 'out_file')]),   \n",
    "                (CONVERT2MGZ, SCAN_AND_LOG, [('out_file', 'mgz')]),\n",
    "                (CREATEDIR, SCAN_AND_LOG, [('createdir_out5', 'reconfolder')]),\n",
    "                (UNPACK, SCAN_AND_LOG, [('unpack_out1', 'subname')]),\n",
    "                (UNPACK, SCAN_AND_LOG, [('unpack_out2', 'subjectdir')]),\n",
    "                (UNPACK, SCAN_AND_LOG, [('unpack_out3', 'scan_info')]),\n",
    "                (CREATEDIR, RECON_JOB, [('USER','USER')]),     # new\n",
    "                (CREATEDIR, RECON_JOB, [('PASS','PASS')]),     # new\n",
    "                (SCAN_AND_LOG, RECON_JOB, [('subname','subjectname')]), \n",
    "                (RECON_JOB, DATASINK, [('warning','backup')])  # backup folder?\n",
    "#                 (RECON_JOB, FS_DETAILS, [('subjectname','subjectname')]),\n",
    "#                 (FS_DETAILS, MAKE_ORIGINAL_DIR, [('subject','subjectname')]), \n",
    "#                 (FS_DETAILS, MAKE_ORIGINAL_DIR, [('recon_name','recon_name')]), \n",
    "#                 (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('subjectname','subjectname')]), \n",
    "#                 (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('recon_name','recon_name')]), \n",
    "#                 (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('recondir','recondir')]), \n",
    "#                 (PREPARE_MANEDITS, DATASINK, [('subjectname','backup')])  # backup folder?\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190315-13:12:48,194 nipype.workflow INFO:\n",
      "\t Workflow leads_workflow settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "190315-13:12:48,230 nipype.workflow INFO:\n",
      "\t Running serially.\n",
      "190315-13:12:48,232 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.SELECTFILES\" in \"/tmp/tmp8jp_z26a/leads_workflow/_subject_name_LDS0370007_20180801/SELECTFILES\".\n",
      "190315-13:12:48,236 nipype.workflow INFO:\n",
      "\t [Node] Running \"SELECTFILES\" (\"nipype.interfaces.io.SelectFiles\")\n",
      "190315-13:12:48,246 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.SELECTFILES\".\n",
      "190315-13:12:48,248 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.PASSWORDS\" in \"/tmp/tmp9t5spt71/leads_workflow/PASSWORDS\".\n",
      "190315-13:12:48,251 nipype.workflow INFO:\n",
      "\t [Node] Running \"PASSWORDS\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "Please enter your PASSWORD for launchpad access: \n",
      "········\n",
      "190315-13:12:56,760 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.PASSWORDS\".\n",
      "190315-13:12:56,762 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CREATEDIR\" in \"/tmp/tmp15dqiwpb/leads_workflow/_subject_name_LDS0370007_20180801/CREATEDIR\".\n",
      "190315-13:12:56,769 nipype.workflow INFO:\n",
      "\t [Node] Running \"CREATEDIR\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190315-13:12:56,792 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CREATEDIR\".\n",
      "190315-13:12:56,793 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CONVERT2MGZ\" in \"/tmp/tmpccm5pk0h/leads_workflow/_subject_name_LDS0370007_20180801/CONVERT2MGZ\".\n",
      "190315-13:12:56,804 nipype.workflow INFO:\n",
      "\t [Node] Running \"CONVERT2MGZ\" (\"nipype.interfaces.freesurfer.preprocess.MRIConvert\"), a CommandLine Interface with command:\n",
      "mri_convert --out_type mgz --input_volume /autofs/cluster/animal/scan_data/leads/LEADS/LDS0370007_20180801/Accelerated_Sagittal_MPRAGE/2018-08-01_10_49_41.0/S711584/LEADS_LDS0370007_MR_Accelerated_Sagittal_MPRAGE__br_raw_20180801153317254_70_S711584_I1029698.dcm --output_volume /autofs/cluster/animal/scan_data/leads/recon_nip/LDS0370007_20180801//mri/orig/001.mgz\n",
      "190315-13:12:56,849 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:12:56.849739:mri_convert.bin --out_type mgz --input_volume /autofs/cluster/animal/scan_data/leads/LEADS/LDS0370007_20180801/Accelerated_Sagittal_MPRAGE/2018-08-01_10_49_41.0/S711584/LEADS_LDS0370007_MR_Accelerated_Sagittal_MPRAGE__br_raw_20180801153317254_70_S711584_I1029698.dcm --output_volume /autofs/cluster/animal/scan_data/leads/recon_nip/LDS0370007_20180801//mri/orig/001.mgz \n",
      "190315-13:12:56,855 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:12:56.855667:$Id: mri_convert.c,v 1.226 2016/02/26 16:15:24 mreuter Exp $\n",
      "190315-13:12:56,856 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:12:56.855667:reading from /autofs/cluster/animal/scan_data/leads/LEADS/LDS0370007_20180801/Accelerated_Sagittal_MPRAGE/2018-08-01_10_49_41.0/S711584/LEADS_LDS0370007_MR_Accelerated_Sagittal_MPRAGE__br_raw_20180801153317254_70_S711584_I1029698.dcm...\n",
      "190315-13:12:56,858 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:12:56.858295:Getting Series No \n",
      "190315-13:12:56,859 nipype.interface INFO:\n",
      "\t stderr 2019-03-15T13:12:56.859726:INFO: Found 210 files in /autofs/cluster/animal/scan_data/leads/LEADS/LDS0370007_20180801/Accelerated_Sagittal_MPRAGE/2018-08-01_10_49_41.0/S711584\n",
      "190315-13:12:56,860 nipype.interface INFO:\n",
      "\t stderr 2019-03-15T13:12:56.859726:INFO: Scanning for Series Number 2\n",
      "190315-13:12:56,861 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:12:56.861518:Scanning Directory \n",
      "190315-13:12:57,914 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:12:57.914568:INFO: loading series header info.\n",
      "190315-13:12:57,916 nipype.interface INFO:\n",
      "\t stderr 2019-03-15T13:12:57.916921:INFO: found 208 files in series\n",
      "190315-13:13:18,255 nipype.interface INFO:\n",
      "\t stderr 2019-03-15T13:13:18.255498:\n",
      "190315-13:13:18,257 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.256976:INFO: sorting.\n",
      "190315-13:13:18,257 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.256976:sdfiSameSlicePos() eps = 0.000001\n",
      "190315-13:13:18,258 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.256976:INFO: (240 256 208), nframes = 1, ismosaic=0\n",
      "190315-13:13:18,259 nipype.interface INFO:\n",
      "\t stderr 2019-03-15T13:13:18.259119:RunNo = 1\n",
      "190315-13:13:18,263 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.262987:sdfi->UseSliceScaleFactor 0\n",
      "190315-13:13:18,263 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.262987:datatype = 4, short=4, float=3\n",
      "190315-13:13:18,264 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.262987:PE Dir ROW ROW\n",
      "190315-13:13:18,272 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.271975:AutoAlign matrix detected \n",
      "190315-13:13:18,279 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.279569:AutoAlign Matrix --------------------- \n",
      "190315-13:13:18,280 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.279569: 1.00000   0.00000   0.00000   0.00000;\n",
      "190315-13:13:18,281 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.279569: 0.00000   1.00000   0.00000   0.00000;\n",
      "190315-13:13:18,281 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.279569: 0.00000   0.00000   1.00000   0.00000;\n",
      "190315-13:13:18,282 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.279569: 0.00000   0.00000   0.00000   1.00000;\n",
      "190315-13:13:18,309 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\n",
      "190315-13:13:18,310 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:FileName \t\t/autofs/cluster/animal/scan_data/leads/LEADS/LDS0370007_20180801/Accelerated_Sagittal_MPRAGE/2018-08-01_10_49_41.0/S711584/LEADS_LDS0370007_MR_Accelerated_Sagittal_MPRAGE__br_raw_20180801153323157_1_S711584_I1029698.dcm\n",
      "190315-13:13:18,311 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:Identification\n",
      "190315-13:13:18,312 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tNumarisVer        syngo MR E11\n",
      "190315-13:13:18,313 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tScannerModel      Prisma\n",
      "190315-13:13:18,314 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tPatientName       DE-IDENTIFIED \n",
      "190315-13:13:18,315 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:Date and time\n",
      "190315-13:13:18,315 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tStudyDate         20180801\n",
      "190315-13:13:18,316 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tStudyTime         104941.088000 \n",
      "190315-13:13:18,317 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tSeriesTime        105804.804000 \n",
      "190315-13:13:18,317 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tAcqTime           105239.832500 \n",
      "190315-13:13:18,318 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:Acquisition parameters\n",
      "190315-13:13:18,319 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tPulseSeq          *tfl3d1_16ns\n",
      "190315-13:13:18,320 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tProtocol          Accelerated Sagittal MPRAGE \n",
      "190315-13:13:18,320 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tPhEncDir          ROW\n",
      "190315-13:13:18,321 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tEchoNo            1\n",
      "190315-13:13:18,321 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tFlipAngle         9\n",
      "190315-13:13:18,322 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tEchoTime          2.98\n",
      "190315-13:13:18,323 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tInversionTime     900\n",
      "190315-13:13:18,324 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tRepetitionTime    2300\n",
      "190315-13:13:18,325 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tPhEncFOV          240\n",
      "190315-13:13:18,326 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tReadoutFOV        256\n",
      "190315-13:13:18,327 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:Image information\n",
      "190315-13:13:18,328 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tRunNo             1\n",
      "190315-13:13:18,329 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tSeriesNo          2\n",
      "190315-13:13:18,330 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tImageNo           1\n",
      "190315-13:13:18,331 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tNImageRows        256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190315-13:13:18,332 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tNImageCols        240\n",
      "190315-13:13:18,333 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tNFrames           1\n",
      "190315-13:13:18,334 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tSliceArraylSize   1\n",
      "190315-13:13:18,334 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tIsMosaic          0\n",
      "190315-13:13:18,335 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tImgPos             98.7542 146.4407 122.8136 \n",
      "190315-13:13:18,336 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tVolRes              1.0000   1.0000   1.0000 \n",
      "190315-13:13:18,336 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tVolDim            240      256      208 \n",
      "190315-13:13:18,337 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tVc                 -0.0000  -1.0000   0.0000 \n",
      "190315-13:13:18,337 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tVr                 -0.0000  -0.0000  -1.0000 \n",
      "190315-13:13:18,338 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tVs                 -1.0000  -0.0000   0.0000 \n",
      "190315-13:13:18,339 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tVolCenter          -5.2458  26.4407  -5.1864 \n",
      "190315-13:13:18,340 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:\tTransferSyntaxUID 1.2.840.10008.1.2\n",
      "190315-13:13:18,340 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:UseSliceScaleFactor 0 (slice 0: 1)\n",
      "190315-13:13:18,341 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:18.309812:IsDWI = 0\n",
      "190315-13:13:22,227 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:22.227161:INFO: no Siemens slice order reversal detected (good!). \n",
      "190315-13:13:22,228 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:22.227161:TR=2300.00, TE=2.98, TI=900.00, flip angle=9.00\n",
      "190315-13:13:22,229 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:22.227161:i_ras = (-0, -1, 0)\n",
      "190315-13:13:22,229 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:22.227161:j_ras = (-0, -0, -1)\n",
      "190315-13:13:22,230 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:22.227161:k_ras = (-1, -0, 0)\n",
      "190315-13:13:22,231 nipype.interface INFO:\n",
      "\t stdout 2019-03-15T13:13:22.227161:writing to /autofs/cluster/animal/scan_data/leads/recon_nip/LDS0370007_20180801//mri/orig/001.mgz...\n",
      "190315-13:13:22,291 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CONVERT2MGZ\".\n",
      "190315-13:13:22,292 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.UNPACK\" in \"/tmp/tmp03l6egm7/leads_workflow/_subject_name_LDS0370007_20180801/UNPACK\".\n",
      "190315-13:13:22,297 nipype.workflow INFO:\n",
      "\t [Node] Running \"UNPACK\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190315-13:13:41,462 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.UNPACK\".\n",
      "190315-13:13:41,466 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.SCAN_AND_LOG\" in \"/tmp/tmppflpct9n/leads_workflow/_subject_name_LDS0370007_20180801/SCAN_AND_LOG\".\n",
      "190315-13:13:41,506 nipype.workflow INFO:\n",
      "\t [Node] Running \"SCAN_AND_LOG\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190315-13:13:41,530 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.SCAN_AND_LOG\".\n",
      "190315-13:13:41,532 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.RECON_JOB\" in \"/tmp/tmpw5qw36iv/leads_workflow/_subject_name_LDS0370007_20180801/RECON_JOB\".\n",
      "190315-13:13:41,538 nipype.workflow INFO:\n",
      "\t [Node] Running \"RECON_JOB\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190315-13:13:54,335 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.RECON_JOB\".\n",
      "190315-13:13:54,340 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.DATASINK\" in \"/tmp/tmp1u93cqm8/leads_workflow/_subject_name_LDS0370007_20180801/DATASINK\".\n",
      "190315-13:13:54,352 nipype.workflow INFO:\n",
      "\t [Node] Running \"DATASINK\" (\"nipype.interfaces.io.DataSink\")\n",
      "190315-13:13:54,361 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.DATASINK\".\n",
      "190315-13:13:54,927 nipype.workflow INFO:\n",
      "\t Generated workflow graph: /autofs/cluster/animal/scan_data/leads/graph.png (graph2use=flat, simple_form=True).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/autofs/cluster/animal/scan_data/leads/graph.png'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute your workflow in sequential way\n",
    "# leads_workflow.run(run(plugin='MultiProc', plugin_args={'n_procs' : 2})\n",
    "leads_workflow.run()\n",
    "\n",
    "leads_workflow.write_graph(graph2use='flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190315-13:13:55,496 nipype.workflow INFO:\n",
      "\t Generated workflow graph: /autofs/cluster/animal/scan_data/leads/graph.png (graph2use=flat, simple_form=True).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/autofs/cluster/animal/scan_data/leads/graph.png'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leads_workflow.write_graph(graph2use='flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
