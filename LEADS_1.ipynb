{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIREMENTS FOR THIS PIPELINE\n",
    "#  the recon output will be named e.g. \"unedit.FS6_02\". After manually editing\n",
    "#  please rename this recon folder to \"edit.FS6_02\". Then the next time \n",
    "#  pipeline is run, it will re-submit the recon for this folder and create symlink.\n",
    "\n",
    "#  the following notes need to be updated upon each batch download so this info can be piped:\n",
    "#  files: LEADS_#####_date.csv (csv download file), Mayo_ADRIL_MRI_Quality_date.csv\n",
    "\n",
    "        #details:\n",
    "        #MOST IMPORTANT: once a new download has been initiated, load the CSV download file into\n",
    "        #/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/\n",
    "        #if it is a new image collection on loni, it will create a separate CSV file, \n",
    "        #otherwise will replace old one with concatenated data of that image collection. So archive\n",
    "        #any csvs that are old versions of downloaded collections.\n",
    "        \n",
    "        #must also download Mayo_ADRIL_MRI_Quality_date.csv and save to:\n",
    "        #/autofs/cluster/animal/scan_data/leads/spreadsheets/MRIQUALITY\n",
    "        #this file will replace the old one (concatenates all data on loni about QC).\n",
    "\n",
    "# if any files need to be re-run / re-processed, delete the files in the \n",
    "# folder. This pipeline does not overwrite anything.\n",
    "\n",
    "# TO DO\n",
    "# send a recon job- just do for ones that do not have output?\n",
    "# decide if I want to have the option to re-run edits manually of implement here?\n",
    "# not sure why but import_loni_notes is creates a new column in the dataframe scannotes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import io, os, sys, types # needed\n",
    "import glob # needed\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode # needed\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "import nipype.interfaces.io as nio\n",
    "import nipype.pipeline.engine as pe\n",
    "from nipype.interfaces.freesurfer import MRIConvert\n",
    "from nipype.interfaces.freesurfer import ReconAll\n",
    "from nipype import config\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import pathlib\n",
    "import pydicom\n",
    "from pydicom.tag import Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and update spreadsheets\n",
    "\n",
    "# these do not concatenate, must do this\n",
    "downloadsdir = '/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/'\n",
    "\n",
    "# vertically concatenate all csvs\n",
    "downloadlist = glob.glob(downloadsdir+'*.csv')\n",
    "downloadlist.remove('/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv')\n",
    "\n",
    "# vertically concatenate all csvs\n",
    "combined_csv = pd.concat( [ pd.read_csv(f) for f in downloadlist ] , sort=False)\n",
    "\n",
    "# drop all non-MPRAGES, sort dataframe by subject column, drop all duplicates\n",
    "combined_csv = combined_csv[combined_csv.Description == 'Accelerated Sagittal MPRAGE']\n",
    "combined_csv = combined_csv.sort_values(by=['Downloaded'])\n",
    "combined_csv = combined_csv.drop_duplicates(['Image Data ID'], keep='last')\n",
    "\n",
    "# # save combined download file\n",
    "combined_csv.to_csv(\"/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv\", index=False,)\n",
    "\n",
    "# # these download already concatenating all sessions ; use latest\n",
    "qualitydir = '/autofs/cluster/animal/scan_data/leads/spreadsheets/MRIQUALITY/'\n",
    "\n",
    "list_of_qualityfiles = glob.glob(qualitydir+'*.csv')\n",
    "MRIQUALITY = max(list_of_qualityfiles, key=os.path.getctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(subject):\n",
    "    subsessions = glob.glob(dicomdir+subject+'/Accelerated_Sagittal_MPRAGE/*/*/')\n",
    "    repeat_tag = '-'\n",
    "    for num in range(len(subsessions)):\n",
    "        # look to see if more than one session on the same date\n",
    "        # then look to see if more than one date (or both)\n",
    "        parentfolder = subsessions[num].split('/')[8]\n",
    "        filename = os.listdir(subsessions[num])[0] # dicom name to extract date\n",
    "        #extract date from dicom:\n",
    "        ds = pydicom.read_file(subsessions[num]+'/'+filename)\n",
    "        date = str(ds[0x08, 0x22].value)\n",
    "        #date = re.search('raw_'+r'+[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]', filename).group()[4:]\n",
    "        if num == len(subsessions)-1:\n",
    "            namedate = dicomdir+subject+'_'+date\n",
    "            os.rename(dicomdir+subject, namedate)\n",
    "        else:\n",
    "            namedateplus = dicomdir+subject+'_'+date+repeat_tag+'/Accelerated_Sagittal_MPRAGE/'+parentfolder\n",
    "            pathlib.Path(namedateplus).mkdir(parents=True, exist_ok=True)\n",
    "            shutil.move(subsessions[num], namedateplus)\n",
    "            if not os.listdir(dicomdir+subject+'/Accelerated_Sagittal_MPRAGE/'+parentfolder):\n",
    "                os.rmdir(dicomdir+subject+'/Accelerated_Sagittal_MPRAGE/'+parentfolder)\n",
    "            repeat_tag = repeat_tag+'-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify variables\n",
    "leadsdir = '/cluster/animal/scan_data/leads/'\n",
    "os.chdir(leadsdir)\n",
    "dicomdir = \"/cluster/animal/scan_data/leads/LEADS/\"\n",
    "unpacklog = \"/autofs/cluster/animal/scan_data/leads/recon/unpack.log\"\n",
    "#recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'\n",
    "recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_FLAIR/'\n",
    "recondir_3t = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_3T/'\n",
    "recondir_edit = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_FLAIR_EDITED/'\n",
    "folders = [x for x in os.listdir(dicomdir) if not x.startswith(\".\")]\n",
    "subjlist = [f for f in os.listdir(dicomdir) if ((\"_\") not in f) and (not f.startswith('.') and (\"duplicate\" not in f))]\n",
    "\n",
    "# wipe clean batch.recon.list\n",
    "open(recondir+'batch.recon.list', 'w').close()\n",
    "\n",
    "# unwravel all subjects with multiple sessions and rename to include date\n",
    "for sub in subjlist:\n",
    "    scan(sub)\n",
    "\n",
    "# now just make a list of subjects by ID (this will be the input to the nodes)\n",
    "# will define dicom path within node\n",
    "sh_dicomlist = [f for f in os.listdir(dicomdir) if ((\"_\" in f) and (\"REPEAT_RUNS\" not in f))]\n",
    "\n",
    "# define workflow\n",
    "leads_workflow = Workflow(name='leads_workflow') #, base_dir = '/autofs/cluster/animal/scan_data/leads/recon_nip/') # add  base_dir='/shared_fs/nipype_scratch'\n",
    "\n",
    "# configure to stop on first crash\n",
    "cfg = dict(execution={'stop_on_first_crash': True})\n",
    "config.update_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging\n",
    "\n",
    "sh_dicomlist = ['LDS3600069_20190426']\n",
    "\n",
    "# ['LDS0370005_20180802','LDS0370020_20181212',  'LDS0670070_20190305',  'LDS3600067_20190320', \\\n",
    "# 'LDS0110022_20181129','LDS0370006_20180726','LDS0370029_20181210','LDS0670076_20190312','LDS3600068_20190325', \\\n",
    "# 'LDS0110040_20190123','LDS0370007_20180801','LDS0370034_20190107','LDS0670077_20190318','LDS3600087_20190417', \\\n",
    "# 'LDS0110041_20190212','LDS0370008_20180815','LDS0370037_20181218','LDS0670080_20190315','LDS9410023_20181127', \\\n",
    "# 'LDS0110052_20190205','LDS0370009_20180816','LDS0370038_20181217','LDS0670085_20190327','LDS9410025_20181128', \\\n",
    "# 'LDS0110053_20190227','LDS0370010_20180815','LDS0370042_20190108','LDS0730001_20180619','LDS9410027_20181105', \\\n",
    "# 'LDS0110078_20190320','LDS0370011_20180822','LDS0370047_20190226','LDS0730024_20181107','LDS9410028_20181109', \\\n",
    "# 'LDS0220026_20181109','LDS0370012_20180824','LDS0370058_20190318','LDS0730044_20190129','LDS9410035_20181126', \\\n",
    "# 'LDS0220031_20181130','LDS0370013_20180822','LDS0370061_20190219','LDS0730051_20190313','LDS9410036_20181203', \\\n",
    "# 'LDS0220050_20190208','LDS0370014_20180913','LDS0370065_20190227','LDS0730055_20190304','LDS9410049_20190118', \\\n",
    "# 'LDS0220062_20190225','LDS0370015_20181113','LDS0370073_20190308','LDS1770064_20190222','LDS9410060_20190219', \\\n",
    "# 'LDS0220071_20190311','LDS0370016_20180912','LDS0370074_20190404','LDS3600030_20181219','LDS9410066_20190227', \\\n",
    "# 'LDS0220081_20190322','LDS0370017_20181001','LDS0370086_20190329','LDS3600032_20190123', \\\n",
    "# 'LDS0370001_20180509','LDS0370018_20181015','LDS0370089_20190403','LDS3600043_20190118', \\\n",
    "# 'LDS0370002_20180606','LDS0370019_20181121','LDS0670048_20190320','LDS3600056_20190323']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST NODE : PASSSWORDS\n",
    "\n",
    "def credentials(): # combined with find_dicom\n",
    "    import getpass\n",
    "    USER = getpass.getuser()\n",
    "    print('Please enter your PASSWORD for launchpad access: ')\n",
    "    PASS= getpass.getpass()\n",
    "    return USER, PASS\n",
    "\n",
    "PASSWORDS = pe.Node(Function(input_names=[\"user\", \"pw\"],\n",
    "                         output_names=[\"USER\",\"PASS\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=credentials),\n",
    "                        name='PASSWORDS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : CREATEDIR\n",
    "def createdir(val, USER, PASS):\n",
    "    import os\n",
    "    import re\n",
    "    import glob\n",
    "    import pydicom\n",
    "    from pydicom.tag import Tag\n",
    "    val = val.split('/')[-1]\n",
    "    dicomdir = \"/autofs/cluster/animal/scan_data/leads/LEADS/\"\n",
    "    pipelines = ['RECON_3T', 'RECON_FLAIR']\n",
    "    for pipe in pipelines:\n",
    "        recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/'+pipe+'/' \n",
    "        reconpath = recondir+val+'/'\n",
    "        imgpath = reconpath+'mri/orig/'\n",
    "        if not glob.glob(reconpath + '/**/*mri', recursive=True): # changed recondir to reconpath\n",
    "            os.makedirs(imgpath) # edited this part!\n",
    "    dumplocation = imgpath+'001.mgz'\n",
    "    flairdumplocation = imgpath+'FLAIR.mgz'\n",
    "    subject = val.split('_')[0]\n",
    "    MPRAGE_path = glob.glob(dicomdir+val+'/Accelerated_Sagittal_MPRAGE/*/*')[0]\n",
    "    pickdicom = glob.glob(dicomdir+val+'/Accelerated_Sagittal_MPRAGE/*/*/*')[0]\n",
    "    pickflair = glob.glob(dicomdir+val+'/Sagittal_3D_FLAIR/*/*/*')[0]\n",
    "    ds = pydicom.read_file(pickdicom)\n",
    "    date = str(ds[0x08, 0x22].value)\n",
    "    sessionid = MPRAGE_path.split(\"/\")[-1]\n",
    "    return reconpath, MPRAGE_path, pickdicom, dumplocation, recondir, USER, PASS, imgpath, date, flairdumplocation, pickflair\n",
    "        \n",
    "CREATEDIR = pe.Node(Function(input_names=[\"val\", \"USER\", \"PASS\"],\n",
    "                         output_names=[\"createdir_out1\",\"createdir_out2\", \"createdir_out3\", \"createdir_out4\", \"createdir_out5\", \"USER\", \"PASS\", \"createdir_out6\", \"date\", \"flairdumplocation\",\"pickflair\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=createdir),\n",
    "                        name='CREATEDIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : IMPORT_LONI_INFO\n",
    "\n",
    "def import_loni_notes(dicomname, date, subjectdir):\n",
    "    import pandas as pd\n",
    "    import glob\n",
    "    import os\n",
    "    import re\n",
    "    # download info\n",
    "    download_df = pd.read_csv('/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv')\n",
    "    recon_dir = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_NOTES/'\n",
    "    mgh_subs = '/autofs/cluster/animal/scan_data/leads/spreadsheets/IDENTIFICATION/MGH_SUBJECTS.csv'\n",
    "    notes_dir = recon_dir+subjectdir.split('/')[-2]\n",
    "    dicom = dicomname.split(\"/\")[-1]\n",
    "    subid = dicom.split(\"_\")[1]\n",
    "    imageid = dicom.split(\"_\")[12][1:-4]\n",
    "    scannotes_df = pd.read_csv(notes_dir+'/scannotes.csv')\n",
    "    try:\n",
    "        download_date = download_df.loc[download_df['Image Data ID'] == float(imageid), 'Downloaded'].values[0]\n",
    "    except(IndexError):\n",
    "        pass\n",
    "    # loni notes\n",
    "    qualitydir = '/autofs/cluster/animal/scan_data/leads/spreadsheets/MRIQUALITY/'\n",
    "    list_qc_files = glob.glob(qualitydir+'*.csv')\n",
    "    MRIQUALITY = max(list_qc_files, key=os.path.getctime)\n",
    "    # these download already concatenating all sessions ; use latest\n",
    "    quality_df = pd.read_csv(MRIQUALITY)\n",
    "    try:\n",
    "        loni_overallpass = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_overallpass'].values[0]\n",
    "        if loni_overallpass == 1:\n",
    "            qc_pass = '1'\n",
    "        elif loni_overallpass == 4:\n",
    "            qc_pass = '0'\n",
    "        else:\n",
    "            qc_pass = ''\n",
    "        study_comments = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_comments'].values[0]\n",
    "        study_protocol_comment = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_protocol_comment'].values[0]\n",
    "        protocol_comments = quality_df.loc[quality_df['loni_image'] == float(imageid), 'protocol_comments'].values[0]\n",
    "        series_comments = quality_df.loc[quality_df['loni_image'] == float(imageid), 'series_comments'].values[0]\n",
    "        series_quality = quality_df.loc[quality_df['loni_image'] == float(imageid), 'series_quality'].values[0]# if 3, needs review; if 2 it is ok\n",
    "        if series_quality == 2:\n",
    "            s_quality = 'Scan quality is acceptable according to MAYO. '\n",
    "        elif series_quality == 3:\n",
    "            s_quality = 'Scan quality is questionable according to MAYO and needs review. '\n",
    "        elif series_quality == 4:\n",
    "            s_quality = 'Scan quality is poor according to MAYO and needs review. '\n",
    "        else:\n",
    "            s_quality = 'No scan quality data recorded from MAYO. '\n",
    "        study_rescan_requested = quality_df.loc[quality_df['loni_image'] == float(imageid), 'study_rescan_requested'].values[0]\n",
    "        if study_rescan_requested == 'TRUE':\n",
    "            rescan_requested = ' Study rescan has been requested. '\n",
    "        else:\n",
    "            rescan_requested = '. No study rescans have been requested. '\n",
    "\n",
    "        # delete duplicates within list, delete nans\n",
    "        comments_list = [s_quality,str(study_comments),str(study_protocol_comment),str(series_comments),str(protocol_comments),rescan_requested]\n",
    "        cleanedList = [x for x in comments_list if (x != 'nan')]\n",
    "        concat_comments = ''.join(cleanedList)+\" QC_pass from original site is \"+qc_pass+\" .\"\n",
    "        xnat_upload = '0'\n",
    "    except(IndexError):\n",
    "        if subid[0:6] == 'LDS360':   # if its MGH data # line 57\n",
    "            mgh_df = pd.read_csv(mgh_subs,index_col=False)\n",
    "            concat_comments = mgh_df.loc[mgh_df['leadsid'] == subid+'_'+date, 'notes'].values[0]\n",
    "            xnat_upload = mgh_df.loc[mgh_df['leadsid'] == subid+'_'+date, 'XNAT_upload'].values[0]\n",
    "            if str(mgh_df.loc[mgh_df['leadsid'] == subid+'_'+date, 'notes'].values[0]) == 'nan':\n",
    "                concat_comments = 'No comments from MAYO. '\n",
    "            qc_pass = \"No data.\"\n",
    "        else:\n",
    "            concat_comments = 'No comments from MAYO. '\n",
    "            qc_pass = 'No data.'\n",
    "            xnat_upload = '0'\n",
    "            \n",
    "    # add recon path by taking newest edit folder (if exists)\n",
    "    try:\n",
    "        edit_dir = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_FLAIR_EDITED/'+subjectdir.split('/')[-2]\n",
    "        edit_folders = [f for f in os.listdir(edit_dir) if f.startswith(\"edit.\")]\n",
    "        ext_folders = [edit_dir + s for s in edit_folders]\n",
    "        try:\n",
    "            recon_folder = max(ext_folders, key=os.path.getctime)\n",
    "        except(ValueError):\n",
    "            recon_folder = ''\n",
    "    except(FileNotFoundError):\n",
    "        recon_folder = ''\n",
    "        \n",
    "    scannotes_df.loc[scannotes_df.index[0], 'xnat_upload'] = xnat_upload\n",
    "    scannotes_df.loc[scannotes_df.index[0], 'recon_path'] = recon_folder\n",
    "    scannotes_df.loc[scannotes_df.index[0], 'loni_overallpass'] = qc_pass\n",
    "    scannotes_df.loc[scannotes_df.index[0], 'scan_notes'] = concat_comments\n",
    "    scannotes_df.loc[scannotes_df.index[0], 'download_date'] = download_date\n",
    "    scannotes_df.to_csv(notes_dir+'/scannotes.csv')\n",
    "    return dicomname, subjectdir, imageid\n",
    "    \n",
    "IMPORT_LONI_INFO = pe.Node(Function(input_names=[\"dicomname\", \"date\", \"subjectdir\"],\n",
    "                        output_names = [\"dicomname\", \"subjectdir\", \"imageid\"], \n",
    "                        function=import_loni_notes),\n",
    "                        name='IMPORT_LONI_INFO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : PREPARE_4_REDCAP\n",
    "# note that QC notes and status are manually recorded in scannotes\n",
    "\n",
    "def prepare_redcap(dicomname, subjectdir, imageid, pickdicom, fsversion):\n",
    "    import pandas as pd\n",
    "    import pydicom\n",
    "    from pydicom.tag import Tag\n",
    "    import csv\n",
    "    subject = subjectdir.split(\"/\")[-1]\n",
    "    convert_sex = '/autofs/cluster/animal/scan_data/leads/spreadsheets/IDENTIFICATION/DEMOGRAPHIC_IDS.csv'\n",
    "    demo_form = '/autofs/cluster/animal/scan_data/leads/spreadsheets/LONI_DOWNLOADS/combined_downloads.csv'\n",
    "    site_conversion = '/autofs/cluster/animal/scan_data/leads/spreadsheets/IDENTIFICATION/SITE_IDS.csv'\n",
    "    notes_dir = subjectdir.replace('RECON_FLAIR','RECON_NOTES')\n",
    "    scannotes = pd.read_csv(notes_dir+'scannotes.csv')\n",
    "    download_df = pd.read_csv(demo_form)\n",
    "    reader = csv.reader(open(convert_sex))\n",
    "    ds = pydicom.read_file(pickdicom)\n",
    "    d={}\n",
    "    for row in reader:\n",
    "        d[row[0]]=row[1:][0]\n",
    "    sex = str(ds[0x10,0x40].value)\n",
    "    SEX = d.get(sex)\n",
    "    AQ_DATE = ds[0x08,0x22].value\n",
    "    age = ds[0x00101010].value\n",
    "    AGE = str(int(age[:-1]))\n",
    "    \n",
    "    try:\n",
    "        SITE = str(ds[0x08, 0x80].value) #Institution Name\n",
    "    except(KeyError):\n",
    "        d={}\n",
    "        reader = csv.reader(open(site_conversion))\n",
    "        for row in reader:\n",
    "            d[row[0]]=row[1:][0]\n",
    "        t = subject[3:6]\n",
    "        SITE = d.get(t)\n",
    "\n",
    "    dicom_path = pickdicom.strip(pickdicom.split('/')[-1]) # or use scaninfo ?\n",
    "    GROUP = download_df.loc[download_df['Image Data ID'] == float(imageid), 'Group'].values[0]\n",
    "    GEN_NOTES = ''\n",
    "    RECON_PATH = scannotes.loc[scannotes.index[0], 'recon_path'] \n",
    "    FS_VERSION = fsversion\n",
    "    \n",
    "    #Can be found in scannotes:\n",
    "    DN_DATE = scannotes.loc[scannotes.index[0], 'download_date']\n",
    "    XNAT = scannotes.loc[scannotes.index[0], 'xnat_upload']\n",
    "    \n",
    "    acq_notes = scannotes.loc[scannotes.index[0], 'scan_notes']\n",
    "    post_notes = scannotes.loc[scannotes.index[0], 'recon_notes'] \n",
    "    overallpass = scannotes.loc[scannotes.index[0], 'dickerson_overallpass'] \n",
    "    if str(overallpass) == '1':\n",
    "        QC_STATUS = '1' # pass\n",
    "    elif str(overallpass) == '0':\n",
    "        QC_STATUS = '0' # fail\n",
    "    else:\n",
    "        QC_STATUS = '2' # in_progress\n",
    "    if not str(post_notes) == 'nan':\n",
    "        post_notes = \" Post-aquisition / recon notes: \"+post_notes\n",
    "        SESSION_NOTES = acq_notes+post_notes+\" Dickerson Lab overall pass is \"+str(overallpass)+\".\"\n",
    "    else:\n",
    "        SESSION_NOTES = acq_notes   # qc_notes in redcap\n",
    "\n",
    "\n",
    "PREPARE_4_REDCAP = pe.Node(Function(input_names=[\"dicomname\",\"subjectdir\", \"imageid\", \"pickdicom\", \"fsversion\"],\n",
    "                        function=prepare_redcap),\n",
    "                          name='PREPARE_4_REDCAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : UNPACK\n",
    "\n",
    "def unpack(subjectdir, MPRAGE_path):\n",
    "    from os import system\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    import os.path\n",
    "    notesdir = subjectdir.replace('RECON_FLAIR',\"RECON_NOTES\")\n",
    "    if not os.path.isfile(notesdir+'scan.info'): \n",
    "        cmdstring = 'unpacksdcmdir -src %s -targ %s -scanonly %s/scan.info' % (MPRAGE_path, notesdir, notesdir)\n",
    "        system(cmdstring)\n",
    "    if not os.path.isfile(notesdir+'scaninfo.csv'):\n",
    "        with open(notesdir+'/scan.info', 'r') as in_file:\n",
    "            for line in in_file:\n",
    "                editline = line.split()\n",
    "                with open(notesdir+'/scaninfo.csv', 'w') as result:\n",
    "                    wr = csv.writer(result, dialect='excel')\n",
    "                    wr.writerow(editline)\n",
    "                result.close()\n",
    "            in_file.close()\n",
    "    scan_info = notesdir+'/scaninfo.csv'\n",
    "    subname = notesdir.split('/')[-2]\n",
    "    return subname, subjectdir, scan_info\n",
    "\n",
    "UNPACK = pe.Node(Function(input_names=[\"subjectdir\",\"MPRAGE_path\"],\n",
    "                         output_names=[\"unpack_out1\",\"unpack_out2\", \"unpack_out3\"], # actual dicom (redundant to create unpacking node visualization)\n",
    "                         function=unpack),\n",
    "                        name='UNPACK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (first option) # # NODE : CONVERT2MGZ (only runs if .mgz is not available)\n",
    "\n",
    "def convert_dicom(in_file, out_file, reconpath):\n",
    "    import os\n",
    "    import glob\n",
    "    from os import system\n",
    "    #import time # just see if this works if waits\n",
    "    # check for a file called 001.mgz\n",
    "    if not glob.glob(reconpath + '/**/*001.mgz', recursive=True):\n",
    "        cmdstring = 'mri_convert %s %s' % (in_file, out_file)\n",
    "        system(cmdstring)\n",
    "        complete = 1\n",
    "    else:\n",
    "        complete = 1\n",
    "\n",
    "    return complete\n",
    "\n",
    "CONVERT2MGZ = pe.Node(Function(input_names=[\"in_file\", \"out_file\", \"reconpath\"],\n",
    "                         output_names=[\"out_file\"],\n",
    "                         function=convert_dicom),\n",
    "                        name='CONVERT2MGZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_flair(pickflair, flairdumplocation, reconpath, out_file):\n",
    "    import os\n",
    "    import glob\n",
    "    from os import system\n",
    "    import shutil\n",
    "    reconpath_3t = reconpath.replace('RECON_FLAIR','RECON_3T')\n",
    "#     if not glob.glob(reconpath + '/**/*FLAIR.mgz', recursive=True):\n",
    "#         cmdstring = 'mri_convert %s %s' % (pickflair, reconpath)\n",
    "#         system(cmdstring)\n",
    "#         complete = 1\n",
    "#     else:\n",
    "#         complete = 1  \n",
    "\n",
    "    # put back after copying all flairs ad hoc\n",
    "    if not glob.glob(reconpath + '/**/*FLAIR.mgz', recursive=True):\n",
    "        cmdstring = 'mri_convert %s %s' % (pickflair, flairdumplocation)\n",
    "        system(cmdstring)\n",
    "        complete = 1\n",
    "    else:\n",
    "        complete = 1  \n",
    "# #     # copy 001 to 3T folder\n",
    "    if not glob.glob(reconpath_3t + '/**/*001.mgz', recursive=True):\n",
    "        for root, dirs, files in os.walk(reconpath): \n",
    "            for file in files:  \n",
    "                if file == '001.mgz': \n",
    "                    shutil.copyfile(root+'/'+str(file), reconpath_3t+'/mri/orig/001.mgz')\n",
    "                    complete = 1\n",
    "    else:\n",
    "        complete = 1\n",
    "    return complete\n",
    "\n",
    "CONVERTFLAIR = pe.Node(Function(input_names=[\"pickflair\", \"flairdumplocation\", \"reconpath\", \"out_file\"],\n",
    "                         output_names=[\"out_file\"],\n",
    "                         function=convert_flair),\n",
    "                        name='CONVERTFLAIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE SCAN_AND_LOG\n",
    "# note: decided to add this afterward precaution to increase efficiency because there are few errors\n",
    "# and want to run the unpack and convert2mgz in parallel)\n",
    "\n",
    "def scan_and_log(subjectdir, scan_info, mgz, reconfolder, subname):\n",
    "    import re\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    notesdir = subjectdir.replace('RECON_FLAIR',\"RECON_NOTES\")\n",
    "    # load in the scaninfo file\n",
    "    dicomdir = \"/cluster/animal/scan_data/leads/LEADS/\"\n",
    "    scaninfo = pd.read_csv(scan_info, header=None)\n",
    "    check = scaninfo.iloc[0,2] # first row (only row); second col (validity)\n",
    "    if check != 'ok':\n",
    "        with open(reconfolder+'/scanerrors', \"a\") as efile:\n",
    "            efile.write(scaninfo.iloc[0,7]) # log for errors in dicoms (or any ommitted scans)\n",
    "    else:\n",
    "        with open(reconfolder+'/batch.recon.list', \"a\") as bfile:\n",
    "            bfile.write(subname)\n",
    "        with open(reconfolder+'/unpack.log', \"a\") as ufile:\n",
    "            ufile.write(subname)\n",
    "        # should I makea scannotes? (will add info after recon)\n",
    "        Elements = {'scan_notes': [''],'loni_overallpass': [''], 'download_date':[''], 'xnat_upload':[''], 'recon_path':[''],'recon_notes':[''], 'dickerson_overallpass':['']}\n",
    "        df = pd.DataFrame(Elements, columns= ['scan_notes', 'loni_overallpass', 'download_date','xnat_upload','recon_path','recon_notes','dickerson_overallpass'])\n",
    "        df.to_csv(notesdir+'/scannotes.csv')\n",
    "    return subjectdir, subname\n",
    "\n",
    "SCAN_AND_LOG = pe.Node(Function(input_names=[\"subjectdir\",\"scan_info\",'mgz', 'reconfolder', 'subname'],\n",
    "                         output_names=[\"subjectdir\", \"subname\"],\n",
    "                         function=scan_and_log),\n",
    "                        name='SCAN_AND_LOG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE RECON_JOB\n",
    "\n",
    "def recon_job(subjectname, USER, PASS): # add in username, pass, and subjectname\n",
    "    # add condition :: run this only is FS_XX, or scripts does not exist!!\n",
    "    import os\n",
    "    import glob\n",
    "    from paramiko import SSHClient\n",
    "    analyses_pipes = ['RECON_FLAIR','RECON_3T']\n",
    "    for pipeline in analyses_pipes:\n",
    "        reconpath = '/autofs/cluster/animal/scan_data/leads/recon_nip/'+pipeline\n",
    "        if not glob.glob('/autofs/cluster/animal/scan_data/leads/recon_nip/'+pipeline+'/'+subjectname + '/**/*scripts', recursive=True):\n",
    "            host=\"launchpad\"\n",
    "            user=USER\n",
    "            pw=PASS\n",
    "            client=SSHClient()\n",
    "            client.load_system_host_keys()\n",
    "            client.connect(host,username=user,password=pw, look_for_keys=False)\n",
    "            tmpstr = '(cd /autofs/cluster/animal/scan_data/leads/analyses_nip/%s; setenv p %s ; ./batch.recon.sh)' % (pipeline, subjectname)\n",
    "            stdin, stdout, stderr = client.exec_command(tmpstr)\n",
    "            #stin = print(\"stdin: \", stdin.readlines())\n",
    "            err = \"stderr: \", stderr.readlines()\n",
    "            out = \"pwd: \", stdout.readlines()\n",
    "            if len(err) < 1:\n",
    "                warning = '0'\n",
    "            else:\n",
    "                warning = '1'\n",
    "            #print err, out warning to text file in both recon dirs\n",
    "            with open(reconpath+'log_nip.txt','a') as outf:\n",
    "                outf.write(tmpstr)\n",
    "        else:\n",
    "            err = \"\"\n",
    "            out = \"\"\n",
    "            warning = \"na\"\n",
    "            \n",
    "    return err, out, warning, subjectname\n",
    "\n",
    "RECON_JOB = pe.Node(Function(input_names=[\"subjectname\",\"USER\", \"PASS\"], \n",
    "                        output_names=[ 'err', 'out', 'warning','subjectname'],\n",
    "                         function=recon_job),\n",
    "                        name='RECON_JOB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE: GATHER_FS_DETAILS (this part only after recon is done)\n",
    "\n",
    "def gather_FS_details(subjectname): # add in username, pass, and subjectname\n",
    "    import csv\n",
    "    import os\n",
    "    recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_FLAIR/'\n",
    "    # if you can access the status (complete) and in directory\n",
    "    if os.path.isfile(recondir+subjectname+'/scripts/recon-all.done'): # replaces from old: recon-all.log\n",
    "        recon_pending = 0\n",
    "        # obtain FS version\n",
    "        versionfile = open(recondir+subjectname+'/scripts/build-stamp.txt', 'r')\n",
    "        versionstring = versionfile.read()\n",
    "        version = versionstring.split('-')\n",
    "        result = [i for i in version if i.startswith('v')][0]\n",
    "        long = result[1:]\n",
    "        \n",
    "        #obtain short verison of long\n",
    "        size = len(long)\n",
    "        x = 0\n",
    "        while x ==0:\n",
    "            if (long[-1] == '0') or (long[-1] == '.'): # shave off any . or 0s from the end of version number.\n",
    "                long = long[:-1]\n",
    "            else:\n",
    "                x =1\n",
    "        vlabel = 'FS'+long\n",
    "\n",
    "        # obtain run number\n",
    "        notesdir = recondir.replace(\"RECON_FLAIR\",\"RECON_NOTES\")\n",
    "        with open(recondir+subjectname+'/scaninfo.csv','r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            scan_list = list(reader)\n",
    "            runstring = scan_list[0][0] # run\n",
    "            if len(runstring) == 1:\n",
    "                runstring = '0'+runstring\n",
    "        recon_name = vlabel+'_'+runstring\n",
    "    else: # otherwise incomplete, not run yet, or already moved\n",
    "        recon_pending = 1\n",
    "        print(subjectname+\" Recon either already organized or not ready yet.\")\n",
    "        recon_name = ''\n",
    "        long = ''\n",
    "    return subjectname, recon_name, recon_pending, long\n",
    "        \n",
    "FS_DETAILS = pe.Node(Function(input_names=[\"subjectname\"], \n",
    "                        output_names=[ 'subjectname', 'recon_name', 'recon_pending', 'long'],\n",
    "                         function=gather_FS_details),\n",
    "                        name='FS_DETAILS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : MAKE_ORIG_FOLDER\n",
    "\n",
    "def create_orig_folder(subjectname, recon_name, recon_pending):\n",
    "    import os\n",
    "    import shutil\n",
    "    recondir = '/autofs/cluster/animal/scan_data/leads/recon_nip/RECON_FLAIR/'\n",
    "    freesurfer_dirs = ['mri', 'stats', 'tmp', 'trash', 'touch', 'label', 'surf', 'scripts']\n",
    "    if recon_pending == 0:\n",
    "        # move all subfolders into this recon_name folder\n",
    "        for fsdir in freesurfer_dirs:\n",
    "            if os.path.isdir(recondir+subjectname+'/'+fsdir):\n",
    "                shutil.move(recondir+subjectname+'/'+fsdir, recondir+subjectname+'/'+recon_name+'/'+fsdir) # does this create FS6_02?\n",
    "    else:\n",
    "        print(subjectname+\" files moves already, or not yet prepared.\")\n",
    "    return subjectname, recon_name, recondir, recon_pending\n",
    "\n",
    "MAKE_ORIGINAL_DIR = pe.Node(Function(input_names=[\"subjectname\", \"recon_name\", \"recon_pending\"], \n",
    "                        output_names=['subjectname', 'recon_name', 'recondir', 'recon_pending'],\n",
    "                         function=create_orig_folder),\n",
    "                        name='MAKE_ORIGINAL_DIR')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE : PREPARE_MANEDITS\n",
    "\n",
    "def preparing_manedits(subjectname, recon_name, recondir, recon_pending):\n",
    "    import shutil\n",
    "    import pathlib\n",
    "    import os\n",
    "    analysesdir = '/autofs/cluster/animal/scan_data/leads/analyses_nip/RECON_FLAIR/'\n",
    "    if recon_pending == 0: # otherwise dir already created or not ready)\n",
    "        recon_name2 = 'unedit.'+recon_name\n",
    "        shutil.copytree(recondir+subjectname+'/'+recon_name, recondir+subjectname+'/'+recon_name2)\n",
    "        shutil.copyfile(recondir+subjectname+'/'+recon_name2+'/mri/brain.finalsurfs.mgz', recondir+subjectname+'/'+recon_name2+'/mri/brain.finalsurfs.manedit.mgz')\n",
    "        with open(recondir+'to_edit.list\\n', \"a\") as myfile:\n",
    "            myfile.write(subjectname)\n",
    "        \n",
    "    # this will symlink before manual editing but ok since renaming unedit >> edit is required\n",
    "    #Now create symlink to edit. in analysis folder!!\n",
    "#     try:\n",
    "#         os.symlink(recondir+subjectname+'/'+recon_name2, analysesdir+sub)\n",
    "#     except(FileExistsError):\n",
    "#         print(subjectname+\" in analyses is already linked to unedit.recon.\")\n",
    "    return subjectname\n",
    "        \n",
    "PREPARE_MANEDITS = pe.Node(Function(input_names=['subjectname', 'recon_name', 'recondir', 'recon_pending'], \n",
    "                        output_names=['subjectname'],\n",
    "                         function=preparing_manedits),\n",
    "                        name='PREPARE_MANEDITS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the 3T recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # NODE : INFOSOURCE\n",
    "INFOSOURCE = Node(IdentityInterface(fields=['subject_name'], mandatory_inputs=False),\n",
    "                  name=\"INFOSOURCE\")\n",
    "\n",
    "INFOSOURCE.iterables = ('subject_name', sh_dicomlist)\n",
    "\n",
    "# NODE : SELECTFILES\n",
    "#templates = dict(dicom=sh_dicomlist[0])    ## THIS WORKED!\n",
    "templates = {\n",
    "    \"dicom\": \"{subject_name}\" \n",
    "    }\n",
    "SELECTFILES = Node(nio.SelectFiles(templates, base_directory=dicomdir),\n",
    "                   name=\"SELECTFILES\")\n",
    "\n",
    "# NODE : DATASINK\n",
    "DATASINK = Node(nio.DataSink(base_directory=leadsdir,\n",
    "                container='recon_nip'),\n",
    "                name=\"DATASINK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect all nodes (including INFOSOURCE, SELECTFILES, and DATASINK) to workflow\n",
    "\n",
    "leads_workflow.connect([(INFOSOURCE, SELECTFILES, [('subject_name', 'subject_name')]),\n",
    "                (SELECTFILES, CREATEDIR, [('dicom', 'val')]),\n",
    "                (PASSWORDS, CREATEDIR, [('USER', 'USER')]),\n",
    "                (PASSWORDS, CREATEDIR, [('PASS', 'PASS')]), \n",
    "                (CREATEDIR, IMPORT_LONI_INFO, [('date', 'date')]),\n",
    "                (CREATEDIR, IMPORT_LONI_INFO, [('createdir_out3', 'dicomname')]),\n",
    "                (CREATEDIR, IMPORT_LONI_INFO, [('createdir_out1', 'subjectdir')]),     # need actual subjectdir name (in case of repeats)\n",
    "                (IMPORT_LONI_INFO, PREPARE_4_REDCAP, [('dicomname', 'dicomname')]),\n",
    "                (IMPORT_LONI_INFO, PREPARE_4_REDCAP, [('subjectdir', 'subjectdir')]),\n",
    "                (IMPORT_LONI_INFO, PREPARE_4_REDCAP, [('imageid', 'imageid')]),\n",
    "                (CREATEDIR, PREPARE_4_REDCAP, [('createdir_out3', 'pickdicom')]),\n",
    "                (CREATEDIR, UNPACK, [('createdir_out1', 'subjectdir')]),\n",
    "                 (CREATEDIR, UNPACK, [('createdir_out2', 'MPRAGE_path')]),\n",
    "                 (CREATEDIR, CONVERT2MGZ, [('createdir_out3', 'in_file')]),\n",
    "                 (CREATEDIR, CONVERT2MGZ, [('createdir_out4', 'out_file')]),\n",
    "                (CREATEDIR, CONVERT2MGZ, [('createdir_out1', 'reconpath')]),\n",
    "                (CONVERT2MGZ, CONVERTFLAIR, [('out_file', 'out_file')]), # added \n",
    "                (CREATEDIR, CONVERTFLAIR, [('flairdumplocation', 'flairdumplocation')]),\n",
    "                 (CREATEDIR, CONVERTFLAIR, [('pickflair', 'pickflair')]),\n",
    "                (CREATEDIR, CONVERTFLAIR, [('createdir_out1', 'reconpath')]),\n",
    "                (CONVERTFLAIR, SCAN_AND_LOG, [('out_file', 'mgz')]), #changed\n",
    "                #(CONVERTFLAIR, SCAN_AND_LOG, [('out_file', 'mgz')]),\n",
    "                (CREATEDIR, SCAN_AND_LOG, [('createdir_out5', 'reconfolder')]),\n",
    "                (UNPACK, SCAN_AND_LOG, [('unpack_out1', 'subname')]),\n",
    "                (UNPACK, SCAN_AND_LOG, [('unpack_out2', 'subjectdir')]),\n",
    "                (UNPACK, SCAN_AND_LOG, [('unpack_out3', 'scan_info')]),\n",
    "                (CREATEDIR, RECON_JOB, [('USER','USER')]),\n",
    "                (CREATEDIR, RECON_JOB, [('PASS','PASS')]),\n",
    "                (SCAN_AND_LOG, RECON_JOB, [('subname','subjectname')]), \n",
    "                (RECON_JOB, FS_DETAILS, [('subjectname','subjectname')]),\n",
    "                (FS_DETAILS, MAKE_ORIGINAL_DIR, [('subjectname','subjectname')]), \n",
    "                (FS_DETAILS, MAKE_ORIGINAL_DIR, [('recon_pending','recon_pending')]), \n",
    "                (FS_DETAILS, MAKE_ORIGINAL_DIR, [('recon_name','recon_name')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('subjectname','subjectname')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('recon_name','recon_name')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('recondir','recondir')]), \n",
    "                (MAKE_ORIGINAL_DIR, PREPARE_MANEDITS, [('recon_pending','recon_pending')]),\n",
    "                (FS_DETAILS, PREPARE_4_REDCAP, [('long','fsversion')]), \n",
    "                (PREPARE_MANEDITS, DATASINK, [('subjectname','backup')])  # backup folder?\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190429-16:53:44,390 nipype.workflow INFO:\n",
      "\t Workflow leads_workflow settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "190429-16:53:44,437 nipype.workflow INFO:\n",
      "\t Running serially.\n",
      "190429-16:53:44,439 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.SELECTFILES\" in \"/tmp/tmpt077hxw6/leads_workflow/_subject_name_LDS3600069_20190426/SELECTFILES\".\n",
      "190429-16:53:44,444 nipype.workflow INFO:\n",
      "\t [Node] Running \"SELECTFILES\" (\"nipype.interfaces.io.SelectFiles\")\n",
      "190429-16:53:44,453 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.SELECTFILES\".\n",
      "190429-16:53:44,455 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.PASSWORDS\" in \"/tmp/tmpzcr345bo/leads_workflow/PASSWORDS\".\n",
      "190429-16:53:44,459 nipype.workflow INFO:\n",
      "\t [Node] Running \"PASSWORDS\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "Please enter your PASSWORD for launchpad access: \n",
      "········\n",
      "190429-16:53:48,335 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.PASSWORDS\".\n",
      "190429-16:53:48,337 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CREATEDIR\" in \"/tmp/tmp5f1o_tta/leads_workflow/_subject_name_LDS3600069_20190426/CREATEDIR\".\n",
      "190429-16:53:48,349 nipype.workflow INFO:\n",
      "\t [Node] Running \"CREATEDIR\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190429-16:53:48,411 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CREATEDIR\".\n",
      "190429-16:53:48,413 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CONVERT2MGZ\" in \"/tmp/tmp5h_8tp53/leads_workflow/_subject_name_LDS3600069_20190426/CONVERT2MGZ\".\n",
      "190429-16:53:48,421 nipype.workflow INFO:\n",
      "\t [Node] Running \"CONVERT2MGZ\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190429-16:53:48,445 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CONVERT2MGZ\".\n",
      "190429-16:53:48,446 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.CONVERTFLAIR\" in \"/tmp/tmp28qrxtob/leads_workflow/_subject_name_LDS3600069_20190426/CONVERTFLAIR\".\n",
      "190429-16:53:48,456 nipype.workflow INFO:\n",
      "\t [Node] Running \"CONVERTFLAIR\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190429-16:53:48,482 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.CONVERTFLAIR\".\n",
      "190429-16:53:48,484 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.UNPACK\" in \"/tmp/tmpmzkk1b3v/leads_workflow/_subject_name_LDS3600069_20190426/UNPACK\".\n",
      "190429-16:53:48,492 nipype.workflow INFO:\n",
      "\t [Node] Running \"UNPACK\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190429-16:53:48,505 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.UNPACK\".\n",
      "190429-16:53:48,507 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.SCAN_AND_LOG\" in \"/tmp/tmperfm8yfq/leads_workflow/_subject_name_LDS3600069_20190426/SCAN_AND_LOG\".\n",
      "190429-16:53:48,517 nipype.workflow INFO:\n",
      "\t [Node] Running \"SCAN_AND_LOG\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190429-16:53:48,548 nipype.workflow INFO:\n",
      "\t [Node] Finished \"leads_workflow.SCAN_AND_LOG\".\n",
      "190429-16:53:48,549 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.RECON_JOB\" in \"/tmp/tmpn0u6zk2a/leads_workflow/_subject_name_LDS3600069_20190426/RECON_JOB\".\n",
      "190429-16:53:48,558 nipype.workflow INFO:\n",
      "\t [Node] Running \"RECON_JOB\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190429-16:54:03,285 nipype.workflow WARNING:\n",
      "\t [Node] Error on \"leads_workflow.RECON_JOB\" (/tmp/tmpn0u6zk2a/leads_workflow/_subject_name_LDS3600069_20190426/RECON_JOB)\n",
      "190429-16:54:03,291 nipype.workflow ERROR:\n",
      "\t Node RECON_JOB.a0 failed to run on host nimbus.nmr.mgh.harvard.edu.\n",
      "190429-16:54:03,297 nipype.workflow ERROR:\n",
      "\t Saving crash info to /autofs/homes/002/rje11/crash-20190429-165403-rje11-RECON_JOB.a0-f532211e-ba03-44e0-8ff2-b557923c5e43.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\", line 44, in run\n",
      "    node.run(updatehash=updatehash)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 473, in run\n",
      "    result = self._run_interface(execute=True)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 557, in _run_interface\n",
      "    return self._run_command(execute)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 637, in _run_command\n",
      "    result = self._interface.run(cwd=outdir)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/interfaces/base/core.py\", line 511, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/interfaces/utility/wrappers.py\", line 144, in _run_interface\n",
      "    out = function_handle(**args)\n",
      "  File \"<string>\", line 28, in recon_job\n",
      "TypeError: write() argument must be str, not tuple\n",
      "\n",
      "190429-16:54:03,302 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"leads_workflow.IMPORT_LONI_INFO\" in \"/tmp/tmpa2bwpfp_/leads_workflow/_subject_name_LDS3600069_20190426/IMPORT_LONI_INFO\".\n",
      "190429-16:54:03,310 nipype.workflow INFO:\n",
      "\t [Node] Running \"IMPORT_LONI_INFO\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190429-16:54:03,363 nipype.workflow WARNING:\n",
      "\t [Node] Error on \"leads_workflow.IMPORT_LONI_INFO\" (/tmp/tmpa2bwpfp_/leads_workflow/_subject_name_LDS3600069_20190426/IMPORT_LONI_INFO)\n",
      "190429-16:54:03,367 nipype.workflow ERROR:\n",
      "\t Node IMPORT_LONI_INFO.a0 failed to run on host nimbus.nmr.mgh.harvard.edu.\n",
      "190429-16:54:03,369 nipype.workflow ERROR:\n",
      "\t Saving crash info to /autofs/homes/002/rje11/crash-20190429-165403-rje11-IMPORT_LONI_INFO.a0-ffaffee5-ec1e-4b6e-a0bd-1c8d1842fcb8.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 26, in import_loni_notes\n",
      "IndexError: index 0 is out of bounds for axis 0 with size 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/pubsw/packages/python/anaconda3-5.2.0/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 3078, in get_loc\n",
      "    return self._engine.get_loc(key)\n",
      "  File \"pandas/_libs/index.pyx\", line 140, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 162, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'leadsid'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\", line 44, in run\n",
      "    node.run(updatehash=updatehash)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 473, in run\n",
      "    result = self._run_interface(execute=True)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 557, in _run_interface\n",
      "    return self._run_command(execute)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 637, in _run_command\n",
      "    result = self._interface.run(cwd=outdir)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/interfaces/base/core.py\", line 511, in run\n",
      "    runtime = self._run_interface(runtime)\n",
      "  File \"/homes/2/rje11/.local/lib/python3.6/site-packages/nipype/interfaces/utility/wrappers.py\", line 144, in _run_interface\n",
      "    out = function_handle(**args)\n",
      "  File \"<string>\", line 60, in import_loni_notes\n",
      "  File \"/usr/pubsw/packages/python/anaconda3-5.2.0/lib/python3.6/site-packages/pandas/core/frame.py\", line 2688, in __getitem__\n",
      "    return self._getitem_column(key)\n",
      "  File \"/usr/pubsw/packages/python/anaconda3-5.2.0/lib/python3.6/site-packages/pandas/core/frame.py\", line 2695, in _getitem_column\n",
      "    return self._get_item_cache(key)\n",
      "  File \"/usr/pubsw/packages/python/anaconda3-5.2.0/lib/python3.6/site-packages/pandas/core/generic.py\", line 2489, in _get_item_cache\n",
      "    values = self._data.get(item)\n",
      "  File \"/usr/pubsw/packages/python/anaconda3-5.2.0/lib/python3.6/site-packages/pandas/core/internals.py\", line 4115, in get\n",
      "    loc = self.items.get_loc(item)\n",
      "  File \"/usr/pubsw/packages/python/anaconda3-5.2.0/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 3080, in get_loc\n",
      "    return self._engine.get_loc(self._maybe_cast_indexer(key))\n",
      "  File \"pandas/_libs/index.pyx\", line 140, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 162, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'leadsid'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190429-16:54:03,373 nipype.workflow INFO:\n",
      "\t ***********************************\n",
      "190429-16:54:03,373 nipype.workflow ERROR:\n",
      "\t could not run node: leads_workflow.RECON_JOB.a0\n",
      "190429-16:54:03,374 nipype.workflow INFO:\n",
      "\t crashfile: /autofs/homes/002/rje11/crash-20190429-165403-rje11-RECON_JOB.a0-f532211e-ba03-44e0-8ff2-b557923c5e43.pklz\n",
      "190429-16:54:03,375 nipype.workflow ERROR:\n",
      "\t could not run node: leads_workflow.IMPORT_LONI_INFO.a0\n",
      "190429-16:54:03,376 nipype.workflow INFO:\n",
      "\t crashfile: /autofs/homes/002/rje11/crash-20190429-165403-rje11-IMPORT_LONI_INFO.a0-ffaffee5-ec1e-4b6e-a0bd-1c8d1842fcb8.pklz\n",
      "190429-16:54:03,376 nipype.workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Workflow did not execute cleanly. Check log for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c00658ff177a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Execute your workflow in sequential way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# leads_workflow.run(run(plugin='MultiProc', plugin_args={'n_procs' : 2})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mleads_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mleads_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph2use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'flat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%dT%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_provenance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nipype/pipeline/plugins/linear.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_callback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mreport_nodes_not_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nipype/pipeline/plugins/tools.py\u001b[0m in \u001b[0;36mreport_nodes_not_run\u001b[0;34m(notrun)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         raise RuntimeError(('Workflow did not execute cleanly. '\n\u001b[0m\u001b[1;32m     83\u001b[0m                             'Check log for details'))\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Workflow did not execute cleanly. Check log for details"
     ]
    }
   ],
   "source": [
    "# Execute your workflow in sequential way\n",
    "# leads_workflow.run(run(plugin='MultiProc', plugin_args={'n_procs' : 2})\n",
    "leads_workflow.run()\n",
    "\n",
    "leads_workflow.write_graph(graph2use='flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_workflow.write_graph(graph2use='flat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
