{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This is LEADS_redcapfill (script 2A and 2B); to include all the latest subjects, this script must be run after running leads_unpack (script 1A and 1B) and requires the completion of manual edits (THE RECON-ALL JOBS MUST BE COMPLETED). This script 2A will re-run the GLM and re-generate the large table of roi summary stats.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into csv for redcap input\n",
    "import re\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydicom\n",
    "from pydicom.tag import Tag\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import csv\n",
    "from redcap import Project, RedcapError\n",
    "import subprocess\n",
    "from paramiko import SSHClient\n",
    "from os import system\n",
    "from nipype.interfaces.freesurfer import MRISPreproc\n",
    "from nipype.interfaces.freesurfer import SurfaceSmooth\n",
    "from nipype.interfaces.freesurfer import GLMFit\n",
    "\n",
    "today = str(datetime.date.today())\n",
    "\n",
    "# make a dictionary for what number ID corresponds to what site:\n",
    "sites = {\n",
    "    \"360\": \"MGHMartinos4\", \n",
    "    \"011\": \"WashUSL\", # CCIR PRISMA (WashUSL)\n",
    "    \"022\": \"UPenn\", # 'HUP' (UPenn)\n",
    "    \"037\": \"IndianaU\", # INDIANA UNIVERSITY\n",
    "    \"073\": \"UCSF\", # Neuroscience Imaging Center\n",
    "    \"941\": \"BrownU\", # BROWN UNIVERSITY MRF / # ButlerHosp / BHMAP\n",
    "    \"007\": \"Mayo\", #MayoCRochester or MayoCJacksonville?\n",
    "    \"010\": \"CPMC\", # CA Pacific medical center?\n",
    "    \"018\": \"JHopkins\",\n",
    "    \"032\": \"Emory\",\n",
    "    \"035\": \"UCLA\",\n",
    "    \"036\": \"MCJ\",\n",
    "    \"067\": \"NorthWesternU\",\n",
    "    \"177\": \"HoustonMNI\",\n",
    "    # Columbia,?\n",
    "    # UWisconsin 127 (no longer involved with LEADS)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passprompt():\n",
    "    USER= input(\"Please enter your USERNAME for launchpad acess: \")\n",
    "    print('PASSWORD: ')\n",
    "    PASS= getpass.getpass()\n",
    "    return USER, PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your USERNAME for launchpad acess: rje11\n",
      "PASSWORD: \n",
      "········\n"
     ]
    }
   ],
   "source": [
    "[USER, PASS] =passprompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ANALYZE.SUBJECTS.sh, which called SUBJECTSFILE\n",
    "# MAKE SURE SUBJECTSFILE IS UP TO DATE WITH ALL MANUALLY EDITED, REPROCESSED RECONS!!!!!!\n",
    "host=\"launchpad\"\n",
    "user=USER\n",
    "pw=PASS\n",
    "client=SSHClient()\n",
    "client.load_system_host_keys()\n",
    "client.connect(host,username=user,password=pw, look_for_keys=False)\n",
    "stdin, stdout, stderr = client.exec_command('(cd /autofs/cluster/animal/scan_data/leads/analyses; ./ANALYZESUBJECTS.sh)')\n",
    "# print(\"stderr: \", stderr.readlines())\n",
    "# print(\"pwd: \", stdout.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/autofs/cluster/animal/scan_data/leads/analyses/ABpos_control.fsgd and lh\n",
      "/autofs/cluster/animal/scan_data/leads/analyses/ABpos_control.fsgd and rh\n",
      "/autofs/cluster/animal/scan_data/leads/analyses/ABneg_control.fsgd and lh\n",
      "/autofs/cluster/animal/scan_data/leads/analyses/ABneg_control.fsgd and rh\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESS (but no smoothing)\n",
    "\n",
    "# THESE MUST BE UPDATED SOMEHOW... connect to demog if using that?\n",
    "amy_pos_cont = '/autofs/cluster/animal/scan_data/leads/analyses/ABpos_control.fsgd'\n",
    "amy_neg_cont = '/autofs/cluster/animal/scan_data/leads/analyses/ABneg_control.fsgd'\n",
    "os.environ['SUBJECTS_DIR'] = '/autofs/cluster/animal/scan_data/leads/analyses/' # use this instead\n",
    "contrastfile = '/autofs/cluster/animal/scan_data/leads/analyses/group.diff.mtx'\n",
    "\n",
    "# create index for both hemispheres, and both conditions (AMY+ vs control, and AMY- vs control) \n",
    "index =  [[amy_pos_cont, 'lh'],[amy_pos_cont, 'rh'],[amy_neg_cont, 'lh'],[amy_neg_cont, 'rh']]\n",
    "\n",
    "os.environ['SUBJECTS_DIR'] = '/autofs/cluster/animal/scan_data/leads/analyses/' # use this instead\n",
    "os.chdir('/autofs/cluster/animal/scan_data/leads/analyses/')\n",
    "\n",
    "for metric in index:\n",
    "\n",
    "    preproc = MRISPreproc()\n",
    "    preproc.inputs.fsgd_file = metric[0]\n",
    "    preproc.inputs.target = 'fsaverage'\n",
    "    preproc.inputs.hemi = metric[1]\n",
    "    preproc.inputs.surf_measure = 'thickness'\n",
    "    preproc.inputs.out_file = '/autofs/cluster/animal/scan_data/leads/analyses/'+metric[1]+'.'+metric[0][-18:-5]+'.thickness.00.mgz'\n",
    "    #preproc.inputs.environ = {\"SUBJECTS_DIR\": \"/autofs/cluster/animal/scan_data/leads/analyses/\"}\n",
    "    #preproc.inputs.subjects_dir = \"/autofs/cluster/animal/scan_data/leads/analyses/\" # does not work? not sure why\n",
    "    system(preproc.cmdline)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/autofs/cluster/animal/scan_data/leads/analyses/ABpos_control.fsgd and lh\n",
      "/autofs/cluster/animal/scan_data/leads/analyses/ABpos_control.fsgd and rh\n",
      "/autofs/cluster/animal/scan_data/leads/analyses/ABneg_control.fsgd and lh\n",
      "/autofs/cluster/animal/scan_data/leads/analyses/ABneg_control.fsgd and rh\n"
     ]
    }
   ],
   "source": [
    "# SMOOTH (wraps mri_surf2surf)\n",
    "\n",
    "for metric in index:\n",
    "    smoother = SurfaceSmooth()\n",
    "    smoother.inputs.hemi = metric[1]\n",
    "    smoother.inputs.subject_id = 'fsaverage'\n",
    "    smoother.inputs.in_file = metric[1]+'.'+metric[0][-18:-5]+'.thickness.00.mgz'\n",
    "    smoother.inputs.fwhm = 10\n",
    "    smoother.inputs.cortex = True # defaults to 1\n",
    "    smoother.inputs.out_file = metric[1]+'.'+metric[0][-18:-5]+'.thickness.10.mgz'\n",
    "    smoother.inputs.subjects_dir = \"/autofs/cluster/animal/scan_data/leads/analyses/\"\n",
    "    system(smoother.cmdline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GLM analysis (mri_glmfit)\n",
    "\n",
    "for metric in index:\n",
    "    glmfit = GLMFit()\n",
    "    glmfit.inputs.in_file = metric[1]+'.'+metric[0][-18:-5]+'.thickness.10.mgz'\n",
    "    glmfit.inputs.fsgd = (metric[0], 'dods')\n",
    "    glmfit.inputs.contrast = contrastfile\n",
    "    glmfit.inputs.cortex = True # defaults to 1\n",
    "    glmfit.inputs.subjects_dir = \"/autofs/cluster/animal/scan_data/leads/analyses/\"\n",
    "    glmfit.inputs.subject_id = 'fsaverage' # not sure if this is correct?\n",
    "    glmfit.inputs.hemi = metric[1] # not sure if this is correct?\n",
    "    glmfit.inputs.surf = True\n",
    "    glmfit.inputs.glm_dir = \"/autofs/cluster/animal/scan_data/leads/analyses/\"+metric[1]+'.'+metric[0][-18:-5]+'.glmdir'\n",
    "    system(glmfit.cmdline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# screen shots??\n",
    "\n",
    "shots = fs.SurfaceSnapshots(subject_id=\"fsaverage\", hemi=\"lh\", surface=\"inflated\")\n",
    "shots.inputs.overlay = '/autofs/cluster/animal/scan_data/leads/analyses/lh.ABneg_control.glmdir/group.diff/sig.mgh'\n",
    "shots.inputs.overlay_range = (1.64, 3)\n",
    "shots.inputs.invert_overlay = True\n",
    "shots.inputs.subjects_dir = '/autofs/cluster/animal/scan_data/leads/analyses/'\n",
    "shots.inputs.screenshot_stem = '/autofs/cluster/animal/scan_data/leads/analyses/screenshots'\n",
    "system(shots.cmdline)\n",
    "\n",
    "# not sure how to make use of this yet.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This is LEADS_redcapfill (script 2B). This script will pull the stats output, dicom info, and info from spreadsheets that are necessary to create the data matrix which will be transformed and imported into RedCap.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysesdir = \"/autofs/cluster/animal/scan_data/leads/analyses/\"\n",
    "recondir = \"/autofs/cluster/animal/scan_data/leads/recon/\"\n",
    "\n",
    "# load in the instrument fields\n",
    "import_instrument = pd.read_csv('/autofs/cluster/animal/scan_data/leads/redcap_instruments/LEADS_ImportTemplate_2019-02-07.csv')\n",
    "fieldlist = list(import_instrument.columns.values) # all fields\n",
    "\n",
    "groupinfo = '/autofs/cluster/animal/scan_data/leads/analyses/demographics.csv'\n",
    "grouplabels = pd.read_csv(groupinfo)\n",
    "\n",
    "statswhole = analysesdir+'/leads_aseg_stats.txt'\n",
    "statslsurf = analysesdir+'/lh_leads_aparc_stats.txt'\n",
    "statsrsurf = analysesdir+'/rh_leads_aparc_stats.txt'\n",
    "\n",
    "whole = pd.read_csv(statswhole,sep='\\t')\n",
    "left = pd.read_csv(statslsurf,sep='\\t')\n",
    "right = pd.read_csv(statsrsurf,sep='\\t')\n",
    "\n",
    "tmpstats = pd.merge(whole, left, right_index=True, left_index=True)\n",
    "totalstats = pd.merge(tmpstats, right, right_index=True, left_index=True)\n",
    "\n",
    "demListOfSeries = []\n",
    "visitListOfSeries = []\n",
    "\n",
    "# conversion table of aparc / aseg names\n",
    "roikeys = '/autofs/cluster/animal/scan_data/leads/redcap_instruments/LEADS_roiname_conversion.csv'\n",
    "with open(roikeys, mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    roidict = {rows[0]:rows[1] for rows in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)\n",
    "        \n",
    "def cutid(sessid):\n",
    "    find = re.compile(r\"^[^_]*\")\n",
    "    subjid = re.search(find, sessid).group(0)\n",
    "    return subjid\n",
    "\n",
    "def group(subject):\n",
    "    groupspec = (grouplabels.loc[grouplabels['Subject'] == subject, 'Group'].values[0])\n",
    "    amyspec = (grouplabels.loc[grouplabels['Subject'] == subject, 'AMYLEVEL'].values[0])\n",
    "    return groupspec, amyspec\n",
    "\n",
    "def convsex(S):\n",
    "    if S == \"O\":\n",
    "        S = \"\"\n",
    "    elif S == \"M\":\n",
    "        S = \"1\"\n",
    "    elif S == \"F\":\n",
    "        S = \"0\"\n",
    "    return S\n",
    "\n",
    "def convgroup(G):\n",
    "    try:\n",
    "        value = math.isnan(float(G))\n",
    "        if value:\n",
    "            G = \"\"\n",
    "    except(ValueError):\n",
    "        pass\n",
    "    if G == \"PT\":\n",
    "        G = \"0\"\n",
    "    elif G == \"CN\":\n",
    "        G = \"1\"\n",
    "    return G\n",
    "\n",
    "def converting2dic(path):\n",
    "    with open(pathtomaster) as fp:\n",
    "        datalist = []\n",
    "        for cnt, line in enumerate(fp):\n",
    "            newline = line.split(\",\")\n",
    "            if cnt == 0:\n",
    "                header = newline\n",
    "            else:\n",
    "                dictionary = dict(zip(header,newline))\n",
    "                datalist.append(dictionary)\n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDS0370020_20181212\n",
      "Updated redcap matrix data for LDS0370020_20181212 ...........\n",
      "LDS0370001_20180509\n",
      "Updated redcap matrix data for LDS0370001_20180509 ...........\n",
      "LDS0370042_20190108\n",
      "Updated redcap matrix data for LDS0370042_20190108 ...........\n",
      "LDS0110022_20181129\n",
      "Updated redcap matrix data for LDS0110022_20181129 ...........\n",
      "LDS9410025_20181128\n",
      "Updated redcap matrix data for LDS9410025_20181128 ...........\n",
      "LDS0370015_20181113\n",
      "Updated redcap matrix data for LDS0370015_20181113 ...........\n",
      "LDS0370005_20180802\n",
      "Updated redcap matrix data for LDS0370005_20180802 ...........\n",
      "LDS3600043_20190122\n",
      "Updated redcap matrix data for LDS3600043_20190122 ...........\n",
      "LDS0110040_20190124\n",
      "Updated redcap matrix data for LDS0110040_20190124 ...........\n",
      "LDS0220026_20181109\n",
      "Updated redcap matrix data for LDS0220026_20181109 ...........\n",
      "LDS3600030_20181219\n",
      "Updated redcap matrix data for LDS3600030_20181219 ...........\n",
      "LDS0110021_20181016\n",
      "Updated redcap matrix data for LDS0110021_20181016 ...........\n",
      "LDS0370002_20180606\n",
      "Updated redcap matrix data for LDS0370002_20180606 ...........\n",
      "LDS0370010_20180815\n",
      "Updated redcap matrix data for LDS0370010_20180815 ...........\n",
      "LDS0370008_20180815\n",
      "Updated redcap matrix data for LDS0370008_20180815 ...........\n",
      "LDS0370014_20180913\n",
      "Updated redcap matrix data for LDS0370014_20180913 ...........\n",
      "LDS0370009_20180816\n",
      "Updated redcap matrix data for LDS0370009_20180816 ...........\n",
      "LDS0370038_20181217\n",
      "Updated redcap matrix data for LDS0370038_20181217 ...........\n",
      "LDS0370019_20181121\n",
      "Updated redcap matrix data for LDS0370019_20181121 ...........\n",
      "LDS0730001_20180619\n",
      "Updated redcap matrix data for LDS0730001_20180619 ...........\n",
      "LDS0370017_20181001\n",
      "Updated redcap matrix data for LDS0370017_20181001 ...........\n",
      "LDS0730024_20181107\n",
      "Updated redcap matrix data for LDS0730024_20181107 ...........\n",
      "LDS0220031_20181130\n",
      "Updated redcap matrix data for LDS0220031_20181130 ...........\n",
      "LDS0370016_20180912\n",
      "Updated redcap matrix data for LDS0370016_20180912 ...........\n",
      "LDS0370011_20180822\n",
      "Updated redcap matrix data for LDS0370011_20180822 ...........\n",
      "LDS9410035_20181126\n",
      "Updated redcap matrix data for LDS9410035_20181126 ...........\n",
      "LDS0370006_20180726\n",
      "Updated redcap matrix data for LDS0370006_20180726 ...........\n",
      "LDS9410023_20181127\n",
      "Updated redcap matrix data for LDS9410023_20181127 ...........\n",
      "LDS3600032_20190124\n",
      "Updated redcap matrix data for LDS3600032_20190124 ...........\n",
      "LDS9410028_20181109\n",
      "Updated redcap matrix data for LDS9410028_20181109 ...........\n",
      "LDS9410036_20181203\n",
      "Updated redcap matrix data for LDS9410036_20181203 ...........\n",
      "LDS9410049_20190122\n",
      "Updated redcap matrix data for LDS9410049_20190122 ...........\n",
      "LDS0370037_20181218\n",
      "Updated redcap matrix data for LDS0370037_20181218 ...........\n",
      "LDS0370013_20180822\n",
      "Updated redcap matrix data for LDS0370013_20180822 ...........\n",
      "LDS9410027_20181105\n",
      "Updated redcap matrix data for LDS9410027_20181105 ...........\n",
      "LDS0370034_20190107\n",
      "Updated redcap matrix data for LDS0370034_20190107 ...........\n",
      "LDS0370018_20181015\n",
      "Updated redcap matrix data for LDS0370018_20181015 ...........\n",
      "LDS0370012_20180824\n",
      "Updated redcap matrix data for LDS0370012_20180824 ...........\n",
      "LDS0370029_20181210\n",
      "Updated redcap matrix data for LDS0370029_20181210 ...........\n"
     ]
    }
   ],
   "source": [
    "folders = [x for x in os.listdir(analysesdir) if x.startswith(\"LDS\")]\n",
    "\n",
    "#folders = ['LDS0370012_20180824','LDS0370005_20180802']\n",
    "\n",
    "dem_df = pd.DataFrame(columns=fieldlist, index=range(len(folders)))\n",
    "visit_df = pd.DataFrame(columns=fieldlist, index=range(len(folders)))\n",
    "a=0\n",
    "\n",
    "#for sessid in folders:\n",
    "for element in range(0,len(folders)):\n",
    "    sessid = folders[element]\n",
    "    print(sessid)\n",
    "\n",
    "    fs_sessid = totalstats.loc[totalstats['Measure:volume'] == sessid] # loading roi stats for particular subject\n",
    "    \n",
    "    dicomlist = '/autofs/cluster/animal/scan_data/leads/recon/'+sessid+'/dicomlist.txt'\n",
    "    with open(dicomlist) as f:\n",
    "        firstline = f.readline()[:-1]\n",
    "\n",
    "    dateshortcut= sessid[-8:-4]+'-'+sessid[-4:-2]+'-'+sessid[-2:]\n",
    "    dicom = find(firstline,'/autofs/cluster/animal/scan_data/leads/LEADS/'+sessid)\n",
    "    editline = dicom.split(\"/\")\n",
    "    sessionserial = editline[-2]\n",
    "    DICOMPATH = dicom.replace(editline[- 1],\"\")\n",
    "    SITE = sites.get(sessid[3:6])\n",
    "\n",
    "    # dicom info\n",
    "    ds = pydicom.read_file(dicom)\n",
    "    TR = str(ds[0x18, 0x80].value)\n",
    "    SEX = convsex(str(ds[0x10,0x40].value))\n",
    "    age = ds[0x00101010].value\n",
    "    AGE = str(int(age[:-1]))\n",
    "    # COIL = ds[0x18, 1250].value  # not found by pydicom\n",
    "    SCANDATE = ds[0x00080022].value\n",
    "    SUBJECTID = cutid(folders[element])\n",
    "    [G, A] = group(SUBJECTID)\n",
    "    GROUP = convgroup(G)\n",
    "    AMYSTATUS = A\n",
    "    \n",
    "    # get FS version\n",
    "    element = '/autofs/cluster/animal/scan_data/leads/recon/'+sessid\n",
    "    prefixed = [filename for filename in os.listdir(element) if filename.startswith(\"edit\")]\n",
    "    editstring=(prefixed[0].replace('edit.','').replace(',',' ').split())[0]\n",
    "    findversion = re.compile(r\"^[^_]*\")\n",
    "    tmpV = re.search(findversion, editstring).group(0)\n",
    "    FSVERSION = re.sub(r'[A-Z]+', '', tmpV, re.I)\n",
    "    \n",
    "    dem_df.loc[a, 'record_id'] = SUBJECTID\n",
    "    #dem_df.loc[a, \"redcap_repeat_instrument\"] = \"\"\n",
    "    #dem_df.loc[a,'redcap_repeat_instance'] = ''\n",
    "    dem_df.loc[a,'site_name'] = SITE\n",
    "    dem_df.loc[a,'sex'] = SEX\n",
    "    dem_df.loc[a,'group'] = GROUP\n",
    "    dem_df.loc[a,'notes'] = \"\"\n",
    "    dem_df.loc[a,'demographics_complete'] = 1\n",
    "    \n",
    "    visit_df.loc[a, 'record_id'] = SUBJECTID\n",
    "    visit_df.loc[a, \"redcap_repeat_instrument\"] = \"visits\"\n",
    "    visit_df.loc[a,'redcap_repeat_instance'] = \"1\"\n",
    "    visit_df.loc[a,'scan_date'] = dateshortcut #convert date to correct format!\n",
    "    visit_df.loc[a,'age'] = AGE\n",
    "    visit_df.loc[a,'session_id'] = sessid\n",
    "    visit_df.loc[a,'dicom_path'] = DICOMPATH\n",
    "    visit_df.loc[a,'fs_version'] = FSVERSION\n",
    "    \n",
    "    \n",
    "    for key in roidict:\n",
    "        FSname = roidict.get(key)\n",
    "        if FSname != 'freesurfer_var':\n",
    "            try:\n",
    "                val = fs_sessid[FSname].values[0]\n",
    "                visit_df.loc[a, key] = val\n",
    "            except(IndexError):\n",
    "                pass\n",
    "            \n",
    "    print(\"Updated redcap matrix data for \"+sessid+\" ...........\")        \n",
    "    a=a+1\n",
    "\n",
    "dem_df.to_csv('/autofs/cluster/animal/scan_data/leads/redcap_instruments/demographics_'+today+'.csv')\n",
    "visit_df.to_csv('/autofs/cluster/animal/scan_data/leads/redcap_instruments/visits_'+today+'.csv')\n",
    "data_for_import = dem_df.append(visit_df)\n",
    "\n",
    "#MUST SAVE WITHOUT COLUMN!!! OTHERWISE MUST MANUALLY EDIT!\n",
    "#data_for_import.to_csv('/autofs/cluster/animal/scan_data/leads/redcap_instruments/data_for_import_'+today+'.csv')\n",
    "data_for_import.to_csv('/autofs/cluster/animal/scan_data/leads/redcap_instruments/data_for_import_'+today+'.csv',mode = 'w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to redcap (note: exporting means retrieving from redcap; importing means uploading to redcap)\n",
    "\n",
    "api_url = 'https://redcap.partners.org/redcap/api/'\n",
    "api_key = '9D668EE32160684D980CD6A95B23CC5B'\n",
    "# to set specific certificate pathway: verify_ssl='path/to/CA_bundle/' or set to False to ignore SSH verification altogether\n",
    "project = Project(api_url, api_key) # pycap verifies SSL certificate under the hood (using requests)\n",
    "# will return a list of dicts from all record IDs with all raw field names as keys\n",
    "data = project.export_records(format='df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send to REDCAP\n",
    "pathtomaster = '/autofs/cluster/animal/scan_data/leads/redcap_instruments/data_for_import_'+today+'.csv'\n",
    "final_dic = converting2dic(pathtomaster)\n",
    "response = project.import_records(final_dic, date_format='YMD')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
