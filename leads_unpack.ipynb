{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This leads_unpack script is script 1A and 1B for the LEADS data. This will unpack the data, create an mgz file and a scaninfo file with run information for the MPRAGE. It also renames the subject folders to session folders. Then it creates two log files: unpack.log contains all sessions that were unpacked (i.e. run by this script), and batch.recon.list which is erased at the onset of running this script to create a batch list that has not been previously run. So, after running this script, run the batch script batch.recon.sh which calls the batch.recon.list to run the initial recon. When the batch jobs have finished on launchpad, you must then move on to script 1B (see below) before completing the manual edits (working on automating this).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from os import system\n",
    "import csv\n",
    "from nipype.interfaces.freesurfer import MRIConvert\n",
    "import subprocess\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "#import nbpackage.mynotebook\n",
    "\n",
    "dicomdir = \"/cluster/animal/scan_data/leads/LEADS/\"\n",
    "unpacklog = \"/autofs/cluster/animal/scan_data/leads/recon/unpack.log\"\n",
    "recondir = \"/autofs/cluster/animal/scan_data/leads/recon/\"\n",
    "folders = [x for x in os.listdir(dicomdir) if not x.startswith(\".\")]\n",
    "\n",
    "# wipe clean batch.recon.list ----------------- either end of file or here?\n",
    "open(recondir+'batch.recon.list', 'w').close()\n",
    "pswd= ''\n",
    "\n",
    "Elements = {'scan_notes': [''],'loni_overallpass': [''], 'download_date':[''], 'xnat_upload':[''], 'recon_path':[''],'recon_notes':[''], 'dickerson_overallpass':['']}\n",
    "df = pd.DataFrame(Elements, columns= ['scan_notes', 'loni_overallpass', 'download_date','xnat_upload','recon_path','recon_notes','dickerson_overallpass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passprompt():\n",
    "    USER= input(\"Please enter your USERNAME for launchpad acess: \")\n",
    "    print('PASSWORD: ')\n",
    "    PASS= getpass.getpass()\n",
    "    return USER, PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your USERNAME for launchpad acess: rje11\n",
      "PASSWORD: \n",
      "········\n"
     ]
    }
   ],
   "source": [
    "[USER, PASS] =passprompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scaninfo(origpath):\n",
    "    with open(origpath+'/scan.info', 'r') as in_file:\n",
    "        for line in in_file:\n",
    "            editline = line.split()\n",
    "            with open(origpath+'/scaninfo.csv', 'w') as result:\n",
    "                wr = csv.writer(result, dialect='excel')\n",
    "                wr.writerow(editline)\n",
    "            result.close()\n",
    "    in_file.close()\n",
    "    \n",
    "def shortversion(long):\n",
    "    size = len(long)\n",
    "    x = 0\n",
    "    while x ==0:\n",
    "        if (long[-1] == '0') or (long[-1] == '.'):\n",
    "            long = long[:-1]\n",
    "        else:\n",
    "            x =1\n",
    "    return long\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for substring in folders:\n",
    "    subjectdir = recondir+substring\n",
    "    #if substring not in open(unpacklog).read():\n",
    "    if \"_\" not in substring:\n",
    "        print(\"will be unpacking \"+ substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpacking LDS0370007 ----------------------- \n",
      "CPU times: user 27.1 ms, sys: 26.5 ms, total: 53.6 ms\n",
      "Wall time: 47.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# APPROX Wall time: 46.5 s PER SUBJECT\n",
    "\n",
    "# load in new version of excel doc from LONI with exclusion criteria (add comments to ours somehow)\n",
    "\n",
    "folders = ['LDS0370007']\n",
    "\n",
    "for substring in folders:\n",
    "    subjectdir = recondir+substring\n",
    "    #if substring not in open(unpacklog).read():\n",
    "    if \"a\" == \"a\":\n",
    "    #if \"_\" not in substring:\n",
    "        print('unpacking ' + substring + \" ----------------------- \")\n",
    "        os.makedirs(recondir+substring+'/mri/orig/')\n",
    "\n",
    "        if substring == 'LDS0370006':\n",
    "            tmp = '/autofs/cluster/animal/scan_data/leads/LEADS/LDS0370006/Accelerated_Sagittal_MPRAGE/2018-07-26_10_41_42.0/S708999/'\n",
    "            MPRAGE_list = glob.glob(tmp, recursive=True)\n",
    "        elif substring == 'LDS0370007':\n",
    "            tmp = '/autofs/cluster/animal/scan_data/leads/LEADS/LDS0370007/Accelerated_Sagittal_MPRAGE/2018-08-01_10_49_41.0/S711584/'\n",
    "            MPRAGE_list = glob.glob(tmp, recursive=True)         \n",
    "        elif substring == 'LDS0370013':\n",
    "            tmp = '/autofs/cluster/animal/scan_data/leads/LEADS/LDS0370013/Accelerated_Sagittal_MPRAGE/2018-08-22_08_55_38.0/S722833/'\n",
    "            MPRAGE_list = glob.glob(tmp, recursive=True)\n",
    "        else:\n",
    "            # check number of date folders; check number of session folders\n",
    "            MPRAGE_list = glob.glob(dicomdir+substring + '/Accelerated_Sagittal_MPRAGE/*/*/', recursive=True)\n",
    "        #elif len(MPRAGE_list) > 1:\n",
    "        #    print(\"There are multiple MPRAGEs for \"+substring+\". Unpacking only the first; check others.\")\n",
    "    \n",
    "        # for now, proceed with unpacking only the first valid run (given multiple)\n",
    "        # check for errors and record RUN#\n",
    "        cmdstring = 'unpacksdcmdir -src %s -targ %s -scanonly %s/scan.info' % (MPRAGE_list[0], subjectdir, subjectdir)\n",
    "        system(cmdstring)\n",
    "\n",
    "        parse_scaninfo(subjectdir)\n",
    "        scan_info = pd.read_csv(subjectdir+'/scaninfo.csv', header=None)\n",
    "        check = scan_info.iloc[0,2] # first row (only row); second col (validity)\n",
    "        \n",
    "        #convert from dicom's MPRAGE to mgz (allow for more than one)\n",
    "        if check != 'ok':\n",
    "            print(MPRAGE_list[0]+\" IS AN INVALID RUN (I.E. NEEDS TO BE OMMITTED FROM ANALYSIS). SKIPPING THE UNPACKING.\")\n",
    "        else:\n",
    "            # make list of all files within MPRAGE_list[0]\n",
    "            dicomlist = os.listdir(MPRAGE_list[0])\n",
    "            with open(subjectdir+'/dicomlist.txt', 'w') as f:\n",
    "                for item in dicomlist:\n",
    "                    f.write(\"%s\\n\" % (item))\n",
    "                    \n",
    "            mc = MRIConvert()\n",
    "            mc.inputs.in_file = MPRAGE_list[0]+os.listdir(MPRAGE_list[0])[0] # first file\n",
    "            #mc.inputs.sdcm_list = subjectdir+'/dicomlist.txt' # no need for this\n",
    "            mc.inputs.out_file = subjectdir+'/mri/orig/001.mgz'\n",
    "            mc.inputs.out_type = 'mgz'\n",
    "            system(mc.cmdline)\n",
    "\n",
    "            #obtain date: must change folder names in both leadsir/recon and /leadsdir/LEADS\n",
    "            datestring = scan_info.iloc[0,7] \n",
    "            date = re.search('raw_'+r'+[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]', datestring).group()[4:]\n",
    "\n",
    "            #rename both folders\n",
    "            os.rename(recondir+substring, recondir+substring+'_'+date)\n",
    "            os.rename(dicomdir+substring, dicomdir+substring+'_'+date) \n",
    "            \n",
    "            # create batch.recon.list to launch recon list\n",
    "            with open(recondir+'batch.recon.list', \"a\") as myfile:\n",
    "                myfile.write(substring+'_'+date+'\\n')\n",
    "\n",
    "            ##add substring to log\n",
    "            with open(unpacklog, \"a\") as myfile:\n",
    "                myfile.write(substring+'_'+date+'\\n') \n",
    "            \n",
    "            # save an empty dataframe for scannotes (will add info after recon)\n",
    "            df.to_csv(subjectdir+'_'+date+'/scannotes.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stderr:  ['Opening pbsjob_321\\n']\n",
      "pwd:  ['Beginning batch submission..\\n', 'qsub -V -S /bin/sh  -m abe -M rje11   -l nodes=1:ppn=1,vmem=7gb -r n   /pbs/rje11/pbsjob_321 \\n', '17040077.launchpad.nmr.mgh.harvard.edu\\n', 'SUBMITTED JOB for LDS0370007_20180801\\n']\n"
     ]
    }
   ],
   "source": [
    "# WORKS. This will not re-run recons that have been processed.\n",
    "#I need to somehow to connect script 2 to this -- if I can make the script wait until\n",
    "# the jobs are done.\n",
    "from paramiko import SSHClient\n",
    "host=\"launchpad\"\n",
    "user=USER\n",
    "pw=PASS\n",
    "client=SSHClient()\n",
    "client.load_system_host_keys()\n",
    "client.connect(host,username=user,password=pw, look_for_keys=False)\n",
    "stdin, stdout, stderr = client.exec_command('(cd /autofs/cluster/animal/scan_data/leads/analyses; ./batch.recon.sh)')\n",
    "print(\"stderr: \", stderr.readlines())\n",
    "print(\"pwd: \", stdout.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbsjob_321 status: R\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-257-8ec0faacfeac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------- #\n",
    "# check the status somehow: do every hour? or set a timer when I start?\n",
    "import time\n",
    "\n",
    "# can I run several jobs through the function simultaneously?\n",
    "\n",
    "#for now only works for one job\n",
    "runningjobs = [] # qeued or running\n",
    "staticjobs = [] # completed with exit status 0 or 1\n",
    "pause= 100 #14400 #4 hours in seconds\n",
    "\n",
    "while submission != []:\n",
    "    stdin, stdout, stderr = client.exec_command('qstat -u rje11')\n",
    "    submission = \"pwd: \", stdout.readlines()\n",
    "    n=5 #start of jobs print\n",
    "    try:\n",
    "        sublist = (submission[1])[n]\n",
    "        job=sublist.split()\n",
    "        username = job[1]\n",
    "        jobname = job[3] # job name col\n",
    "        jobstatus=job[9]\n",
    "        if jobstatus==\"C\": #change back to C\n",
    "            print(jobname+\" status: complete\")\n",
    "            staticjobs.append(jobname)\n",
    "            # check if completed with no errors\n",
    "            stdin2, stdout2, stderr2 = client.exec_command('tail -n 1 /pbs/'+username+'/'+jobname+'.status')\n",
    "            complete_status = \"pwd: \", stdout2.readlines()\n",
    "            exit = (complete_status[1][0][-2])\n",
    "            subj = (complete_status[1][0])\n",
    "            # add the session_name \n",
    "            stdin3, stdout3, stderr3 = client.exec_command('head -n 4 /pbs/'+username+'/'+jobname+'.status')\n",
    "            session_name = \"pwd: \", stdout3.readlines()\n",
    "            subj = (session_name[1])[3] # take name out of this string\n",
    "            if exit == '1':\n",
    "                print(jobname+' exited with status 1. CHECK THIS SESSION and RE-RUN.')\n",
    "            elif exit == '0':\n",
    "                print(\"Will continue with execution\")\n",
    "            # make other script a function and call it from here.\n",
    "            # need the directory name\n",
    "        else:\n",
    "            runningjobs.append(jobname)\n",
    "            print(jobname+\" status: \"+jobstatus)\n",
    "        n=n+1\n",
    "    except(IndexError):\n",
    "        pass\n",
    "    time.sleep(pause) # is this bad practice? It runs until completely done with job..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This is script 1B, which should be run after script 1A. This relabels and relocates the original recon output folder to a subdirectory within the sesion folder. The subdirectory is named according to FSversion_Run#. This script also makes a copy of this folder and renames it \"unedit.FSversion_Run#'. The edit folder is the one that will be manually edited and while making edits, you can insert comments into the scannotes.csv in the session directory. NOTE: the unedit.recon output is symlinked to the analysis folder. After re-running the manually edited recons, you can rename the folder to edit.recon so that it is known which have been completed. After manual edit recon jobs are finished running, you can move on to LEADS_redcapfill (scripts 2A and 2B) to create the data matrix which will be used to import the data into RedCap)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN ORDER TO RUN THIS-- I NEED TO WAIT FOR THE RECONS TO FINISH-- how can I automate this?\n",
    "\n",
    "# this makes a copy of all initial recopn output and labels them according to FSV_run#\n",
    "# then creates symlinks to an analysis folder\n",
    "\n",
    "realrecons= '/autofs/cluster/animal/scan_data/leads/recon/'\n",
    "analysesdir = '/autofs/cluster/animal/scan_data/leads/analyses/'\n",
    "\n",
    "folders = [x for x in os.listdir(realrecons) if x.startswith(\"LDS\")]\n",
    "\n",
    "#folders = ['LDS0370020_20181212', 'LDS0110021_20181016']\n",
    "freesurfer_dirs = ['mri', 'stats', 'tmp', 'trash', 'touch', 'label', 'surf', 'scripts']\n",
    "\n",
    "for sub in folders:\n",
    "    print(sub)\n",
    "    \n",
    "    try:\n",
    "        # obtain FS version\n",
    "        versionfile = open(realrecons+sub+'/scripts/build-stamp.txt', 'r')\n",
    "        versionstring = versionfile.read()\n",
    "        version = versionstring.split('-')\n",
    "        result = [i for i in version if i.startswith('v')][0]\n",
    "        long = result[1:]\n",
    "        vlabel = 'FS'+shortversion(long)\n",
    "\n",
    "        # obtain run number\n",
    "        with open(realrecons+sub+'/scaninfo.csv','r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            scan_list = list(reader)\n",
    "            runstring = scan_list[0][0] # run\n",
    "            if len(runstring) == 1:\n",
    "                runstring = '0'+runstring\n",
    "        recon_name = vlabel+'_'+runstring\n",
    "\n",
    "        print(recon_name)\n",
    "    except(FileNotFoundError):\n",
    "        print(sub+\" freesurfer files not in directory.\")\n",
    "    \n",
    "    # now go into each subject folder and create a folder with recon_name\n",
    "    try:\n",
    "        os.mkdir(realrecons+sub+'/'+recon_name)\n",
    "    except(FileExistsError):\n",
    "        print(sub+\" original recon folder already created.\")\n",
    "    \n",
    "    # move all subfolders into this recon_name folder\n",
    "    for fsdir in freesurfer_dirs:\n",
    "        FS = Path(realrecons+sub+'/'+fsdir)\n",
    "        if FS.exists():\n",
    "            shutil.move(realrecons+sub+'/'+fsdir, realrecons+sub+'/'+recon_name+'/'+fsdir)\n",
    "            \n",
    "    try:\n",
    "        recon_name2 = 'unedit.'+recon_name\n",
    "        FS2 = Path(realrecons+sub+'/'+recon_name2)\n",
    "        shutil.copytree(realrecons+sub+'/'+recon_name, FS2) # if edit.recon folder does not already exist, add to log\n",
    "        with open(realrecons+'to_edit.list\\n', \"a\") as myfile:\n",
    "            myfile.write(sub)\n",
    "    except(FileExistsError):\n",
    "        print(sub+\" unedit.recon folder already created.\")\n",
    "        \n",
    "    #Now create symlink to edit. in analysis folder!!\n",
    "    try:\n",
    "        os.symlink(realrecons+sub+'/'+recon_name2, analysesdir+sub)\n",
    "    except(FileExistsError):\n",
    "        print(sub+\" in analyses is already linked to unedit.recon.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
